{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "LSTM development script\n"
     ]
    }
   ],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '..') #sys allows for the .ipynb file to connect to the shared folder files\n",
    "from shared_scripts import lstm_dataprocessing\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n",
    "modelname = \"LSTM\"\n",
    "\n",
    "print(f\"{modelname} development script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c232",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* establish lookback\n",
    "* figure out how to randomize training data\n",
    "* add new dataset\n",
    "* import Evaluation_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2   \n",
       "1         67.7           1.2         0.12          2.94          27.2   \n",
       "2         67.7           1.2         0.12          2.94          27.2   \n",
       "3         67.7           1.2         0.12          2.94          27.2   \n",
       "4         67.7           1.2         0.12          2.94          27.2   \n",
       "\n",
       "   Mean_Ann_Precip_in    datetime  flow_cfs        s1        s2  storage  swe  \\\n",
       "0                34.8  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2   \n",
       "1                34.8  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2   \n",
       "2                34.8  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1   \n",
       "3                34.8  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2   \n",
       "4                34.8  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      55.0  301  \n",
       "1      55.0  302  \n",
       "2      54.0  303  \n",
       "3      54.0  304  \n",
       "4      54.0  305  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get streamstats data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "file = \"Streamstats.csv\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = 'Streamstats/Streamstats.csv'      \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "\n",
    "#Get processed training data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/Processed\"\n",
    "file = \"raw_training_data.parquet\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    df = pd.read_parquet(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = \"NWM_ML\"+datapath.split(\"NWM_ML\",1)[1]+'/'+file       \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    df = pd.read_parquet(filepath)\n",
    "\n",
    "df.pop('Unnamed: 0')\n",
    "df['station_id'] = df['station_id'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814add4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>18.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>1992-10-01</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>18.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>1992-10-02</td>\n",
       "      <td>8.654167</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>18.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>1992-10-03</td>\n",
       "      <td>9.466667</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>18.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>1992-10-04</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>18.9</td>\n",
       "      <td>44.2</td>\n",
       "      <td>33.2</td>\n",
       "      <td>1992-10-05</td>\n",
       "      <td>10.195833</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id       Lat       Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10105900  41.57549 -111.85522              180.0              6700.0   \n",
       "1   10105900  41.57549 -111.85522              180.0              6700.0   \n",
       "2   10105900  41.57549 -111.85522              180.0              6700.0   \n",
       "3   10105900  41.57549 -111.85522              180.0              6700.0   \n",
       "4   10105900  41.57549 -111.85522              180.0              6700.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         20.5          1.01       0.0653          18.9          44.2   \n",
       "1         20.5          1.01       0.0653          18.9          44.2   \n",
       "2         20.5          1.01       0.0653          18.9          44.2   \n",
       "3         20.5          1.01       0.0653          18.9          44.2   \n",
       "4         20.5          1.01       0.0653          18.9          44.2   \n",
       "\n",
       "   Mean_Ann_Precip_in   datetime   flow_cfs        s1        s2  storage  swe  \\\n",
       "0                33.2 1992-10-01   9.250000 -0.891007 -0.453991      0.0  0.0   \n",
       "1                33.2 1992-10-02   8.654167 -0.891007 -0.453991      0.0  0.0   \n",
       "2                33.2 1992-10-03   9.466667 -0.891007 -0.453991      0.0  0.0   \n",
       "3                33.2 1992-10-04  11.833333 -0.891007 -0.453991      0.0  0.0   \n",
       "4                33.2 1992-10-05  10.195833 -0.891007 -0.453991      0.0  0.0   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      37.0  275  \n",
       "1      36.0  276  \n",
       "2      36.0  277  \n",
       "3      36.0  278  \n",
       "4      36.0  279  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove headwater stations\n",
    "headwater_stations = ['10011500', # Bear River headwaters before WY state line\n",
    "                      '10109000', # Logan River above dams\n",
    "                      '10113500', # HW Blacksmith fork\n",
    "                      '10128500', # Upper Weber above Oakley\n",
    "                      '10131000', #Chalk creek before Weber - lots of upstream irrigation, potentially include\n",
    "                        '10146400', #Currant Creek above Mona Reservoir - lots of upstream irrigation, potentially include\n",
    "                        '10150500', #Spanish fork after diamond fork - potentially include because of 6th water diversion CUP\n",
    "                        '10154200', #Upper Provo river after confluence of N/S forks - potentially include because of duchense tunnel water diversion CUP\n",
    "                        '10172700', #Vernon creek 2 ranges west of Utah Lake, shouldnt be included because not in GSL basin \n",
    "                        '10172800', #Willow creek west of Gransville,  shouldnt be included because does not make it to GSL\n",
    "                          '10172952'] #Dunn creek in Raft River Range, shouldnt be included because drains to bonnevile salt flats \n",
    "df = df[~df['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "\n",
    "# #reset index to clean up df\n",
    "df.reset_index( inplace =  True, drop = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608eedb0",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "Scaling your data for ML is done for all types of applications and helps the model converge faster. \n",
    "If there is no scaling performed, the model will essentially be forced to think certain features are more important than others, rather than being able to learn those things.\n",
    "\n",
    "There are different ways you can scale the data, such as min-max or standard scaling; both of which are applicable for your model. \n",
    "If you know you have a fixed min and max in your dataset (e.g. images), you can use min-max scaling to fix your input and/or output data to be between 0 and 1.\n",
    "\n",
    "For other applications where you do not have fixed bounds, standard scaling is useful.\n",
    "This gives all of your features zero-mean and unit variance. Therefore, the distributions of inputs and/or outputs are the same, and the model can treat them as such.\n",
    "\n",
    "The scaling for your outputs is important in defining the activation function for the output layer.\n",
    "If you have min-max scaled outputs, you can use sigmoid, because it bounds the outputs to between 0 and 1. \n",
    "If you are using standard scaling for the outputs, you would want to be sure you use a linear activation function, because technically standard-scaled outputs are not bounded. \n",
    "The choice of output activation is important, and knowledge of how your outputs are scaled is important in determining which activation to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185b2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121479, 5, 14]) torch.Size([121479])\n"
     ]
    }
   ],
   "source": [
    "#Process DF to input tensors into lstm\n",
    "#adding one feature at a time to observe changes in model performance\n",
    "input_columns =['Drainage_area_mi2', \n",
    "                'Mean_Basin_Elev_ft',\n",
    "                'Perc_Forest', \n",
    "                'Perc_Develop', \n",
    "                'Perc_Imperv', \n",
    "                'Perc_Herbace',\n",
    "                'Perc_Slop_30', \n",
    "                'Mean_Ann_Precip_in',\n",
    "                's1',\n",
    "                's2', \n",
    "                'storage', \n",
    "                'swe', \n",
    "                'NWM_flow', \n",
    "                'DOY']\n",
    "\n",
    "target = ['flow_cfs']\n",
    "\n",
    "scalername_x = \"All_feat_scaler_x.save\"\n",
    "scalername_y = \"scaler_y.save\"\n",
    "\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "x_scaler_path = f\"{modelpath}/{scalername_x}\"\n",
    "y_scaler_path = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "lookback = 5\n",
    "test_years = [2019,2020]\n",
    "\n",
    "#split data into train/test - note, for LSTM there is not need to randomize the split - hmmmmmmmmm\n",
    "#train_type either ratio or hold out\n",
    "x_train_scaled_t, X_test_dic, y_train_scaled_t, y_test_dic = lstm_dataprocessing.Multisite_DataProcessing(df, \n",
    "                                                                                   input_columns, \n",
    "                                                                                   target, \n",
    "                                                                                   lookback, \n",
    "                                                                                   test_years, \n",
    "                                                                                   x_scaler_path, \n",
    "                                                                                   y_scaler_path, \n",
    "                                                                                   random_shuffle = True)\n",
    "\n",
    "print(x_train_scaled_t.shape, y_train_scaled_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "* add lookback\n",
    "* make the model a .py file and class when finalized. PyTorch only saves the weights of the layer/node, not the overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666548fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3212985474723602\n",
      "Epoch 2/5, Loss: 0.31936510849759414\n",
      "Epoch 3/5, Loss: 0.3193691355210763\n",
      "Epoch 4/5, Loss: 0.3193684895680029\n",
      "Epoch 5/5, Loss: 0.3193681662925232\n",
      "finish\n",
      "Run Time: 0.4882497310638428 minutes \n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming you have your data loaded into NumPy arrays as x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, x_scaled, y_scaled\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 300\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_scaled_t, y_train_scaled_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False ) # might shuffle this\n",
    "\n",
    "# Build the model, this needs to be a class, see LSTM-Tutorials\n",
    "model = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay) #\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    fc.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "\n",
    "        output, _ = model(batch_x)\n",
    "        output = fc(output[:, -1, :])\n",
    "        loss = criterion(output, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s minutes \" % ((time.time() - start_time)/60))\n",
    "#save model - https://pytorch.org/tutorials/beginner/saving_loading_models.html, for a more efficient way to save... need to make the model as a class and we save the class...\n",
    "if os.path.exists(model_path) == False:\n",
    "    os.mkdir(model_path)\n",
    "torch.save(model.state_dict(), f\"{model_path}/{modelname}_model.pkl\")\n",
    "torch.save(fc.state_dict(), f\"{model_path}/{modelname}_model_fc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc059ce0",
   "metadata": {},
   "source": [
    "## Load the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and load the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "\n",
    "# Build the model\n",
    "model_P = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc_P = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "#this requires the model structure to be preloaded\n",
    "model_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model.pkl\"))\n",
    "fc_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model_fc.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1a412",
   "metadata": {},
   "source": [
    "# Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get annual supply diffs\n",
    "cfsday_AFday = 1.983\n",
    "year = 2020\n",
    "\n",
    "model_P = model_P.to(device)\n",
    "fc_P = fc_P.to(device)\n",
    "\n",
    "\n",
    "Preds_Dict = {}\n",
    "for station_number in station_index_list.drop_duplicates():\n",
    "  index = station_index_list == station_number\n",
    "  X_test = x_test_temp_1[index]\n",
    "  X_test_scaled_t = torch.Tensor(x_test_1_scaled[index]).unsqueeze(1)\n",
    "  X_test_scaled_t = X_test_scaled_t.to(device)\n",
    "  l = len(y_test_temp_1.values)\n",
    "  y_test = torch.Tensor(np.array(y_test_temp_1.values).reshape(l,1))\n",
    "  y_test = y_test.to(device)\n",
    "\n",
    "  # Evaluation\n",
    "  model_P.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions_scaled, _ = model_P(X_test_scaled_t)\n",
    "    predictions_scaled = fc_P(predictions_scaled[:, -1, :])\n",
    "\n",
    "  # Invert scaling for actual\n",
    "  predictions = scaler_y.inverse_transform(predictions_scaled.to('cpu').numpy())\n",
    "  predictions[predictions<0] = 0\n",
    "  predictions = pd.DataFrame(predictions, columns=[f\"{modelname}_flow\"])\n",
    "\n",
    "  #save predictions, need to convert to NHDPlus reach - Need to add Datetime column and flow predictions\n",
    "  #make daterange\n",
    "  dates = pd.date_range(pd.to_datetime(\"2020-01-01\"), periods=len(predictions)).strftime(\"%Y-%m-%d\").tolist()\n",
    "  predictions['Datetime'] = dates\n",
    "    \n",
    "  #get reach id for model eval\n",
    "  nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "  nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "\n",
    "  #put columns in correct order\n",
    "  cols = ['Datetime', f\"{modelname}_flow\"]\n",
    "  predictions = predictions[cols]\n",
    "\n",
    "  #save predictions to AWS so we can use CSES\n",
    "  state = StreamStats['state_id'][StreamStats['NWIS_site_id'].astype(str)== station_number].values[0].lower()\n",
    "  csv_key = f\"{modelname}/NHD_segments_{state}.h5/{modelname[:3]}_{nhdreach}.csv\"\n",
    "  predictions.to_csv(f\"s3://{BUCKET_NAME}/{csv_key}\", index = False,  storage_options={'key': ACCESS['Access key ID'][0],\n",
    "                           'secret': ACCESS['Secret access key'][0]})\n",
    "\n",
    "  #Concat DFS and put into dictionary\n",
    "  x_test_temp['nwm_feature_id'] = nhdreach\n",
    "  Dfs = [predictions.reset_index(drop=True),x_test_temp[x_test_temp['station_id']==station_number].reset_index(drop=True)]\n",
    "  Preds_Dict[station_number] = pd.concat(Dfs, axis=1)\n",
    "\n",
    "  #reorganize columns\n",
    "  Preds_Dict[station_number].pop('datetime')\n",
    "  Preds_Dict[station_number].insert(1, f\"{modelname}_flow\", Preds_Dict[station_number].pop(f\"{modelname}_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"NWM_flow\", Preds_Dict[station_number].pop(\"NWM_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"flow_cfs\", Preds_Dict[station_number].pop(\"flow_cfs\"))\n",
    "  Preds_Dict[station_number].insert(1, \"nwm_feature_id\", Preds_Dict[station_number].pop(\"nwm_feature_id\"))\n",
    "  Preds_Dict[station_number].insert(1, \"station_id\", Preds_Dict[station_number].pop(\"station_id\"))\n",
    "\n",
    "  #push data to AWS so we can use CSES\n",
    "  \n",
    "  \n",
    "#save predictions as compressed pkl file\n",
    "pred_path = f\"{HOME}/NWM_ML/Predictions/Hindcast/{modelname}/{year}\"\n",
    "file_path = f\"{pred_path}/{modelname}_predictions.pkl\"\n",
    "if os.path.exists(pred_path) == False:\n",
    "  os.makedirs(pred_path)\n",
    "\n",
    "with open(file_path, 'wb') as handle:\n",
    "  pkl.dump(Preds_Dict, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preds_Dict['10105900']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc5cb-9850-4535-a180-e0499407265e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3481cdd-b452-42b7-a4bd-c8e4908eab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
