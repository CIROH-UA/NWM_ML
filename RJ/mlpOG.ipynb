{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "MLP development script\n"
     ]
    }
   ],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '..') #sys allows for the .ipynb file to connect to the shared folder files\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "modelname = \"MLP\"\n",
    "\n",
    "print(f\"{modelname} development script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2   \n",
       "1         67.7           1.2         0.12          2.94          27.2   \n",
       "2         67.7           1.2         0.12          2.94          27.2   \n",
       "3         67.7           1.2         0.12          2.94          27.2   \n",
       "4         67.7           1.2         0.12          2.94          27.2   \n",
       "\n",
       "   Mean_Ann_Precip_in    datetime  flow_cfs        s1        s2  storage  swe  \\\n",
       "0                34.8  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2   \n",
       "1                34.8  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2   \n",
       "2                34.8  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1   \n",
       "3                34.8  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2   \n",
       "4                34.8  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      55.0  301  \n",
       "1      55.0  302  \n",
       "2      54.0  303  \n",
       "3      54.0  304  \n",
       "4      54.0  305  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get streamstats data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "file = \"Streamstats.csv\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = 'Streamstats/Streamstats.csv'      \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "\n",
    "#Get processed training data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/Processed\"\n",
    "file = \"raw_training_data.parquet\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = \"NWM_ML\"+datapath.split(\"NWM_ML\",1)[1]+'/'+file       \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    raw_training_data = pd.read_parquet(filepath)\n",
    "\n",
    "raw_training_data.pop('Unnamed: 0')\n",
    "raw_training_data['station_id'] = raw_training_data['station_id'].astype('str')\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8b8226-79f9-4f8d-8528-8989050bbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128879, 1)\n",
      "(128879, 12)\n"
     ]
    }
   ],
   "source": [
    "# Editing the features based on the feature importance should be done here!!!!!!!!!!!!!!!\n",
    "Training_DF = raw_training_data.copy()\n",
    "Training_DF.drop(['Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest',\n",
    "                        'Mean_Basin_Elev_ft'], axis=1, inplace=True)\n",
    "\n",
    "#remove headwater stations\n",
    "headwater_stations = ['10011500', # Bear River headwaters before WY state line\n",
    "                      '10109000', # Logan River above dams\n",
    "                      '10113500', # HW Blacksmith fork\n",
    "                      '10128500', # Upper Weber above Oakley\n",
    "                      '10131000', #Chalk creek before Weber - lots of upstream irrigation, potentially include\n",
    "                        '10146400', #Currant Creek above Mona Reservoir - lots of upstream irrigation, potentially include\n",
    "                        '10150500', #Spanish fork after diamond fork - potentially include because of 6th water diversion CUP\n",
    "                        '10154200', #Upper Provo river after confluence of N/S forks - potentially include because of duchense tunnel water diversion CUP\n",
    "                        '10172700', #Vernon creek 2 ranges west of Utah Lake, shouldnt be included because not in GSL basin \n",
    "                        '10172800', #Willow creek west of Gransville,  shouldnt be included because does not make it to GSL\n",
    "                          '10172952'] #Dunn creek in Raft River Range, shouldnt be included because drains to bonnevile salt flats \n",
    "Training_DF = Training_DF[~raw_training_data['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "Training_DF.datetime = pd.to_datetime(Training_DF.datetime)\n",
    "\n",
    "#Select training data - testing is going to be done on 2020\n",
    "x_train_temp = Training_DF[Training_DF.datetime.dt.year != 2020]\n",
    "x_train_temp.pop('station_id')\n",
    "x_train_temp.pop('datetime')\n",
    "y_train_temp = x_train_temp['flow_cfs']\n",
    "x_train_temp.pop('flow_cfs')\n",
    "\n",
    "#Convert dataframe to numpy, scale, save scalers\n",
    "y_train = y_train_temp.to_numpy()\n",
    "x_train = x_train_temp.to_numpy()\n",
    "\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "if not os.path.exists(modelpath):\n",
    "    os.makedirs(modelpath, exist_ok=True)\n",
    "\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "joblib.dump(scaler, scalerfilepath_x)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "joblib.dump(scaler, scalerfilepath_y)  \n",
    "print(y_scaled_train.shape)\n",
    "print(x_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a73375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>41.575490</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.20</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>41.575490</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.20</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>41.575490</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.20</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>41.575490</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.20</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>41.575490</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.20</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179201</th>\n",
       "      <td>40.733557</td>\n",
       "      <td>-111.92327</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>4.3700</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3309.0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179202</th>\n",
       "      <td>40.733557</td>\n",
       "      <td>-111.92327</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>4.3700</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3311.0</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179203</th>\n",
       "      <td>40.733557</td>\n",
       "      <td>-111.92327</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>4.3700</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3313.0</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179204</th>\n",
       "      <td>40.733557</td>\n",
       "      <td>-111.92327</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>4.3700</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3315.0</td>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179205</th>\n",
       "      <td>40.733557</td>\n",
       "      <td>-111.92327</td>\n",
       "      <td>3430.0</td>\n",
       "      <td>14.70</td>\n",
       "      <td>4.3700</td>\n",
       "      <td>4.94</td>\n",
       "      <td>-0.829038</td>\n",
       "      <td>0.559193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3317.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128879 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Lat       Long  Drainage_area_mi2  Perc_Develop  Perc_Imperv  \\\n",
       "3079    41.575490 -111.85522              180.0          1.01       0.0653   \n",
       "3080    41.575490 -111.85522              180.0          1.01       0.0653   \n",
       "3081    41.575490 -111.85522              180.0          1.01       0.0653   \n",
       "3082    41.575490 -111.85522              180.0          1.01       0.0653   \n",
       "3083    41.575490 -111.85522              180.0          1.01       0.0653   \n",
       "...           ...        ...                ...           ...          ...   \n",
       "179201  40.733557 -111.92327             3430.0         14.70       4.3700   \n",
       "179202  40.733557 -111.92327             3430.0         14.70       4.3700   \n",
       "179203  40.733557 -111.92327             3430.0         14.70       4.3700   \n",
       "179204  40.733557 -111.92327             3430.0         14.70       4.3700   \n",
       "179205  40.733557 -111.92327             3430.0         14.70       4.3700   \n",
       "\n",
       "        Perc_Slop_30        s1        s2  storage  swe  NWM_flow  DOY  \n",
       "3079           44.20 -0.891007 -0.453991      0.0  0.0      37.0  275  \n",
       "3080           44.20 -0.891007 -0.453991      0.0  0.0      36.0  276  \n",
       "3081           44.20 -0.891007 -0.453991      0.0  0.0      36.0  277  \n",
       "3082           44.20 -0.891007 -0.453991      0.0  0.0      36.0  278  \n",
       "3083           44.20 -0.891007 -0.453991      0.0  0.0      36.0  279  \n",
       "...              ...       ...       ...      ...  ...       ...  ...  \n",
       "179201          4.94 -0.829038  0.559193      0.0  0.0    3309.0  361  \n",
       "179202          4.94 -0.829038  0.559193      0.0  0.0    3311.0  362  \n",
       "179203          4.94 -0.829038  0.559193      0.0  0.0    3313.0  363  \n",
       "179204          4.94 -0.829038  0.559193      0.0  0.0    3315.0  364  \n",
       "179205          4.94 -0.829038  0.559193      0.0  0.0    3317.0  365  \n",
       "\n",
       "[128879 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62296d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527f9cda",
   "metadata": {},
   "source": [
    "### Set up Testing year\n",
    "* Select year(s) not used in training\n",
    "* Convert to numpy array\n",
    "* Load scaler and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd46d43cce5d1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:44.100124Z",
     "start_time": "2023-11-10T04:40:44.086373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5473, 1)\n",
      "(5473, 12)\n"
     ]
    }
   ],
   "source": [
    "#Get water year for testing from larger dataset\n",
    "x_test_temp = Training_DF[Training_DF.datetime.dt.year == 2020]\n",
    "x_test_temp_1 = x_test_temp.copy()\n",
    "station_index_list = x_test_temp_1['station_id']\n",
    "x_test_temp_1.pop('station_id')\n",
    "x_test_temp_1.pop('datetime')\n",
    "\n",
    "#Get target variable (y) and convert to numpy arrays\n",
    "y_test_temp_1 = x_test_temp_1['flow_cfs']\n",
    "x_test_temp_1.pop('flow_cfs')\n",
    "x_test_1_np = x_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "y_test_1_np = y_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "\n",
    "#load scalers and scale\n",
    "scalername_x = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_x.save\"\n",
    "scalername_y = \"Area_Perc_Seas_stor_swe_NWM_DOY_scaler_y.save\"\n",
    "modelpath = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "scalerfilepath_x = f\"{modelpath}/{scalername_x}\"\n",
    "scalerfilepath_y = f\"{modelpath}/{scalername_y}\"\n",
    "\n",
    "#load scalers\n",
    "scaler_x = joblib.load(scalerfilepath_x)\n",
    "scaler_y = joblib.load(scalerfilepath_y)\n",
    "\n",
    "#scale the testing data\n",
    "x_test_1_scaled = scaler_x.fit_transform(x_test_1_np)\n",
    "y_scaled_test_1 = scaler_y.fit_transform(y_test_1_np.reshape(-1, 1))\n",
    "print(y_scaled_test_1.shape)\n",
    "print(x_test_1_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16349b",
   "metadata": {},
   "source": [
    "### Set up model training framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0434049b0d18cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:49.443442Z",
     "start_time": "2023-11-10T04:40:49.440536Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% MLP\n",
    "\n",
    "n_targets = 1\n",
    "tries = 10\n",
    "#model performance metrics\n",
    "cri_temp_nse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_rmse = np.zeros([3, n_targets, tries])\n",
    "cri_temp_r2 = np.zeros([3, n_targets, tries])\n",
    "cri_temp_kge = np.zeros([3, n_targets, tries])\n",
    "cri_temp_lognse = np.zeros([3, n_targets, tries])\n",
    "\n",
    "# Convert to tensor for PyTorch\n",
    "x_train_scaled_t = torch.Tensor(x_train_scaled)\n",
    "y_train_scaled_t = torch.Tensor(y_scaled_train)\n",
    "#Make sure the tensors on are the respective device (cpu/gpu)\n",
    "x_train_scaled_t = x_train_scaled_t.to(device)\n",
    "y_train_scaled_t = y_train_scaled_t.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "447219f501a987b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:50:15.960043Z",
     "start_time": "2023-11-10T04:40:59.662734Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.0012856289789382174\n",
      "Epoch 2/20, Loss: 0.001025355682417852\n",
      "Epoch 3/20, Loss: 0.0008956932600489864\n",
      "Epoch 4/20, Loss: 0.000903131655158326\n",
      "Epoch 5/20, Loss: 0.0006740855669195825\n",
      "Epoch 6/20, Loss: 0.0005337522594036082\n",
      "Epoch 7/20, Loss: 0.0005204764505290946\n",
      "Epoch 8/20, Loss: 0.0006055072047146098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     50\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "File \u001b[0;32m~/envs/PyTorch_GPU_env/lib/python3.11/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 150\n",
    "LD1=128\n",
    "LD2=128\n",
    "LD3=64\n",
    "LD4=64\n",
    "LD5=32\n",
    "LD6=16\n",
    "LD7=5\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_scaled_t, y_train_scaled_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False )\n",
    "\n",
    "# Build the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(12, LD1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD1, LD2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD2, LD3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD3, LD4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD4, LD5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD5, LD6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD6, 1)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))\n",
    "\n",
    "#save model\n",
    "if os.path.exists(model_path) == False:\n",
    "    os.makedirs(model_path)\n",
    "torch.save(model.state_dict(), f\"{model_path}/{modelname}_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc059ce0",
   "metadata": {},
   "source": [
    "## Load the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bab945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and load the model\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "decay = 1e-2\n",
    "validation_split = 0.2\n",
    "neurons = 150\n",
    "LD1=128\n",
    "LD2=128\n",
    "LD3=64\n",
    "LD4=64\n",
    "LD5=32\n",
    "LD6=16\n",
    "LD7=5\n",
    "\n",
    "#device = torch.device('cpu') # for some reason had to change to cpu\n",
    "models = nn.Sequential(\n",
    "    nn.Linear(12, LD1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD1, LD2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD2, LD3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD3, LD4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD4, LD5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD5, LD6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(LD6, 1)\n",
    ").to(device)\n",
    "\n",
    "models.load_state_dict(torch.load(f\"{model_path}/{modelname}_model.pkl\"))\n",
    "\n",
    "#put the model scores into a dataframe for comparison\n",
    "#Evaluation columns for prediction time series\n",
    "cols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{modelname}_rmse\", 'NWM_pbias', f\"{modelname}_pbias\", \n",
    "        'NWM_kge', f\"{modelname}__kge\", 'NWM_mape',  f\"{modelname}_mape\"]\n",
    "\n",
    "#Evaluation columns for accumulated supply time series\n",
    "supcols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{modelname}_rmse\", 'NWM_pbias', f\"{modelname}_pbias\", \n",
    "        'NWM_kge', f\"{modelname}__kge\", 'NWM_mape',  f\"{modelname}_mape\", 'Obs_vol', 'NWM_vol', f\"{modelname}_vol\",\n",
    "        'NWM_vol_err', f\"{modelname}_vol_err\", 'NWM_vol_Perc_diff', f\"{modelname}_vol_Perc_diff\"]\n",
    "\n",
    "\n",
    "EvalDF = pd.DataFrame(columns = cols)\n",
    "SupplyEvalDF = pd.DataFrame(columns = supcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fef2f",
   "metadata": {},
   "source": [
    "# Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432359e3ae01a43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:50:18.893916Z",
     "start_time": "2023-11-10T04:50:18.778135Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get annual supply diffs\n",
    "cfsday_AFday = 1.983\n",
    "year = 2020\n",
    "\n",
    "\n",
    "Preds_Dict = {}\n",
    "for station_number in station_index_list.drop_duplicates():\n",
    "  #print(station_number)\n",
    "  index = station_index_list == station_number\n",
    "  X_test = x_test_temp_1[index]\n",
    "  X_test_scaled_t = torch.Tensor(x_test_1_scaled[index])\n",
    "  X_test_scaled_t = X_test_scaled_t.to(device)\n",
    "  l = len(y_test_temp_1.values)\n",
    "  y_test = torch.Tensor(np.array(y_test_temp_1.values).reshape(l,1))\n",
    "  y_test = y_test.to(device)\n",
    "\n",
    "  # Evaluation\n",
    "  models.eval()\n",
    "  with torch.no_grad():\n",
    "      predictions_scaled= models(X_test_scaled_t)\n",
    "\n",
    "  # Invert scaling for actual\n",
    "  predictions = scaler_y.inverse_transform(predictions_scaled.to('cpu').numpy())\n",
    "  predictions[predictions<0] = 0\n",
    "\n",
    "  #print('Model Predictions complete')\n",
    "\n",
    "  predictions = pd.DataFrame(predictions, columns=[f\"{modelname}_flow\"])\n",
    "\n",
    "  #save predictions, need to convert to NHDPlus reach - Need to add Datetime column and flow predictions\n",
    "  #make daterange\n",
    "  dates = pd.date_range(pd.to_datetime(\"2020-01-01\"), periods=len(predictions)).strftime(\"%Y-%m-%d\").tolist()\n",
    "  predictions['Datetime'] = dates\n",
    "    \n",
    "  #get reach id for model eval\n",
    "  nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "  nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "\n",
    "  #put columns in correct order\n",
    "  cols = ['Datetime', f\"{modelname}_flow\"]\n",
    "  predictions = predictions[cols]\n",
    "\n",
    "  #save predictions to AWS so we can use CSES\n",
    "  state = StreamStats['state_id'][StreamStats['NWIS_site_id'].astype(str)== station_number].values[0].lower()\n",
    "  csv_key = f\"{modelname}/NHD_segments_{state}.h5/{modelname[:3]}_{nhdreach}.csv\"\n",
    "  predictions.to_csv(f\"s3://{BUCKET_NAME}/{csv_key}\", index = False,  storage_options={'key': ACCESS['Access key ID'][0],\n",
    "                           'secret': ACCESS['Secret access key'][0]})\n",
    "\n",
    "  #Concat DFS and put into dictionary\n",
    "  x_test_temp['nwm_feature_id'] = nhdreach\n",
    "  Dfs = [predictions.reset_index(drop=True),x_test_temp[x_test_temp['station_id']==station_number].reset_index(drop=True)]\n",
    "  Preds_Dict[station_number] = pd.concat(Dfs, axis=1)\n",
    "\n",
    "  #reorganize columns\n",
    "  Preds_Dict[station_number].pop('datetime')\n",
    "  Preds_Dict[station_number].insert(1, f\"{modelname}_flow\", Preds_Dict[station_number].pop(f\"{modelname}_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"NWM_flow\", Preds_Dict[station_number].pop(\"NWM_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"flow_cfs\", Preds_Dict[station_number].pop(\"flow_cfs\"))\n",
    "  Preds_Dict[station_number].insert(1, \"nwm_feature_id\", Preds_Dict[station_number].pop(\"nwm_feature_id\"))\n",
    "  Preds_Dict[station_number].insert(1, \"station_id\", Preds_Dict[station_number].pop(\"station_id\"))\n",
    "\n",
    "  #push data to AWS so we can use CSES\n",
    "  \n",
    "  \n",
    "#save predictions as compressed pkl file\n",
    "pred_path = f\"{HOME}/NWM_ML/Predictions/Hindcast/{modelname}/{year}\"\n",
    "file_path = f\"{pred_path}/{modelname}_predictions.pkl\"\n",
    "if os.path.exists(pred_path) == False:\n",
    "    os.makedirs(pred_path)\n",
    "with open(file_path, 'wb') as handle:\n",
    "    pkl.dump(Preds_Dict, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855ed6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>station_id</th>\n",
       "      <th>nwm_feature_id</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>MLP_flow</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.583889</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>35.578308</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.515900</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.510326</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>35.504749</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>2020-09-19</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>40.438568</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>2020-09-20</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.336639</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>2020-09-21</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.302391</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>2020-09-22</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.017273</td>\n",
       "      <td>62.0</td>\n",
       "      <td>40.268143</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2020-09-23</td>\n",
       "      <td>10157500</td>\n",
       "      <td>10375690</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>40.166210</td>\n",
       "      <td>40.460789</td>\n",
       "      <td>-111.472687</td>\n",
       "      <td>49.8</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>49.1</td>\n",
       "      <td>-0.529919</td>\n",
       "      <td>-0.848048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Datetime station_id  nwm_feature_id  flow_cfs  NWM_flow   MLP_flow  \\\n",
       "0    2020-01-01   10157500        10375690  0.225000      49.0  35.583889   \n",
       "1    2020-01-02   10157500        10375690  0.270000      49.0  35.578308   \n",
       "2    2020-01-03   10157500        10375690  0.275000      48.0  35.515900   \n",
       "3    2020-01-04   10157500        10375690  0.250000      48.0  35.510326   \n",
       "4    2020-01-05   10157500        10375690  0.285000      48.0  35.504749   \n",
       "..          ...        ...             ...       ...       ...        ...   \n",
       "262  2020-09-19   10157500        10375690  0.010000      63.0  40.438568   \n",
       "263  2020-09-20   10157500        10375690  0.010000      62.0  40.336639   \n",
       "264  2020-09-21   10157500        10375690  0.010833      62.0  40.302391   \n",
       "265  2020-09-22   10157500        10375690  0.017273      62.0  40.268143   \n",
       "266  2020-09-23   10157500        10375690  0.010000      61.0  40.166210   \n",
       "\n",
       "           Lat        Long  Drainage_area_mi2  Perc_Develop  Perc_Imperv  \\\n",
       "0    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "1    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "2    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "3    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "4    40.460789 -111.472687               49.8          2.37         0.16   \n",
       "..         ...         ...                ...           ...          ...   \n",
       "262  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "263  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "264  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "265  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "266  40.460789 -111.472687               49.8          2.37         0.16   \n",
       "\n",
       "     Perc_Slop_30        s1        s2  storage  swe  DOY  \n",
       "0            49.1 -0.438371  0.898794      0.0  0.0    1  \n",
       "1            49.1 -0.438371  0.898794      0.0  0.0    2  \n",
       "2            49.1 -0.438371  0.898794      0.0  0.0    3  \n",
       "3            49.1 -0.438371  0.898794      0.0  0.0    4  \n",
       "4            49.1 -0.438371  0.898794      0.0  0.0    5  \n",
       "..            ...       ...       ...      ...  ...  ...  \n",
       "262          49.1 -0.529919 -0.848048      0.0  0.0  270  \n",
       "263          49.1 -0.529919 -0.848048      0.0  0.0  271  \n",
       "264          49.1 -0.529919 -0.848048      0.0  0.0  272  \n",
       "265          49.1 -0.529919 -0.848048      0.0  0.0  273  \n",
       "266          49.1 -0.529919 -0.848048      0.0  0.0  274  \n",
       "\n",
       "[267 rows x 17 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preds_Dict['10157500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328b1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
