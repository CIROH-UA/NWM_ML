{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '../..')  #sys allows for the .ipynb file to connect to the shared folder files\n",
    "from shared_scripts import lstm_dataprocessing, Simple_Eval\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6d2a7",
   "metadata": {},
   "source": [
    "## Asign key variables for use throughout script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09aadb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "LSTM development script\n"
     ]
    }
   ],
   "source": [
    "modelname = 'LSTM'\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "cfsday_AFday = 1.983\n",
    "print(f\"{modelname} development script\")\n",
    "\n",
    "#input columns\n",
    "input_columns =[\n",
    "                # 'Lat', \n",
    "                # 'Long', \n",
    "                'Drainage_area_mi2', \n",
    "                'Mean_Basin_Elev_ft',       \n",
    "                'Perc_Forest', \n",
    "                'Perc_Develop', \n",
    "                'Perc_Imperv', \n",
    "                'Perc_Herbace',       \n",
    "                'Perc_Slop_30', \n",
    "                'Mean_Ann_Precip_in', \n",
    "                's1',       \n",
    "                's2', \n",
    "                'storage', \n",
    "                'swe', # this needs to be redone for catchments, Using in american fork and there is no SWE observations and there should be!\n",
    "                'NWM_flow', \n",
    "                'DOY', \n",
    "                'tempe(F)', \n",
    "                'precip(mm)'\n",
    "                ]\n",
    "\n",
    "target = ['flow_cfs']\n",
    "\n",
    "scalername_x = \"scaler_x.save\"\n",
    "scalername_y = \"scaler_y.save\"\n",
    "x_scaler_path = f\"{model_path}/{scalername_x}\"\n",
    "y_scaler_path = f\"{model_path}/{scalername_y}\"\n",
    "\n",
    "lookback = 100\n",
    "test_years = [2019,2020]\n",
    "\n",
    "#headwater stations for removeal\n",
    "headwater_stations = [\n",
    "                      '10011500', # Bear River headwaters before WY state line\n",
    "                      '10109000', # Logan River above dams\n",
    "                      '10113500', # HW Blacksmith fork\n",
    "                      '10128500', # Upper Weber above Oakley\n",
    "                      '10131000', #Chalk creek before Weber - lots of upstream irrigation, potentially include\n",
    "                      '10146400', #Currant Creek above Mona Reservoir - lots of upstream irrigation, potentially include\n",
    "                      '10150500', #Spanish fork after diamond fork - potentially include because of 6th water diversion CUP\n",
    "                      '10154200', #Upper Provo river after confluence of N/S forks - potentially include because of duchense tunnel water diversion CUP\n",
    "                      '10172700', #Vernon creek 2 ranges west of Utah Lake, shouldnt be included because not in GSL basin \n",
    "                      '10172800', #Willow creek west of Gransville,  shouldnt be included because does not make it to GSL\n",
    "                      '10172952',#Dunn creek in Raft River Range, shouldnt be included because drains to bonnevile salt flats \n",
    "                      '10164500', # American fork by powerplant, removing for now because now swe or upstream storage, which it should because it has two reservoirs upstream\n",
    "\n",
    "                      '10105900', #no swe or storage!!!!!\n",
    "                      #'10126000',  #have swe and storage\n",
    "                      '10129900',  #no swe or storage!!!!!\n",
    "                      '10133650', #no swe or storage!!!!!\n",
    "                      '10133800', #no swe or storage!!!!!\n",
    "                      '10133980', #no swe or storage!!!!!\n",
    "                       #'10134500',      #have swe and storage \n",
    "                      #'10136500', #have swe and storage\n",
    "                      '10137500', #no swe or storage!!!!!\n",
    "                      '10140100', #no swe or storage!!!!!\n",
    "                      #'10140700', #have swe and storage\n",
    "                      #'10141000',     #have swe and storage  \n",
    "                      '10145400', #no swe or storage!!!!!\n",
    "                      '10149000', #no swe or storage!!!!!\n",
    "                      '10153100',#no swe or storage!!!!!\n",
    "                      '10155000', #no swe or storage!!!!!\n",
    "                      #'10155200', #have swe and storage\n",
    "                      '10157500', #no swe or storage!!!!!\n",
    "       ] \n",
    "\n",
    "\n",
    "#The following sites have swe \n",
    "\n",
    "'''\n",
    "['10011500', '10105900', '10109000', '10126000', '10131000',\n",
    "       '10133650', '10133800', '10133980', '10134500', '10136500',\n",
    "       '10140700', '10141000', '10150500', '10154200', '10155000',\n",
    "       '10155200']\n",
    "'''\n",
    "\n",
    "#the following sites have swe and storage\n",
    "'''\n",
    "['10126000', '10134500', '10136500', '10140700', '10141000',\n",
    "       '10155200']\n",
    "'''\n",
    "                          \n",
    "\n",
    "pred_path = f\"{HOME}/NWM_ML/Predictions/Hindcast/{modelname}/Multilocation\"\n",
    "file_path = f\"{pred_path}/{modelname}_predictions.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed2c232",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* establish lookback\n",
    "* figure out how to randomize training data\n",
    "* add new dataset\n",
    "* import Evaluation_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df needs no processing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "      <th>tempe(F)</th>\n",
       "      <th>precip(mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "      <td>39.239582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "      <td>45.068712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "      <td>50.945891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "      <td>45.480097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "      <td>46.656777</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  ...  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "1         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "2         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "3         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "4         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "\n",
       "     datetime  flow_cfs        s1        s2  storage  swe  NWM_flow  DOY  \\\n",
       "0  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2      55.0  301   \n",
       "1  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2      55.0  302   \n",
       "2  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1      54.0  303   \n",
       "3  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2      54.0  304   \n",
       "4  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2      54.0  305   \n",
       "\n",
       "    tempe(F)  precip(mm)  \n",
       "0  39.239582         0.0  \n",
       "1  45.068712         0.0  \n",
       "2  50.945891         0.0  \n",
       "3  45.480097         0.0  \n",
       "4  46.656777         0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get streamstats data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "file = \"Streamstats.csv\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = 'Streamstats/Streamstats.csv'      \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    StreamStats = pd.read_csv(filepath)\n",
    "\n",
    "#Get processed training data \n",
    "datapath = f\"{HOME}/NWM_ML/Data/Processed\"\n",
    "# file = \"raw_training_data.parquet\"  #note, this file has no swe or storage, all 0s\n",
    "file = \"final_input.parquet\"\n",
    "filepath = f\"{datapath}/{file}\"\n",
    "try:\n",
    "    df = pd.read_parquet(filepath)\n",
    "except:\n",
    "    print(\"Data not found, retreiving from AWS S3\")\n",
    "    if not os.path.exists(datapath):\n",
    "        os.makedirs(datapath, exist_ok=True)\n",
    "    key = \"NWM_ML\"+datapath.split(\"NWM_ML\",1)[1]+'/'+file       \n",
    "    S3.meta.client.download_file(BUCKET_NAME, key,filepath)\n",
    "    df = pd.read_parquet(filepath)\n",
    "\n",
    "try:\n",
    "    df.pop('Unnamed: 0')\n",
    "except:\n",
    "    print('df needs no processing')\n",
    "df['station_id'] = df['station_id'].astype('str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "814add4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "      <th>tempe(F)</th>\n",
       "      <th>precip(mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10126000</td>\n",
       "      <td>41.576321</td>\n",
       "      <td>-112.100782</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>1162.60410</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>34.071895</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1183.0</td>\n",
       "      <td>301</td>\n",
       "      <td>41.423048</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10126000</td>\n",
       "      <td>41.576321</td>\n",
       "      <td>-112.100782</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>1152.39590</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>34.673203</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>302</td>\n",
       "      <td>52.055238</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10126000</td>\n",
       "      <td>41.576321</td>\n",
       "      <td>-112.100782</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>1036.51040</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>35.346405</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>303</td>\n",
       "      <td>51.981724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10126000</td>\n",
       "      <td>41.576321</td>\n",
       "      <td>-112.100782</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>680.29170</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>36.039216</td>\n",
       "      <td>1.078571</td>\n",
       "      <td>1179.0</td>\n",
       "      <td>304</td>\n",
       "      <td>43.891912</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10126000</td>\n",
       "      <td>41.576321</td>\n",
       "      <td>-112.100782</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>6620.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.2</td>\n",
       "      <td>1.94</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>557.59375</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>36.699346</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>305</td>\n",
       "      <td>45.246638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10126000  41.576321 -112.100782             7040.0              6620.0   \n",
       "1   10126000  41.576321 -112.100782             7040.0              6620.0   \n",
       "2   10126000  41.576321 -112.100782             7040.0              6620.0   \n",
       "3   10126000  41.576321 -112.100782             7040.0              6620.0   \n",
       "4   10126000  41.576321 -112.100782             7040.0              6620.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  ...  \\\n",
       "0         15.6          4.28         0.55          15.2          1.94  ...   \n",
       "1         15.6          4.28         0.55          15.2          1.94  ...   \n",
       "2         15.6          4.28         0.55          15.2          1.94  ...   \n",
       "3         15.6          4.28         0.55          15.2          1.94  ...   \n",
       "4         15.6          4.28         0.55          15.2          1.94  ...   \n",
       "\n",
       "    datetime    flow_cfs        s1        s2    storage       swe  NWM_flow  \\\n",
       "0 2010-10-28  1162.60410 -0.891007 -0.453991  34.071895  1.142857    1183.0   \n",
       "1 2010-10-29  1152.39590 -0.891007 -0.453991  34.673203  1.128571    1181.0   \n",
       "2 2010-10-30  1036.51040 -0.891007 -0.453991  35.346405  1.071429    1181.0   \n",
       "3 2010-10-31   680.29170 -0.891007 -0.453991  36.039216  1.078571    1179.0   \n",
       "4 2010-11-01   557.59375 -0.998630  0.052336  36.699346  1.100000    1175.0   \n",
       "\n",
       "   DOY   tempe(F)  precip(mm)  \n",
       "0  301  41.423048         0.0  \n",
       "1  302  52.055238         0.0  \n",
       "2  303  51.981724         0.0  \n",
       "3  304  43.891912         0.0  \n",
       "4  305  45.246638         0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove headwater stations\n",
    "df = df[~df['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "\n",
    "# #reset index to clean up df\n",
    "df.reset_index( inplace =  True, drop = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608eedb0",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "Scaling your data for ML is done for all types of applications and helps the model converge faster. \n",
    "If there is no scaling performed, the model will essentially be forced to think certain features are more important than others, rather than being able to learn those things.\n",
    "\n",
    "There are different ways you can scale the data, such as min-max or standard scaling; both of which are applicable for your model. \n",
    "If you know you have a fixed min and max in your dataset (e.g. images), you can use min-max scaling to fix your input and/or output data to be between 0 and 1.\n",
    "\n",
    "For other applications where you do not have fixed bounds, standard scaling is useful.\n",
    "This gives all of your features zero-mean and unit variance. Therefore, the distributions of inputs and/or outputs are the same, and the model can treat them as such.\n",
    "\n",
    "The scaling for your outputs is important in defining the activation function for the output layer.\n",
    "If you have min-max scaled outputs, you can use sigmoid, because it bounds the outputs to between 0 and 1. \n",
    "If you are using standard scaling for the outputs, you would want to be sure you use a linear activation function, because technically standard-scaled outputs are not bounded. \n",
    "The choice of output activation is important, and knowledge of how your outputs are scaled is important in determining which activation to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "185b2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25489, 100, 16]) torch.Size([25489])\n"
     ]
    }
   ],
   "source": [
    "#split data into train/test - note, for LSTM there is not need to randomize the split - hmmmmmmmmm\n",
    "#add scaler option - minmax standard\n",
    "x_train_scaled_t, X_test_dic, y_train_scaled_t, y_test_dic = lstm_dataprocessing.Multisite_DataProcessing(df, \n",
    "                                                                                   input_columns, \n",
    "                                                                                   target, \n",
    "                                                                                   lookback, \n",
    "                                                                                   test_years, \n",
    "                                                                                   x_scaler_path, \n",
    "                                                                                   y_scaler_path, \n",
    "                                                                                   random_shuffle = True) \n",
    "# maybe adjust to have x - timesteps in a row, way to only use lookback for timeseries changes (and different time series, e.g., stream/snow),\n",
    "#Different dataframe set up for static catchment features \n",
    "\n",
    "print(x_train_scaled_t.shape, y_train_scaled_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "* make the model a .py file and class when finalized. PyTorch only saves the weights of the layer/node, not the overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "666548fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.37652541537776246\n",
      "Epoch 2/5, Loss: 0.36117725498955366\n",
      "Epoch 3/5, Loss: 0.36111927322531473\n",
      "Epoch 4/5, Loss: 0.3610977425261308\n",
      "Epoch 5/5, Loss: 0.3610896081035677\n",
      "finish\n",
      "Run Time: 0.4342179377873739 minutes \n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "\n",
    "# Assuming you have your data loaded into NumPy arrays as x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, x_scaled, y_scaled\n",
    "# Hyperparameters\n",
    "epochs = 5 #early stopping - Savalan is lookin into it\n",
    "batch_size = 65\n",
    "learning_rate = 0.0001 #initial rate\n",
    "decay = 0.001 #reduces to \n",
    "validation_split = 0.2  #can add cross validation - Savalan is looking into it.\n",
    "neurons = 300\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "train_dataset = TensorDataset(x_train_scaled_t, y_train_scaled_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False ) # might shuffle this\n",
    "\n",
    "# Build the model, this needs to be a class, see LSTM-Tutorials #add layers w/respect to embedding, add cnns, embedding, MLPs...\n",
    "model = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "# Define loss and optimizer - change loss criterian (e.g. MSE), differnt optizers\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay) #\n",
    "\n",
    "# Training loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    fc.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "\n",
    "        output, _ = model(batch_x)\n",
    "        output = fc(output[:, -1, :])\n",
    "        loss = criterion(output, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s minutes \" % ((time.time() - start_time)/60))\n",
    "#save model - https://pytorch.org/tutorials/beginner/saving_loading_models.html, for a more efficient way to save... need to make the model as a class and we save the class...\n",
    "if os.path.exists(model_path) == False:\n",
    "    os.mkdir(model_path)\n",
    "torch.save(model.state_dict(), f\"{model_path}/{modelname}_model.pkl\")\n",
    "torch.save(fc.state_dict(), f\"{model_path}/{modelname}_model_fc.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1a412",
   "metadata": {},
   "source": [
    "# Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae73c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model_P = nn.LSTM(input_size=x_train_scaled_t.shape[2], hidden_size=neurons, bidirectional=True, batch_first=True).to(device)\n",
    "fc_P = nn.Linear(neurons * 2, 1).to(device)  # Multiply by 2 for bidirectional LSTM\n",
    "\n",
    "#this requires the model structure to be preloaded\n",
    "model_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model.pkl\"))\n",
    "fc_P.load_state_dict(torch.load(f\"{model_path}/{modelname}_model_fc.pkl\"))\n",
    "\n",
    "model_P = model_P.to(device)\n",
    "fc_P = fc_P.to(device)\n",
    "\n",
    "#load y scaler\n",
    "scaler_y = joblib.load(y_scaler_path)\n",
    "\n",
    "#get prediction locations\n",
    "station_index_list = list(X_test_dic.keys())\n",
    "\n",
    "#get dataframe for testing\n",
    "x_test_temp = df[df.datetime.dt.year.isin(test_years)]\n",
    "#x_test_temp.set_index('datetime', inplace = True)\n",
    "\n",
    "Preds_Dict = {}\n",
    "for station_number in station_index_list:\n",
    "  index = station_index_list = station_number\n",
    "\n",
    "  #X_test = x_test_temp_1[index]\n",
    "  # X_test_scaled_t = torch.Tensor(X_test_dic[index]).unsqueeze(1)\n",
    "  # X_test_scaled_t = X_test_scaled_t.to(device)\n",
    "  X_test_scaled_t = Variable(torch.from_numpy(X_test_dic[index]).float(), requires_grad=False).to(device)\n",
    "  y_test_scaled_t = Variable(torch.from_numpy(y_test_dic[index]).float(), requires_grad=False).to(device)\n",
    "\n",
    "  # l = len(y_test_scaled_t.values)\n",
    "  # y_test = torch.Tensor(np.array(y_test_temp_1.values).reshape(l,1))\n",
    "  # y_test = y_test.to(device)\n",
    "\n",
    "  # Evaluation\n",
    "  model_P.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions_scaled, _ = model_P(X_test_scaled_t)\n",
    "    predictions_scaled = fc_P(predictions_scaled[:, -1, :])\n",
    "\n",
    "  # Invert scaling for actual\n",
    "  predictions = scaler_y.inverse_transform(predictions_scaled.to('cpu').numpy())\n",
    "  predictions[predictions<0] = 0\n",
    "  predictions = pd.DataFrame(predictions, columns=[f\"{modelname}_flow\"])\n",
    "\n",
    "  #save predictions, need to convert to NHDPlus reach - Need to add Datetime column and flow predictions\n",
    "  #make daterange\n",
    "  dates = pd.date_range(pd.to_datetime(f\"{test_years[0]}-01-01\"), periods=len(predictions)).strftime(\"%Y-%m-%d\").tolist()\n",
    "  predictions['Datetime'] = dates\n",
    "    \n",
    "  #get reach id for model eval\n",
    "  nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "  nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "\n",
    "  #put columns in correct order\n",
    "  cols = ['Datetime', f\"{modelname}_flow\"]\n",
    "  predictions = predictions[cols]\n",
    "\n",
    "  #save predictions to AWS so we can use CSES\n",
    "  state = StreamStats['state_id'][StreamStats['NWIS_site_id'].astype(str)== station_number].values[0].lower()\n",
    "  csv_key = f\"{modelname}/NHD_segments_{state}.h5/{modelname[:3]}_{nhdreach}.csv\"\n",
    "  predictions.to_csv(f\"s3://{BUCKET_NAME}/{csv_key}\", index = False,  storage_options={'key': ACCESS['Access key ID'][0],\n",
    "                           'secret': ACCESS['Secret access key'][0]})\n",
    "\n",
    "  #Concat DFS and put into dictionary\n",
    "  x_test_temp['nwm_feature_id'] = nhdreach\n",
    "  Dfs = [predictions.reset_index(drop=True),x_test_temp[x_test_temp['station_id']==station_number].reset_index(drop=True)]\n",
    "  Preds_Dict[station_number] = pd.concat(Dfs, axis=1)\n",
    "\n",
    "  #reorganize columns\n",
    "  Preds_Dict[station_number].pop('datetime')\n",
    "  Preds_Dict[station_number].insert(1, f\"{modelname}_flow\", Preds_Dict[station_number].pop(f\"{modelname}_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"NWM_flow\", Preds_Dict[station_number].pop(\"NWM_flow\"))\n",
    "  Preds_Dict[station_number].insert(1, \"flow_cfs\", Preds_Dict[station_number].pop(\"flow_cfs\"))\n",
    "  Preds_Dict[station_number].insert(1, \"nwm_feature_id\", Preds_Dict[station_number].pop(\"nwm_feature_id\"))\n",
    "  Preds_Dict[station_number].insert(1, \"station_id\", Preds_Dict[station_number].pop(\"station_id\")) \n",
    "  Preds_Dict[station_number].set_index('Datetime', inplace = True)\n",
    "  \n",
    "#save predictions as compressed pkl file\n",
    "if os.path.exists(pred_path) == False:\n",
    "  os.makedirs(pred_path)\n",
    "\n",
    "with open(file_path, 'wb') as handle:\n",
    "  pkl.dump(Preds_Dict, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e26547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site: 10126000\n",
      "modeled flow min: 117.76563262939453\n",
      "modeled flow max: 128.08303833007812\n",
      "Obs flow min: 100.57917\n",
      "Obs flow max: 4331.146\n",
      "Storage min: 33.8562091503268\n",
      "Storage max: 97.62745098039215\n",
      "SWE min: 0.0\n",
      "SWE max: 18.12142857142857\n",
      "NWM flow min: 881.0\n",
      "NWM flow max: 5460.0\n",
      " \n",
      "site: 10134500\n",
      "modeled flow min: 116.96257781982422\n",
      "modeled flow max: 124.55060577392578\n",
      "Obs flow min: 5.775104\n",
      "Obs flow max: 188.66667\n",
      "Storage min: 54.78383838383838\n",
      "Storage max: 101.43232323232324\n",
      "SWE min: 0.0\n",
      "SWE max: 27.3\n",
      "NWM flow min: 83.0\n",
      "NWM flow max: 389.0\n",
      " \n",
      "site: 10136500\n",
      "modeled flow min: 117.43383026123047\n",
      "modeled flow max: 127.88802337646484\n",
      "Obs flow min: 28.016666\n",
      "Obs flow max: 2272.9167\n",
      "Storage min: 42.94176347735585\n",
      "Storage max: 101.85339557938764\n",
      "SWE min: 0.0\n",
      "SWE max: 25.0625\n",
      "NWM flow min: 399.0\n",
      "NWM flow max: 2628.0\n",
      " \n",
      "site: 10140700\n",
      "modeled flow min: 114.24027252197266\n",
      "modeled flow max: 126.50806427001953\n",
      "Obs flow min: 15.409375\n",
      "Obs flow max: 1754.4791\n",
      "Storage min: 38.32999449923885\n",
      "Storage max: 100.77646441775082\n",
      "SWE min: 0.0\n",
      "SWE max: 37.3\n",
      "NWM flow min: 124.0\n",
      "NWM flow max: 972.0\n",
      " \n",
      "site: 10141000\n",
      "modeled flow min: 114.26251983642578\n",
      "modeled flow max: 126.65287017822266\n",
      "Obs flow min: 16.032711\n",
      "Obs flow max: 3109.7917\n",
      "Storage min: 42.55583484774393\n",
      "Storage max: 101.30686376054976\n",
      "SWE min: 0.0\n",
      "SWE max: 26.74\n",
      "NWM flow min: 603.0\n",
      "NWM flow max: 3818.0\n",
      " \n",
      "site: 10155200\n",
      "modeled flow min: 117.2249526977539\n",
      "modeled flow max: 126.35784149169922\n",
      "Obs flow min: 109.958336\n",
      "Obs flow max: 1713.9584\n",
      "Storage min: 60.55955414012739\n",
      "Storage max: 99.46178343949045\n",
      "SWE min: 0.0\n",
      "SWE max: 16.9\n",
      "NWM flow min: 57.0\n",
      "NWM flow max: 952.0\n",
      " \n"
     ]
    }
   ],
   "source": [
    "modflow = f\"{modelname}_flow\"\n",
    "for station_number in Preds_Dict.keys():\n",
    "    print(f\"site: {station_number}\")\n",
    "    print(f\"modeled flow min: {min(Preds_Dict[station_number][modflow])}\")\n",
    "    print(f\"modeled flow max: {max(Preds_Dict[station_number][modflow])}\")\n",
    "    print(f\"Obs flow min: {min(Preds_Dict[station_number]['flow_cfs'])}\")\n",
    "    print(f\"Obs flow max: {max(Preds_Dict[station_number]['flow_cfs'])}\")\n",
    "    print(f\"Storage min: {min(Preds_Dict[station_number]['storage'])}\")\n",
    "    print(f\"Storage max: {max(Preds_Dict[station_number]['storage'])}\")\n",
    "    print(f\"SWE min: {min(Preds_Dict[station_number]['swe'])}\")\n",
    "    print(f\"SWE max: {max(Preds_Dict[station_number]['swe'])}\")\n",
    "    print(f\"NWM flow min: {min(Preds_Dict[station_number]['NWM_flow'])}\")\n",
    "    print(f\"NWM flow max: {max(Preds_Dict[station_number]['NWM_flow'])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecc9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
