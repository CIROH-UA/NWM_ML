{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import bz2file as bz2\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "#Shared/Utility scripts\n",
    "import sys\n",
    "import boto3\n",
    "import s3fs\n",
    "sys.path.insert(0, '../..')  #sys allows for the .ipynb file to connect to the shared folder files\n",
    "from shared_scripts import lstm_dataprocessing, Simple_Eval, dataloader, lstm_model\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"NWM_ML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'streamflow-app-data'\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)\n",
    "\n",
    "#s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False, key=ACCESS['Access key ID'][0], secret=ACCESS['Secret access key'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25c69fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df needs no processing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "      <th>tempe(F)</th>\n",
       "      <th>precip(mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "      <td>39.239582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "      <td>45.068712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "      <td>50.945891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "      <td>45.480097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "      <td>46.656777</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  ...  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "1         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "2         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "3         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "4         67.7           1.2         0.12          2.94          27.2  ...   \n",
       "\n",
       "     datetime  flow_cfs        s1        s2  storage  swe  NWM_flow  DOY  \\\n",
       "0  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2      55.0  301   \n",
       "1  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2      55.0  302   \n",
       "2  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1      54.0  303   \n",
       "3  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2      54.0  304   \n",
       "4  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2      54.0  305   \n",
       "\n",
       "    tempe(F)  precip(mm)  \n",
       "0  39.239582         0.0  \n",
       "1  45.068712         0.0  \n",
       "2  50.945891         0.0  \n",
       "3  45.480097         0.0  \n",
       "4  46.656777         0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelname = 'LSTM'\n",
    "model_path = f\"{HOME}/NWM_ML/Model/{modelname}\"\n",
    "\n",
    "cfsday_AFday = 1.983\n",
    "\n",
    "#input columns\n",
    "input_columns =[\n",
    "                'Lat', \n",
    "                'Long', \n",
    "                'Drainage_area_mi2', \n",
    "                'Mean_Basin_Elev_ft',       \n",
    "                'Perc_Forest', \n",
    "                'Perc_Develop', \n",
    "                'Perc_Imperv', \n",
    "                'Perc_Herbace',       \n",
    "                'Perc_Slop_30', \n",
    "                'Mean_Ann_Precip_in', \n",
    "                's1',       \n",
    "                's2', \n",
    "                'storage', \n",
    "                'swe', \n",
    "                'NWM_flow', \n",
    "                'DOY', \n",
    "                'tempe(F)', \n",
    "                'precip(mm)'\n",
    "                ]\n",
    "\n",
    "target = ['flow_cfs']\n",
    "\n",
    "\n",
    "test_years = [2019, 2020]                 \n",
    "\n",
    "#load data\n",
    "datapath = f\"{HOME}/NWM_ML/Data/input\"\n",
    "trainingfile = \"final_input.parquet\"\n",
    "\n",
    "df, StreamStats = dataloader.get_ML_Data(datapath, trainingfile)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d99bfd6",
   "metadata": {},
   "source": [
    "## Dataprocessing\n",
    "* Editing the features based on the feature importance\n",
    "* Remove headwater stations from dataset\n",
    "* make sure dates are in datetime format\n",
    "\n",
    "### Data scaling\n",
    "\n",
    "Scaling your data for ML is done for all types of applications and helps the model converge faster. \n",
    "If there is no scaling performed, the model will essentially be forced to think certain features are more important than others, rather than being able to learn those things.\n",
    "\n",
    "There are different ways you can scale the data, such as min-max or standard scaling; both of which are applicable for your model. \n",
    "If you know you have a fixed min and max in your dataset (e.g. images), you can use min-max scaling to fix your input and/or output data to be between 0 and 1.\n",
    "\n",
    "For other applications where you do not have fixed bounds, standard scaling is useful.\n",
    "This gives all of your features zero-mean and unit variance. Therefore, the distributions of inputs and/or outputs are the same, and the model can treat them as such.\n",
    "\n",
    "The scaling for your outputs is important in defining the activation function for the output layer.\n",
    "If you have min-max scaled outputs, you can use sigmoid, because it bounds the outputs to between 0 and 1. \n",
    "If you are using standard scaling for the outputs, you would want to be sure you use a linear activation function, because technically standard-scaled outputs are not bounded. \n",
    "The choice of output activation is important, and knowledge of how your outputs are scaled is important in determining which activation to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "320dfdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26029, 10, 18]) torch.Size([26029])\n"
     ]
    }
   ],
   "source": [
    "#get non headwater stations\n",
    "headwater_stations = ['10011500', # Bear River headwaters before WY state line\n",
    "                      '10109000', # Logan River above dams\n",
    "                      '10113500', # HW Blacksmith fork\n",
    "                      '10128500', # Upper Weber above Oakley\n",
    "                      '10131000', #Chalk creek before Weber - lots of upstream irrigation, potentially include\n",
    "                        '10146400', #Currant Creek above Mona Reservoir - lots of upstream irrigation, potentially include\n",
    "                        '10150500', #Spanish fork after diamond fork - potentially include because of 6th water diversion CUP\n",
    "                        '10154200', #Upper Provo river after confluence of N/S forks - potentially include because of duchense tunnel water diversion CUP\n",
    "                        '10172700', #Vernon creek 2 ranges west of Utah Lake, shouldnt be included because not in GSL basin \n",
    "                        '10172800', #Willow creek west of Gransville,  shouldnt be included because does not make it to GSL\n",
    "                          '10172952'\n",
    "                          ] #Dunn creek in Raft River Range, shouldnt be included because drains to bonnevile salt flats \n",
    "\n",
    "#remove headwater stations\n",
    "df = df[~df['station_id'].isin(headwater_stations)]\n",
    "\n",
    "#get stations with correct swe and storage features\n",
    "#The following sites have swe \n",
    "\n",
    "'''\n",
    "['10011500', '10105900', '10109000', '10126000', '10131000',\n",
    "       '10133650', '10133800', '10133980', '10134500', '10136500',\n",
    "       '10140700', '10141000', '10150500', '10154200', '10155000',\n",
    "       '10155200']\n",
    "'''\n",
    "\n",
    "#the following sites have swe and storage\n",
    "'''\n",
    "['10126000', '10134500', '10136500', '10140700', '10141000',\n",
    "       '10155200']\n",
    "'''\n",
    "\n",
    "stations = df['station_id'][(df['swe']>0) & (df['storage']>0)].unique()\n",
    "\n",
    "#Train model with these stations\n",
    "df = df[df['station_id'].isin(stations)]\n",
    "\n",
    "#convert dates to datetime format\n",
    "df.datetime = pd.to_datetime(df.datetime)\n",
    "\n",
    "# #reset index to clean up df\n",
    "df.reset_index( inplace =  True, drop = True)\n",
    "\n",
    "#Set lookback period - This will need to be a grid search variable\n",
    "lookback = 10\n",
    "scalertype = 'MinMax'\n",
    "#add scaler option - minmax standard\n",
    "x_train_scaled_t, X_test_dic, y_train_scaled_t, y_test_dic = lstm_dataprocessing.Multisite_DataProcessing(df, \n",
    "                                                                                   input_columns, \n",
    "                                                                                   target, \n",
    "                                                                                   lookback, \n",
    "                                                                                   test_years, \n",
    "                                                                                   model_path,\n",
    "                                                                                   scalertype) \n",
    "# maybe adjust to have x - timesteps in a row, way to only use lookback for timeseries changes (and different time series, e.g., stream/snow),\n",
    "#Different dataframe set up for static catchment features \n",
    "\n",
    "print(x_train_scaled_t.shape, y_train_scaled_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c944c1e",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "* make the model a .py file and class when finalized. PyTorch only saves the weights of the layer/node, not the overall structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666548fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.025307490539149453\n",
      "Epoch 2/5, Loss: 0.024108693075651985\n",
      "Epoch 3/5, Loss: 0.02405263412688067\n",
      "Epoch 4/5, Loss: 0.0240680480775356\n",
      "Epoch 5/5, Loss: 0.02403112902234023\n",
      "finish\n",
      "Run Time: 6.189437627792358 seconds \n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "# Assuming you have your data loaded into NumPy arrays as x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, x_scaled, y_scaled\n",
    "# Hyperparameters\n",
    "epochs = 5 #early stopping - Savalan is lookin into it\n",
    "batch_size = 65\n",
    "learning_rate = 0.00001 #initial rate\n",
    "decay = 0.0005 #reduces to \n",
    "validation_split = 0.2  #can add cross validation - Savalan is looking into it.\n",
    "neurons = 300\n",
    "shuffle = True\n",
    "bidirectional = True\n",
    "input_shape = x_train_scaled_t.shape[2]\n",
    "\n",
    "training_params = epochs, batch_size, learning_rate, decay, neurons, bidirectional\n",
    "model_params = bidirectional, input_shape, neurons\n",
    "\n",
    "lstm_model.LSTM_train(training_params,\n",
    "            x_train_scaled_t,\n",
    "            y_train_scaled_t, \n",
    "            shuffle, \n",
    "            model_path,\n",
    "            modelname)\n",
    "\n",
    "Preds_Dict = lstm_model.LSTM_predict(model_params, \n",
    "                        test_years, \n",
    "                        df, \n",
    "                        X_test_dic, \n",
    "                        input_shape, \n",
    "                        StreamStats, \n",
    "                        model_path, \n",
    "                        modelname)\n",
    "\n",
    "#Evaluate model performance of the different models, 'flow_cfs_pred', \n",
    "prediction_columns = ['NWM_flow', f\"{modelname}_flow\"]\n",
    "Eval_DF = Simple_Eval.Simple_Eval(Preds_Dict, \n",
    "                                prediction_columns, \n",
    "                                modelname, \n",
    "                                supply = supply,\n",
    "                                plots = False, \n",
    "                                keystats = False        \n",
    "                                )\n",
    "\n",
    "#create dataframe to store key model perf metrics, and inputs\n",
    "cols = [f\"{modelname}_flow_kge\", f\"{modelname}_flow_rmse\", f\"{modelname}_flow_mape\", f\"{modelname}_flow_pbias\"]\n",
    "model_eval = Eval_DF[cols].copy()\n",
    "\n",
    "#Get mean scoring metrics for AOI - aver kge, mape, pbias\n",
    "model_eval = pd.DataFrame(model_eval.mean(axis=0)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad276f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'shared_scripts.lstm_model' from '/home/rjohnson18/NWM_ML/RJ/ModelDevelopment/../../shared_scripts/lstm_model.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26ff37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing the LSTM model by evaluating 2 models using grid search validation\n",
      "Training 1 of 2 models\n",
      "Parameters: (10, 60, 0.0001, 0.005, 100, 1, True)\n",
      "60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f275349a0545afa45680a91d8e1dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.004690202174081643\n",
      "Epoch 2/10, Loss: 0.004374723489453141\n",
      "Epoch 3/10, Loss: 0.004375076788697019\n",
      "Epoch 4/10, Loss: 0.0043720846798824585\n",
      "Epoch 5/10, Loss: 0.00437717598589075\n",
      "Epoch 6/10, Loss: 0.004373149613247356\n",
      "Epoch 7/10, Loss: 0.004378744148826849\n",
      "Epoch 8/10, Loss: 0.0043730469823445835\n",
      "Epoch 9/10, Loss: 0.004374976375988955\n",
      "Epoch 10/10, Loss: 0.004375041546573657\n",
      "finish\n",
      "Run Time: 9.191747665405273 seconds \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWM_flow_kge</th>\n",
       "      <th>LSTM_flow_kge</th>\n",
       "      <th>NWM_flow_rmse</th>\n",
       "      <th>LSTM_flow_rmse</th>\n",
       "      <th>NWM_flow_mape</th>\n",
       "      <th>LSTM_flow_mape</th>\n",
       "      <th>NWM_flow_pbias</th>\n",
       "      <th>LSTM_flow_pbias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10155200</th>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>192.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>29.28</td>\n",
       "      <td>58.28</td>\n",
       "      <td>15.23</td>\n",
       "      <td>19.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10140700</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>236.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>221.61</td>\n",
       "      <td>271.39</td>\n",
       "      <td>-30.30</td>\n",
       "      <td>-11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136500</th>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>644.0</td>\n",
       "      <td>441.0</td>\n",
       "      <td>330.76</td>\n",
       "      <td>128.16</td>\n",
       "      <td>-137.08</td>\n",
       "      <td>38.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10141000</th>\n",
       "      <td>-2.10</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1111.22</td>\n",
       "      <td>164.51</td>\n",
       "      <td>-304.72</td>\n",
       "      <td>30.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10126000</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>-1.05</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>306.30</td>\n",
       "      <td>70.30</td>\n",
       "      <td>-39.65</td>\n",
       "      <td>81.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134500</th>\n",
       "      <td>-0.97</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>132.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>545.15</td>\n",
       "      <td>1071.86</td>\n",
       "      <td>-173.33</td>\n",
       "      <td>-266.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NWM_flow_kge  LSTM_flow_kge  NWM_flow_rmse  LSTM_flow_rmse  \\\n",
       "station_id                                                               \n",
       "10155200            0.63          -0.58          192.0           307.0   \n",
       "10140700            0.29          -0.70          236.0           277.0   \n",
       "10136500           -0.42          -0.75          644.0           441.0   \n",
       "10141000           -2.10          -0.75         1169.0           526.0   \n",
       "10126000           -0.31          -1.05         1716.0          1452.0   \n",
       "10134500           -0.97          -2.02          132.0           170.0   \n",
       "\n",
       "            NWM_flow_mape  LSTM_flow_mape  NWM_flow_pbias  LSTM_flow_pbias  \n",
       "station_id                                                                  \n",
       "10155200            29.28           58.28           15.23            19.07  \n",
       "10140700           221.61          271.39          -30.30           -11.76  \n",
       "10136500           330.76          128.16         -137.08            38.20  \n",
       "10141000          1111.22          164.51         -304.72            30.54  \n",
       "10126000           306.30           70.30          -39.65            81.69  \n",
       "10134500           545.15         1071.86         -173.33          -266.82  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2 of 2 models\n",
      "Parameters: (10, 80, 0.0001, 0.005, 100, 1, True)\n",
      "80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409e385429db4668921461a8e988275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.004785245909545559\n",
      "Epoch 2/10, Loss: 0.004370355443847566\n",
      "Epoch 3/10, Loss: 0.0043696581448165505\n",
      "Epoch 4/10, Loss: 0.004364768752365436\n",
      "Epoch 5/10, Loss: 0.004370675746767862\n",
      "Epoch 6/10, Loss: 0.004367977150868013\n",
      "Epoch 7/10, Loss: 0.004369670668748304\n",
      "Epoch 8/10, Loss: 0.004364942609721029\n",
      "Epoch 9/10, Loss: 0.004368402069203121\n",
      "Epoch 10/10, Loss: 0.004365497969638733\n",
      "finish\n",
      "Run Time: 7.745797157287598 seconds \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NWM_flow_kge</th>\n",
       "      <th>LSTM_flow_kge</th>\n",
       "      <th>NWM_flow_rmse</th>\n",
       "      <th>LSTM_flow_rmse</th>\n",
       "      <th>NWM_flow_mape</th>\n",
       "      <th>LSTM_flow_mape</th>\n",
       "      <th>NWM_flow_pbias</th>\n",
       "      <th>LSTM_flow_pbias</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10155200</th>\n",
       "      <td>0.63</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>192.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>29.28</td>\n",
       "      <td>59.81</td>\n",
       "      <td>15.23</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10140700</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>236.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>221.61</td>\n",
       "      <td>279.49</td>\n",
       "      <td>-30.30</td>\n",
       "      <td>-14.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10141000</th>\n",
       "      <td>-2.10</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>525.0</td>\n",
       "      <td>1111.22</td>\n",
       "      <td>167.98</td>\n",
       "      <td>-304.72</td>\n",
       "      <td>29.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136500</th>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>644.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>330.76</td>\n",
       "      <td>130.04</td>\n",
       "      <td>-137.08</td>\n",
       "      <td>37.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10126000</th>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>306.30</td>\n",
       "      <td>70.24</td>\n",
       "      <td>-39.65</td>\n",
       "      <td>81.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10134500</th>\n",
       "      <td>-0.97</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>132.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>545.15</td>\n",
       "      <td>1103.95</td>\n",
       "      <td>-173.33</td>\n",
       "      <td>-276.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            NWM_flow_kge  LSTM_flow_kge  NWM_flow_rmse  LSTM_flow_rmse  \\\n",
       "station_id                                                               \n",
       "10155200            0.63          -0.62          192.0           307.0   \n",
       "10140700            0.29          -0.66          236.0           278.0   \n",
       "10141000           -2.10          -0.68         1169.0           525.0   \n",
       "10136500           -0.42          -0.73          644.0           440.0   \n",
       "10126000           -0.31          -0.96         1716.0          1450.0   \n",
       "10134500           -0.97          -2.14          132.0           175.0   \n",
       "\n",
       "            NWM_flow_mape  LSTM_flow_mape  NWM_flow_pbias  LSTM_flow_pbias  \n",
       "station_id                                                                  \n",
       "10155200            29.28           59.81           15.23            17.87  \n",
       "10140700           221.61          279.49          -30.30           -14.57  \n",
       "10141000          1111.22          167.98         -304.72            29.22  \n",
       "10136500           330.76          130.04         -137.08            37.27  \n",
       "10126000           306.30           70.24          -39.65            81.57  \n",
       "10134500           545.15         1103.95         -173.33          -276.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTM_flow_kge</th>\n",
       "      <th>LSTM_flow_rmse</th>\n",
       "      <th>LSTM_flow_mape</th>\n",
       "      <th>LSTM_flow_pbias</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Batchsize</th>\n",
       "      <th>LR</th>\n",
       "      <th>Decay</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Bidirectional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.965</td>\n",
       "      <td>529.166667</td>\n",
       "      <td>301.918333</td>\n",
       "      <td>-20.773333</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.975</td>\n",
       "      <td>528.833333</td>\n",
       "      <td>294.083333</td>\n",
       "      <td>-18.180000</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LSTM_flow_kge  LSTM_flow_rmse  LSTM_flow_mape  LSTM_flow_pbias  Epochs  \\\n",
       "0         -0.965      529.166667      301.918333       -20.773333      10   \n",
       "1         -0.975      528.833333      294.083333       -18.180000      10   \n",
       "\n",
       "   Batchsize      LR  Decay  Neurons  Bidirectional  \n",
       "0         80  0.0001  0.005      100           True  \n",
       "1         60  0.0001  0.005      100           True  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "# Assuming you have your data loaded into NumPy arrays as x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, x_scaled, y_scaled\n",
    "# parameters\n",
    "epochs = [10] #early stopping - Savalan is lookin into it\n",
    "batch_size = [60,80]\n",
    "learning_rate = [0.0001] #initial rate\n",
    "decay = [0.005] #reduces to \n",
    "neurons = [100]\n",
    "num_layers = [1]\n",
    "shuffle = [True]\n",
    "bidirectional = [True]\n",
    "input_shape = x_train_scaled_t.shape[2]\n",
    "supply = False\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "training_params = epochs, batch_size, learning_rate, decay, neurons, num_layers, bidirectional\n",
    "model_params = bidirectional, input_shape, neurons, num_layers\n",
    "\n",
    "GS_Eval_DF, GS_Eval_dict = lstm_model.LSTM_optimization(training_params,\n",
    "                            loss_func,\n",
    "                            x_train_scaled_t,\n",
    "                            y_train_scaled_t, \n",
    "                            shuffle,  \n",
    "                            model_path,\n",
    "                            modelname,\n",
    "                            model_params, \n",
    "                            test_years, \n",
    "                            df, \n",
    "                            X_test_dic, \n",
    "                            input_shape, \n",
    "                            StreamStats,\n",
    "                            supply)\n",
    "\n",
    "GS_Eval_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1a412",
   "metadata": {},
   "source": [
    "# Finalize automatic training protocol for final model.\n",
    "Make a prediction for each location, save as compressed pkl file, and send predictions to AWS for use in CSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the MLP final model to lstm\n",
    "\n",
    "Eval_DF, Preds_Dict = mlp_model.Final_Model(GS_Eval_DF,\n",
    "                x_train_scaled_t,\n",
    "                y_train_scaled_t, \n",
    "                loss_func, \n",
    "                model_path, \n",
    "                modelname,\n",
    "                test_years, \n",
    "                stations, \n",
    "                x_test_temp,\n",
    "                x_test_scaled, \n",
    "                y_test_temp,\n",
    "                StreamStats,\n",
    "                station_index_list)\n",
    "Eval_DF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
