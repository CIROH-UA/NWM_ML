{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b2d62-be71-4ba4-8a8f-1da3e858df44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install progressbar xgboost matplotlib boto3 openpyxl tqdm hydroeval hydrotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hydrological packages\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# my packages\n",
    "from g_evaluation_metric import MAPE, RMSE, KGE, PBias\n",
    "from s_evalaution_table import evtab\n",
    "import s_FigureGenerator\n",
    "from g_mlp_model import CustomMLP\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58486c593749230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:41.288250Z",
     "start_time": "2023-11-09T04:18:41.280798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    onedrive_path = 'E:/OneDrive/OneDrive - The University of Alabama/10.material/01.data/usgs_data/'\n",
    "    box_path = 'C:/Users/snaserneisary/Box/NWM-ML/'\n",
    "\n",
    "elif platform.system() == 'Darwin':\n",
    "    onedrive_path = '/Users/savalan/Library/CloudStorage/OneDrive-TheUniversityofAlabama/02.projects/03.ciroh/04.data/'\n",
    "    box_path = '/Users/savalan/Library/CloudStorage/Box-Box/NWM-ML/Data/NWM/ut/'\n",
    "    \n",
    "elif platform.system() == 'Linux':\n",
    "    path_general = '/home/snaserneisary/01.projects/01.ciroh_p8/NWM-ML/Savalan/'\n",
    "    path_model_save = f\"{path_general}/03.output/02.mlp/03.model_parameters/best_model.pkl\"\n",
    "    path_save_data = f\"{path_general}/03.output/02.mlp/02.data/\" \n",
    "    path_save_figure = f\"{path_general}/03.output/02.mlp/01.figures\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3b260-099e-4a37-84b5-7e7eb82146ad",
   "metadata": {},
   "source": [
    "## 2. Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>Mean_Ann_Precip_in</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-28</td>\n",
       "      <td>78.55521</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-29</td>\n",
       "      <td>98.61146</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>97.60208</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-10-31</td>\n",
       "      <td>99.33125</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.0</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>34.8</td>\n",
       "      <td>2010-11-01</td>\n",
       "      <td>95.76354</td>\n",
       "      <td>-0.998630</td>\n",
       "      <td>0.052336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id        Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "0   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "1   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "2   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "3   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "4   10011500  40.965225 -110.853508              174.0              9720.0   \n",
       "\n",
       "   Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "0         67.7           1.2         0.12          2.94          27.2   \n",
       "1         67.7           1.2         0.12          2.94          27.2   \n",
       "2         67.7           1.2         0.12          2.94          27.2   \n",
       "3         67.7           1.2         0.12          2.94          27.2   \n",
       "4         67.7           1.2         0.12          2.94          27.2   \n",
       "\n",
       "   Mean_Ann_Precip_in    datetime  flow_cfs        s1        s2  storage  swe  \\\n",
       "0                34.8  2010-10-28  78.55521 -0.891007 -0.453991      0.0  1.2   \n",
       "1                34.8  2010-10-29  98.61146 -0.891007 -0.453991      0.0  1.2   \n",
       "2                34.8  2010-10-30  97.60208 -0.891007 -0.453991      0.0  1.1   \n",
       "3                34.8  2010-10-31  99.33125 -0.891007 -0.453991      0.0  1.2   \n",
       "4                34.8  2010-11-01  95.76354 -0.998630  0.052336      0.0  1.2   \n",
       "\n",
       "   NWM_flow  DOY  \n",
       "0      55.0  301  \n",
       "1      55.0  302  \n",
       "2      54.0  303  \n",
       "3      54.0  304  \n",
       "4      54.0  305  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data = pd.read_csv(path_general + '03.output/01.data_preparation/raw_training_data.csv')\n",
    "raw_training_data.pop('Unnamed: 0')\n",
    "raw_training_data['station_id'] = raw_training_data['station_id'].astype('str')\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f58d9b00594ff3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:23.114069Z",
     "start_time": "2023-11-10T04:40:23.082463Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Training_DF = raw_training_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Editing the features based on the feature importance should be in the next cell!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8b8226-79f9-4f8d-8528-8989050bbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Editing the features based on the feature importance should be done here!!!!!!!!!!!!!!!\n",
    "\n",
    "#Training_DF.drop(['precipitation_in', 'temperature_F', 'Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest', 'Mean_Basin_Elev_ft'], axis=1, inplace=True)\n",
    "\n",
    "Training_DF.drop(['Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest', 'Mean_Basin_Elev_ft'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765d89e-a4e4-481d-bdd1-f708678004af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove headwater stations!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2172979-7110-45a7-9778-e30173e34736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headwater_stations = ['10011500', '10109000', '10113500', '10128500', '10131000', '10146400', '10150500', '10154200',\n",
    "'10172700', '10172800', '10172952']\n",
    "Training_DF = Training_DF[~raw_training_data['station_id'].isin(headwater_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be2448b6151fb377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:30.482191Z",
     "start_time": "2023-11-10T04:40:30.463560Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>1992-10-01</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>1992-10-02</td>\n",
       "      <td>8.654167</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>1992-10-03</td>\n",
       "      <td>9.466667</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>1992-10-04</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>1992-10-05</td>\n",
       "      <td>10.195833</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     station_id       Lat       Long  Drainage_area_mi2  Perc_Develop  \\\n",
       "3079   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "3080   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "3081   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "3082   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "3083   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "\n",
       "      Perc_Imperv  Perc_Slop_30   datetime   flow_cfs        s1        s2  \\\n",
       "3079       0.0653          44.2 1992-10-01   9.250000 -0.891007 -0.453991   \n",
       "3080       0.0653          44.2 1992-10-02   8.654167 -0.891007 -0.453991   \n",
       "3081       0.0653          44.2 1992-10-03   9.466667 -0.891007 -0.453991   \n",
       "3082       0.0653          44.2 1992-10-04  11.833333 -0.891007 -0.453991   \n",
       "3083       0.0653          44.2 1992-10-05  10.195833 -0.891007 -0.453991   \n",
       "\n",
       "      storage  swe  NWM_flow  DOY  \n",
       "3079      0.0  0.0      37.0  275  \n",
       "3080      0.0  0.0      36.0  276  \n",
       "3081      0.0  0.0      36.0  277  \n",
       "3082      0.0  0.0      36.0  278  \n",
       "3083      0.0  0.0      36.0  279  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_DF.datetime = pd.to_datetime(Training_DF.datetime)\n",
    "Training_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805906c-4949-466e-b168-103230d00cb7",
   "metadata": {},
   "source": [
    "### 2.1. Create the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a160a8633a96ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:40.495727Z",
     "start_time": "2023-11-10T04:40:40.474155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>-0.891007</td>\n",
       "      <td>-0.453991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Lat       Long  Drainage_area_mi2  Perc_Develop  Perc_Imperv  \\\n",
       "3079  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "3080  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "3081  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "3082  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "3083  41.57549 -111.85522              180.0          1.01       0.0653   \n",
       "\n",
       "      Perc_Slop_30        s1        s2  storage  swe  NWM_flow  DOY  \n",
       "3079          44.2 -0.891007 -0.453991      0.0  0.0      37.0  275  \n",
       "3080          44.2 -0.891007 -0.453991      0.0  0.0      36.0  276  \n",
       "3081          44.2 -0.891007 -0.453991      0.0  0.0      36.0  277  \n",
       "3082          44.2 -0.891007 -0.453991      0.0  0.0      36.0  278  \n",
       "3083          44.2 -0.891007 -0.453991      0.0  0.0      36.0  279  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training is from 1980 to the end of 2015.\n",
    "x_train_temp = Training_DF[Training_DF.datetime < '01-01-2015']\n",
    "x_train_temp.pop('station_id')\n",
    "x_train_temp.pop('datetime')\n",
    "y_train_temp = x_train_temp['flow_cfs']\n",
    "x_train_temp.pop('flow_cfs')\n",
    "x_train_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a433ea14-d874-4409-82e4-8745b5658249",
   "metadata": {},
   "source": [
    "#### 2.1.1. Scale the train inputs of the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0faa488a9e9e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:41.923386Z",
     "start_time": "2023-11-10T04:40:41.912929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93299, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we need to convert it from pandas dataframe to a numpy array \n",
    "y_train = y_train_temp.to_numpy()\n",
    "x_train = x_train_temp.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "y_scaled_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288d7dffbf2edc7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:52.472498Z",
     "start_time": "2023-11-10T04:40:52.464352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x shape torch.Size([93299, 12])\n",
      "train y shape torch.Size([93299, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert and reshape inputs to tensors for MLP model\n",
    "x_train_scaled_test = torch.Tensor(x_train_scaled)\n",
    "y_train_scaled_test = torch.Tensor(y_scaled_train)\n",
    "print('train x shape', x_train_scaled_test.shape)\n",
    "print('train y shape', y_train_scaled_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cffbb3-9d5d-4cc5-9091-53de73411711",
   "metadata": {},
   "source": [
    "### 2.2. Create the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd46d43cce5d1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:44.100124Z",
     "start_time": "2023-11-10T04:40:44.086373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>21.627083</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>23.531250</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>25.044792</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10570</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>26.103125</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>39.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10571</th>\n",
       "      <td>10105900</td>\n",
       "      <td>41.57549</td>\n",
       "      <td>-111.85522</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>44.2</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>26.742708</td>\n",
       "      <td>-0.438371</td>\n",
       "      <td>0.898794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      station_id       Lat       Long  Drainage_area_mi2  Perc_Develop  \\\n",
       "10567   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "10568   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "10569   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "10570   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "10571   10105900  41.57549 -111.85522              180.0          1.01   \n",
       "\n",
       "       Perc_Imperv  Perc_Slop_30   datetime   flow_cfs        s1        s2  \\\n",
       "10567       0.0653          44.2 2015-01-01  21.627083 -0.438371  0.898794   \n",
       "10568       0.0653          44.2 2015-01-02  23.531250 -0.438371  0.898794   \n",
       "10569       0.0653          44.2 2015-01-03  25.044792 -0.438371  0.898794   \n",
       "10570       0.0653          44.2 2015-01-04  26.103125 -0.438371  0.898794   \n",
       "10571       0.0653          44.2 2015-01-05  26.742708 -0.438371  0.898794   \n",
       "\n",
       "       storage   swe  NWM_flow  DOY  \n",
       "10567      0.0  5.75      39.0    1  \n",
       "10568      0.0  5.75      39.0    2  \n",
       "10569      0.0  5.75      39.0    3  \n",
       "10570      0.0  5.75      39.0    4  \n",
       "10571      0.0  5.75      39.0    5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determining the test dataset. \n",
    "x_test_temp = Training_DF[Training_DF.datetime >= '01-01-2015']\n",
    "x_test_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadf937-9387-4d22-9fbe-c180c85bddeb",
   "metadata": {},
   "source": [
    "#### 2.2.1. Scale the test inputs of the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b48a068a-2fdd-4ac7-9079-9d35edd9e013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we need to convert it from pandas dataframe to a numpy array \n",
    "x_test_temp_1 = x_test_temp.copy()\n",
    "station_index_list = x_test_temp_1['station_id']\n",
    "x_test_temp_1.pop('station_id')\n",
    "x_test_temp_1.pop('datetime')\n",
    "y_test_temp_1 = x_test_temp_1['flow_cfs']\n",
    "x_test_temp_1.pop('flow_cfs')\n",
    "x_test_1_np = x_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "y_test_1_np = y_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "x_test_1_scaled = scaler.fit_transform(x_test_1_np)\n",
    "y_scaled_test_1 = scaler.fit_transform(y_test_1_np.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e8feb-33be-4a4f-83a8-f93320d0d6d4",
   "metadata": {},
   "source": [
    "### 2.3. Create the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36b119b-2344-40ba-a0ae-378d56ff15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train_scaled_test, y_train_scaled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863a2ef-1769-45be-9bba-de633b10dd50",
   "metadata": {},
   "source": [
    "## 3. MLP Model Preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3c50dd-fad7-43cb-aaae-f613149b25ef",
   "metadata": {},
   "source": [
    "### 3.1. Create the model variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe54f58-8061-4c5b-b4b9-fa85fbac9512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_name='MLP'\n",
    "tries = 30\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "learning_rate = 1e-4\n",
    "early_stopping_patience = 0\n",
    "decay = 1e-2\n",
    "neurons = 300\n",
    "layer_sizes = [x_train_scaled_test.shape[1] ,128, 128, 64, 64, 32, 16, 1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae42da-628e-4728-8e48-bc4a50e0bd22",
   "metadata": {},
   "source": [
    "### 3.2. Create input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5009367f-7038-453b-a523-1f18755d3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train_scaled_test, y_train_scaled_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_dataset = TensorDataset(X_valid, y_valid)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a89ebd-ffe4-4adc-889d-b9df49f9d0ab",
   "metadata": {},
   "source": [
    "### 3.3. Run and evaluate the model\n",
    "Here first we train the model and then we test it.\n",
    "We do it 30 times so we have firm evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e84a3b6-9949-4ff6-86b3-fea539b657c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial Number 1 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00018038495909422636 Validation Loss: 0.001953898743237943\n",
      "Epoch 2/100, Training Loss: 6.983758794376627e-05 Validation Loss: 0.001876069893568878\n",
      "Epoch 3/100, Training Loss: 4.7491397708654404e-05 Validation Loss: 0.0018645256001223777\n",
      "Epoch 4/100, Training Loss: 3.969627869082615e-05 Validation Loss: 0.0018616710532589396\n",
      "Epoch 5/100, Training Loss: 3.559304605005309e-05 Validation Loss: 0.0018606661382332915\n",
      "Epoch 6/100, Training Loss: 3.3252690627705306e-05 Validation Loss: 0.0018603390977729914\n",
      "Epoch 7/100, Training Loss: 3.208085399819538e-05 Validation Loss: 0.0018602691867537517\n",
      "Epoch 8/100, Training Loss: 3.1643245165469125e-05 Validation Loss: 0.0018602627952656002\n",
      "Epoch 9/100, Training Loss: 3.150842894683592e-05 Validation Loss: 0.001860263151062267\n",
      "Epoch 10/100, Training Loss: 3.139959153486416e-05 Validation Loss: 0.0018602643476570541\n",
      "Epoch 11/100, Training Loss: 3.117653614026494e-05 Validation Loss: 0.0018602691911832735\n",
      "Epoch 12/100, Training Loss: 3.043694050575141e-05 Validation Loss: 0.0018603117840932553\n",
      "Epoch 13/100, Training Loss: 3.060817834921181e-05 Validation Loss: 0.0018602977534266946\n",
      "Epoch 14/100, Training Loss: 3.0318433346110396e-05 Validation Loss: 0.0018603219164062255\n",
      "Epoch 15/100, Training Loss: 3.001327240781393e-05 Validation Loss: 0.0018603549602652111\n",
      "Epoch 16/100, Training Loss: 2.9695365810766816e-05 Validation Loss: 0.0018603985354060752\n",
      "Epoch 17/100, Training Loss: 2.9360089683905244e-05 Validation Loss: 0.001860455169775678\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002435708011034876\n",
      "Validation Loss: 0.038819022476673126\n",
      "Validation Loss: 0.0002725691010709852\n",
      "Validation Loss: 0.00023761617194395512\n",
      "Validation Loss: 0.00020526940352283418\n",
      "Validation Loss: 0.00018328562146052718\n",
      "Validation Loss: 0.00017612590454518795\n",
      "Validation Loss: 0.002176327630877495\n",
      "Validation Loss: 0.00027760289958678186\n",
      "Validation Loss: 0.0006848510238341987\n",
      "Validation Loss: 0.0009688325226306915\n",
      "Validation Loss: 0.003056567395105958\n",
      "Validation Loss: 0.00025173561880365014\n",
      "Validation Loss: 0.00019840255845338106\n",
      "Validation Loss: 0.0001980672823265195\n",
      "Validation Loss: 0.003002416342496872\n",
      "Validation Loss: 0.0007897716714069247\n",
      "Validation Loss: 0.00028220683452673256\n",
      "Validation Loss: 0.000199870701180771\n",
      "Validation Loss: 2.783401396300178e-05\n",
      "Trial Number 2 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.530223900976125e-05 Validation Loss: 0.0018628295842909512\n",
      "Epoch 2/100, Training Loss: 2.3878099455032498e-05 Validation Loss: 0.0018681397189666683\n",
      "Epoch 3/100, Training Loss: 2.4029430278460495e-05 Validation Loss: 0.0018703221181998507\n",
      "Epoch 4/100, Training Loss: 2.409280205029063e-05 Validation Loss: 0.001870789332627264\n",
      "Epoch 5/100, Training Loss: 2.4045266400207765e-05 Validation Loss: 0.0018704335277876143\n",
      "Epoch 6/100, Training Loss: 2.3975002477527596e-05 Validation Loss: 0.0018698180075395465\n",
      "Epoch 7/100, Training Loss: 2.3913884433568455e-05 Validation Loss: 0.0018690779177813381\n",
      "Epoch 8/100, Training Loss: 2.409365333733149e-05 Validation Loss: 0.0018707736859338644\n",
      "Epoch 9/100, Training Loss: 2.3879036234575324e-05 Validation Loss: 0.001867771741925636\n",
      "Epoch 10/100, Training Loss: 2.3959166355780326e-05 Validation Loss: 0.0018664729434074506\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002724955265875906\n",
      "Validation Loss: 0.03834497556090355\n",
      "Validation Loss: 0.00033373109181411564\n",
      "Validation Loss: 0.0002945655141957104\n",
      "Validation Loss: 0.0002574427053332329\n",
      "Validation Loss: 0.0002302106295246631\n",
      "Validation Loss: 0.00022240761609282345\n",
      "Validation Loss: 0.0021106041967868805\n",
      "Validation Loss: 0.0002972132642753422\n",
      "Validation Loss: 0.0007064322126097977\n",
      "Validation Loss: 0.000964812352322042\n",
      "Validation Loss: 0.003016372211277485\n",
      "Validation Loss: 0.00030999761656858027\n",
      "Validation Loss: 0.00025083046057261527\n",
      "Validation Loss: 0.0002458554517943412\n",
      "Validation Loss: 0.0029463432729244232\n",
      "Validation Loss: 0.0007555378251709044\n",
      "Validation Loss: 0.0003441954031586647\n",
      "Validation Loss: 0.0002480248804204166\n",
      "Validation Loss: 4.4427037209970877e-05\n",
      "Trial Number 3 ==========================================================\n",
      "Epoch 1/100, Training Loss: 6.418652628781274e-05 Validation Loss: 0.001872838991130168\n",
      "Epoch 2/100, Training Loss: 3.071189348702319e-05 Validation Loss: 0.0018602943588532349\n",
      "Epoch 3/100, Training Loss: 2.4801029212540016e-05 Validation Loss: 0.0018635889111507967\n",
      "Epoch 4/100, Training Loss: 2.4109927835525014e-05 Validation Loss: 0.001865555026936809\n",
      "Epoch 5/100, Training Loss: 2.393130307609681e-05 Validation Loss: 0.0018667614613140614\n",
      "Epoch 6/100, Training Loss: 2.390719782852102e-05 Validation Loss: 0.00186706399073423\n",
      "Epoch 7/100, Training Loss: 2.3931099349283613e-05 Validation Loss: 0.001866754707104216\n",
      "Epoch 8/100, Training Loss: 2.4000071789487265e-05 Validation Loss: 0.001866161469909156\n",
      "Epoch 9/100, Training Loss: 2.4139815650414675e-05 Validation Loss: 0.001865398346325173\n",
      "Epoch 10/100, Training Loss: 2.436453360132873e-05 Validation Loss: 0.0018645892944012241\n",
      "Epoch 11/100, Training Loss: 2.480934199411422e-05 Validation Loss: 0.0018635469730614034\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002610654046293348\n",
      "Validation Loss: 0.038519736379384995\n",
      "Validation Loss: 0.0003103668277617544\n",
      "Validation Loss: 0.0002727608080022037\n",
      "Validation Loss: 0.00023740607139188796\n",
      "Validation Loss: 0.00021211686544120312\n",
      "Validation Loss: 0.0002045519941020757\n",
      "Validation Loss: 0.0021342122927308083\n",
      "Validation Loss: 0.00028923130594193935\n",
      "Validation Loss: 0.0006977206212468445\n",
      "Validation Loss: 0.0009655783651396632\n",
      "Validation Loss: 0.0030305299442261457\n",
      "Validation Loss: 0.000287706934614107\n",
      "Validation Loss: 0.000230699559324421\n",
      "Validation Loss: 0.00022744215675629675\n",
      "Validation Loss: 0.0029663790483027697\n",
      "Validation Loss: 0.0007674887892790139\n",
      "Validation Loss: 0.00032052514143288136\n",
      "Validation Loss: 0.00022947612160351127\n",
      "Validation Loss: 3.7562058423645794e-05\n",
      "Trial Number 4 ==========================================================\n",
      "Epoch 1/100, Training Loss: 3.532337723299861e-05 Validation Loss: 0.0018605763749703606\n",
      "Epoch 2/100, Training Loss: 3.059044684050605e-05 Validation Loss: 0.001860303081330491\n",
      "Epoch 3/100, Training Loss: 2.9047350835753605e-05 Validation Loss: 0.0018605234135478327\n",
      "Epoch 4/100, Training Loss: 2.881974432966672e-05 Validation Loss: 0.001860575083109659\n",
      "Epoch 5/100, Training Loss: 2.9064423870295286e-05 Validation Loss: 0.001860516255752394\n",
      "Epoch 6/100, Training Loss: 2.9375198209891096e-05 Validation Loss: 0.0018604526426398574\n",
      "Epoch 7/100, Training Loss: 2.9459430152201094e-05 Validation Loss: 0.001860437207128593\n",
      "Epoch 8/100, Training Loss: 2.924570617324207e-05 Validation Loss: 0.0018604772149446438\n",
      "Epoch 9/100, Training Loss: 2.8889795430586673e-05 Validation Loss: 0.0018605554012465914\n",
      "Epoch 10/100, Training Loss: 2.8523209039121866e-05 Validation Loss: 0.0018606522723324677\n",
      "Epoch 11/100, Training Loss: 2.827998650900554e-05 Validation Loss: 0.0018607268326028398\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00024761559325270355\n",
      "Validation Loss: 0.03874571993947029\n",
      "Validation Loss: 0.0002815712068695575\n",
      "Validation Loss: 0.0002459704701323062\n",
      "Validation Loss: 0.0002128892665496096\n",
      "Validation Loss: 0.00019009842071682215\n",
      "Validation Loss: 0.00018283976532984525\n",
      "Validation Loss: 0.002165817888453603\n",
      "Validation Loss: 0.0002802153758239001\n",
      "Validation Loss: 0.0006877665291540325\n",
      "Validation Loss: 0.0009678112692199647\n",
      "Validation Loss: 0.00304998317733407\n",
      "Validation Loss: 0.00026029173750430346\n",
      "Validation Loss: 0.00020606158068403602\n",
      "Validation Loss: 0.0002050128096016124\n",
      "Validation Loss: 0.0029933901969343424\n",
      "Validation Loss: 0.0007841041660867631\n",
      "Validation Loss: 0.0002913360367529094\n",
      "Validation Loss: 0.000206872500712052\n",
      "Validation Loss: 2.99824787362013e-05\n",
      "Trial Number 5 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00013550596486311406 Validation Loss: 0.00192027743702399\n",
      "Epoch 2/100, Training Loss: 6.715059862472117e-05 Validation Loss: 0.0018745496383274726\n",
      "Epoch 3/100, Training Loss: 4.9032620154321194e-05 Validation Loss: 0.0018652081271206275\n",
      "Epoch 4/100, Training Loss: 4.0007995266932994e-05 Validation Loss: 0.0018617721990206043\n",
      "Epoch 5/100, Training Loss: 3.519980236887932e-05 Validation Loss: 0.0018605990280444152\n",
      "Epoch 6/100, Training Loss: 3.2767944503575563e-05 Validation Loss: 0.0018603021392148559\n",
      "Epoch 7/100, Training Loss: 3.163485962431878e-05 Validation Loss: 0.0018602628240886865\n",
      "Epoch 8/100, Training Loss: 3.065666896873154e-05 Validation Loss: 0.0018602943363936872\n",
      "Epoch 9/100, Training Loss: 3.0727424018550664e-05 Validation Loss: 0.001860289748157659\n",
      "Epoch 10/100, Training Loss: 3.0093313398538157e-05 Validation Loss: 0.0018603454364188184\n",
      "Epoch 11/100, Training Loss: 3.028490573342424e-05 Validation Loss: 0.0018603250869456977\n",
      "Epoch 12/100, Training Loss: 2.9832663130946457e-05 Validation Loss: 0.0018603784976215655\n",
      "Epoch 13/100, Training Loss: 2.916171433753334e-05 Validation Loss: 0.0018604943721050083\n",
      "Epoch 14/100, Training Loss: 2.8697762900264934e-05 Validation Loss: 0.0018606040057664317\n",
      "Epoch 15/100, Training Loss: 2.821474481606856e-05 Validation Loss: 0.0018607482505265509\n",
      "Epoch 16/100, Training Loss: 2.7708543711923994e-05 Validation Loss: 0.0018609380627141832\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002502207935322076\n",
      "Validation Loss: 0.038699932396411896\n",
      "Validation Loss: 0.00028727800236083567\n",
      "Validation Loss: 0.0002512719656806439\n",
      "Validation Loss: 0.00021773128537461162\n",
      "Validation Loss: 0.00019443545897956938\n",
      "Validation Loss: 0.00018711491429712623\n",
      "Validation Loss: 0.0021593167912214994\n",
      "Validation Loss: 0.0002819244109559804\n",
      "Validation Loss: 0.0006896652630530298\n",
      "Validation Loss: 0.0009672468877397478\n",
      "Validation Loss: 0.0030459382105618715\n",
      "Validation Loss: 0.00026571957278065383\n",
      "Validation Loss: 0.00021092806127853692\n",
      "Validation Loss: 0.0002094328956445679\n",
      "Validation Loss: 0.002987818093970418\n",
      "Validation Loss: 0.0007806327776052058\n",
      "Validation Loss: 0.00029712237301282585\n",
      "Validation Loss: 0.00021132781694177538\n",
      "Validation Loss: 3.140121407341212e-05\n",
      "Trial Number 6 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00038665623287670314 Validation Loss: 0.0023631886925971554\n",
      "Epoch 2/100, Training Loss: 0.0001201227933051996 Validation Loss: 0.0020197248818140203\n",
      "Epoch 3/100, Training Loss: 6.803390715504065e-05 Validation Loss: 0.0019495147008384954\n",
      "Epoch 4/100, Training Loss: 4.871834971709177e-05 Validation Loss: 0.0019207288704374316\n",
      "Epoch 5/100, Training Loss: 4.05973732995335e-05 Validation Loss: 0.0019075404030359715\n",
      "Epoch 6/100, Training Loss: 3.6742829252034426e-05 Validation Loss: 0.001900849577340438\n",
      "Epoch 7/100, Training Loss: 3.385246964171529e-05 Validation Loss: 0.001895550948753354\n",
      "Epoch 8/100, Training Loss: 3.142152490909211e-05 Validation Loss: 0.001890817473942987\n",
      "Epoch 9/100, Training Loss: 2.9713839467149228e-05 Validation Loss: 0.001887272746803285\n",
      "Epoch 10/100, Training Loss: 2.986371146107558e-05 Validation Loss: 0.0018875756888552535\n",
      "Epoch 11/100, Training Loss: 2.8309692424954847e-05 Validation Loss: 0.001884125051872719\n",
      "Epoch 12/100, Training Loss: 2.747291546256747e-05 Validation Loss: 0.001882125784546668\n",
      "Epoch 13/100, Training Loss: 2.6941901523969136e-05 Validation Loss: 0.001880783195275961\n",
      "Epoch 14/100, Training Loss: 2.7455813324195333e-05 Validation Loss: 0.0018820724501708744\n",
      "Epoch 15/100, Training Loss: 2.6221443476970308e-05 Validation Loss: 0.0018788374214141659\n",
      "Epoch 16/100, Training Loss: 2.5657733203843236e-05 Validation Loss: 0.0018771787359087757\n",
      "Epoch 17/100, Training Loss: 2.51807359745726e-05 Validation Loss: 0.0018756294508598394\n",
      "Epoch 18/100, Training Loss: 2.4942297386587597e-05 Validation Loss: 0.0018747772376121912\n",
      "Epoch 19/100, Training Loss: 2.5241355615435168e-05 Validation Loss: 0.0018758284698399174\n",
      "Epoch 20/100, Training Loss: 2.437580951664131e-05 Validation Loss: 0.001872397392627854\n",
      "Epoch 21/100, Training Loss: 2.4395434593316168e-05 Validation Loss: 0.0018724900534219785\n",
      "Epoch 22/100, Training Loss: 2.41460875258781e-05 Validation Loss: 0.0018711202912180338\n",
      "Epoch 23/100, Training Loss: 2.4014190785237588e-05 Validation Loss: 0.0018701626287722342\n",
      "Epoch 24/100, Training Loss: 2.388353277638089e-05 Validation Loss: 0.0018684053757988587\n",
      "Epoch 25/100, Training Loss: 2.3904523914097808e-05 Validation Loss: 0.001867088711895907\n",
      "Epoch 26/100, Training Loss: 2.4342363758478314e-05 Validation Loss: 0.0018646532017338347\n",
      "Epoch 27/100, Training Loss: 2.4730527002247982e-05 Validation Loss: 0.001863698955385228\n",
      "Epoch 28/100, Training Loss: 2.4144908820744604e-05 Validation Loss: 0.0018653659907265467\n",
      "Epoch 29/100, Training Loss: 2.5006747819134034e-05 Validation Loss: 0.0018632026059335418\n",
      "Epoch 30/100, Training Loss: 2.5160828954540193e-05 Validation Loss: 0.0018629674926511984\n",
      "Epoch 31/100, Training Loss: 2.483869502611924e-05 Validation Loss: 0.0018634908127754183\n",
      "Epoch 32/100, Training Loss: 2.514827974664513e-05 Validation Loss: 0.0018629857888225654\n",
      "Epoch 33/100, Training Loss: 2.5137904231087305e-05 Validation Loss: 0.001863000938224148\n",
      "Epoch 34/100, Training Loss: 2.4502756787114777e-05 Validation Loss: 0.0018642079864333145\n",
      "Epoch 35/100, Training Loss: 2.4189739633584395e-05 Validation Loss: 0.001865180329624803\n",
      "Epoch 36/100, Training Loss: 2.5006302166730165e-05 Validation Loss: 0.0018632033754225983\n",
      "Epoch 37/100, Training Loss: 2.5157978598144837e-05 Validation Loss: 0.0018629716424269469\n",
      "Epoch 38/100, Training Loss: 2.512103492335882e-05 Validation Loss: 0.0018630258583399843\n",
      "Epoch 39/100, Training Loss: 2.4246870452770963e-05 Validation Loss: 0.0018649650120005682\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002627828798722476\n",
      "Validation Loss: 0.03849256783723831\n",
      "Validation Loss: 0.00031393562676385045\n",
      "Validation Loss: 0.0002760876377578825\n",
      "Validation Loss: 0.0002404586412012577\n",
      "Validation Loss: 0.00021486803598236293\n",
      "Validation Loss: 0.00020726626098621637\n",
      "Validation Loss: 0.002130494685843587\n",
      "Validation Loss: 0.00029041385278105736\n",
      "Validation Loss: 0.0006990163819864392\n",
      "Validation Loss: 0.0009654038585722446\n",
      "Validation Loss: 0.0030282780062407255\n",
      "Validation Loss: 0.0002911091432906687\n",
      "Validation Loss: 0.00023376673925668\n",
      "Validation Loss: 0.00023024287656880915\n",
      "Validation Loss: 0.0029632155783474445\n",
      "Validation Loss: 0.0007655792287550867\n",
      "Validation Loss: 0.00032414140878245234\n",
      "Validation Loss: 0.00023229786893352866\n",
      "Validation Loss: 3.857135379803367e-05\n",
      "Trial Number 7 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.9113614800735377e-05 Validation Loss: 0.0018862276187686636\n",
      "Epoch 2/100, Training Loss: 2.873371704481542e-05 Validation Loss: 0.0018852713789480429\n",
      "Epoch 3/100, Training Loss: 2.927692730736453e-05 Validation Loss: 0.001886425451884627\n",
      "Epoch 4/100, Training Loss: 2.953373950731475e-05 Validation Loss: 0.0018869538225996182\n",
      "Epoch 5/100, Training Loss: 2.9703511245315894e-05 Validation Loss: 0.0018873006094317975\n",
      "Epoch 6/100, Training Loss: 2.989280255860649e-05 Validation Loss: 0.001887692217410633\n",
      "Epoch 7/100, Training Loss: 3.002608718816191e-05 Validation Loss: 0.001887964729707796\n",
      "Epoch 8/100, Training Loss: 3.0038991098990664e-05 Validation Loss: 0.0018879826842444886\n",
      "Epoch 9/100, Training Loss: 2.989803033415228e-05 Validation Loss: 0.0018876749211258483\n",
      "Epoch 10/100, Training Loss: 2.9596429158118553e-05 Validation Loss: 0.0018870204803533852\n",
      "Epoch 11/100, Training Loss: 2.9176326279412024e-05 Validation Loss: 0.0018860972007923226\n",
      "Epoch 12/100, Training Loss: 2.8695574656012468e-05 Validation Loss: 0.0018850173094331828\n",
      "Epoch 13/100, Training Loss: 2.7979054721072316e-05 Validation Loss: 0.001883355269344805\n",
      "Epoch 14/100, Training Loss: 2.740908894338645e-05 Validation Loss: 0.0018819702631596273\n",
      "Epoch 15/100, Training Loss: 2.705793122004252e-05 Validation Loss: 0.001881080650647609\n",
      "Epoch 16/100, Training Loss: 2.649102680152282e-05 Validation Loss: 0.0018795841965686614\n",
      "Epoch 17/100, Training Loss: 2.6372887077741325e-05 Validation Loss: 0.001879254337934236\n",
      "Epoch 18/100, Training Loss: 2.5458948584855534e-05 Validation Loss: 0.0018765515529167302\n",
      "Epoch 19/100, Training Loss: 2.493108513590414e-05 Validation Loss: 0.001874737831649629\n",
      "Epoch 20/100, Training Loss: 2.4494989702361636e-05 Validation Loss: 0.0018729631031518014\n",
      "Epoch 21/100, Training Loss: 2.457695154589601e-05 Validation Loss: 0.001873320667010743\n",
      "Epoch 22/100, Training Loss: 2.404326369287446e-05 Validation Loss: 0.001870401989092645\n",
      "Epoch 23/100, Training Loss: 2.40127101278631e-05 Validation Loss: 0.0018701500107488312\n",
      "Epoch 24/100, Training Loss: 2.3884473193902522e-05 Validation Loss: 0.001868437189685674\n",
      "Epoch 25/100, Training Loss: 2.4170598408090882e-05 Validation Loss: 0.0018652609126100323\n",
      "Epoch 26/100, Training Loss: 2.463786040607374e-05 Validation Loss: 0.001863892556124218\n",
      "Epoch 27/100, Training Loss: 2.470419894962106e-05 Validation Loss: 0.0018637512538767752\n",
      "Epoch 28/100, Training Loss: 2.510777449060697e-05 Validation Loss: 0.001863045728988027\n",
      "Epoch 29/100, Training Loss: 2.5075323719647713e-05 Validation Loss: 0.001863094842154233\n",
      "Epoch 30/100, Training Loss: 2.5170207663904876e-05 Validation Loss: 0.001862953989035355\n",
      "Epoch 31/100, Training Loss: 2.4345774363609962e-05 Validation Loss: 0.0018646402171207043\n",
      "Epoch 32/100, Training Loss: 2.4441678760922514e-05 Validation Loss: 0.0018643665107866473\n",
      "Epoch 33/100, Training Loss: 2.4234739612438716e-05 Validation Loss: 0.0018650098037626495\n",
      "Epoch 34/100, Training Loss: 2.4471833967254497e-05 Validation Loss: 0.0018642863730619495\n",
      "Epoch 35/100, Training Loss: 2.5049188479897566e-05 Validation Loss: 0.0018631353838212533\n",
      "Epoch 36/100, Training Loss: 2.504559961380437e-05 Validation Loss: 0.0018631409537266752\n",
      "Epoch 37/100, Training Loss: 2.4878459953470156e-05 Validation Loss: 0.0018634191794208144\n",
      "Epoch 38/100, Training Loss: 2.449305247864686e-05 Validation Loss: 0.0018642322407483392\n",
      "Epoch 39/100, Training Loss: 2.503056384739466e-05 Validation Loss: 0.001863164707193788\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00026139291003346443\n",
      "Validation Loss: 0.038514528423547745\n",
      "Validation Loss: 0.0003110491088591516\n",
      "Validation Loss: 0.0002733966684900224\n",
      "Validation Loss: 0.0002379894140176475\n",
      "Validation Loss: 0.00021264243696350604\n",
      "Validation Loss: 0.00020507049339357764\n",
      "Validation Loss: 0.0021334984339773655\n",
      "Validation Loss: 0.0002894563367590308\n",
      "Validation Loss: 0.0006979672471061349\n",
      "Validation Loss: 0.0009655433241277933\n",
      "Validation Loss: 0.003030096646398306\n",
      "Validation Loss: 0.0002883572888094932\n",
      "Validation Loss: 0.00023128566681407392\n",
      "Validation Loss: 0.00022797721612732857\n",
      "Validation Loss: 0.002965771360322833\n",
      "Validation Loss: 0.0007671213243156672\n",
      "Validation Loss: 0.00032121650292538106\n",
      "Validation Loss: 0.00023001519730314612\n",
      "Validation Loss: 3.7753870856249705e-05\n",
      "Trial Number 8 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.0001175155775854364 Validation Loss: 0.00201499943672864\n",
      "Epoch 2/100, Training Loss: 5.2957508160034195e-05 Validation Loss: 0.001927365850490525\n",
      "Epoch 3/100, Training Loss: 3.6803510738536716e-05 Validation Loss: 0.0019011015873147815\n",
      "Epoch 4/100, Training Loss: 3.400382411200553e-05 Validation Loss: 0.0018959172592335656\n",
      "Epoch 5/100, Training Loss: 3.189139169990085e-05 Validation Loss: 0.0018918093679835151\n",
      "Epoch 6/100, Training Loss: 3.067869329242967e-05 Validation Loss: 0.0018893321072068845\n",
      "Epoch 7/100, Training Loss: 2.9945987989776768e-05 Validation Loss: 0.0018877809476591488\n",
      "Epoch 8/100, Training Loss: 2.9458862627507187e-05 Validation Loss: 0.001886722710111357\n",
      "Epoch 9/100, Training Loss: 2.903782660723664e-05 Validation Loss: 0.0018857892152617734\n",
      "Epoch 10/100, Training Loss: 2.8557375117088668e-05 Validation Loss: 0.0018847003779528784\n",
      "Epoch 11/100, Training Loss: 2.7824726203107275e-05 Validation Loss: 0.0018829833809735548\n",
      "Epoch 12/100, Training Loss: 2.7282501832814887e-05 Validation Loss: 0.0018816489282978493\n",
      "Epoch 13/100, Training Loss: 2.663420673343353e-05 Validation Loss: 0.0018799694580408281\n",
      "Epoch 14/100, Training Loss: 2.710740409384016e-05 Validation Loss: 0.0018811977292123548\n",
      "Epoch 15/100, Training Loss: 2.5744655431481078e-05 Validation Loss: 0.001877476166262983\n",
      "Epoch 16/100, Training Loss: 2.5218361770384945e-05 Validation Loss: 0.0018757534499598582\n",
      "Epoch 17/100, Training Loss: 2.453689739922993e-05 Validation Loss: 0.001873149967149483\n",
      "Epoch 18/100, Training Loss: 2.413670881651342e-05 Validation Loss: 0.0018710655608565805\n",
      "Epoch 19/100, Training Loss: 2.400540870439727e-05 Validation Loss: 0.0018700892026466292\n",
      "Epoch 20/100, Training Loss: 2.3958908059285022e-05 Validation Loss: 0.001869640043137609\n",
      "Epoch 21/100, Training Loss: 2.3889686417533085e-05 Validation Loss: 0.0018685972131514226\n",
      "Epoch 22/100, Training Loss: 2.3962182240211405e-05 Validation Loss: 0.0018664464072023876\n",
      "Epoch 23/100, Training Loss: 2.4105520424200222e-05 Validation Loss: 0.0018655535694993314\n",
      "Epoch 24/100, Training Loss: 2.4184400899684988e-05 Validation Loss: 0.0018652067496017068\n",
      "Epoch 25/100, Training Loss: 2.435473470541183e-05 Validation Loss: 0.0018646170558993848\n",
      "Epoch 26/100, Training Loss: 2.4489367206115276e-05 Validation Loss: 0.0018642435541215761\n",
      "Epoch 27/100, Training Loss: 2.4695727915968746e-05 Validation Loss: 0.001863770334447843\n",
      "Epoch 28/100, Training Loss: 2.487221900082659e-05 Validation Loss: 0.001863431314189618\n",
      "Epoch 29/100, Training Loss: 2.5024131900863722e-05 Validation Loss: 0.001863175234358083\n",
      "Epoch 30/100, Training Loss: 2.4836312150000595e-05 Validation Loss: 0.001863495023940597\n",
      "Epoch 31/100, Training Loss: 2.4556044081691653e-05 Validation Loss: 0.001864077925563781\n",
      "Epoch 32/100, Training Loss: 2.493399369996041e-05 Validation Loss: 0.0018633227922758018\n",
      "Epoch 33/100, Training Loss: 2.4538519937777892e-05 Validation Loss: 0.0018641185974961772\n",
      "Epoch 34/100, Training Loss: 2.4631517590023577e-05 Validation Loss: 0.0018639054925117086\n",
      "Epoch 35/100, Training Loss: 2.5093628210015595e-05 Validation Loss: 0.0018630670239457147\n",
      "Epoch 36/100, Training Loss: 2.5171819288516417e-05 Validation Loss: 0.0018629516658447043\n",
      "Epoch 37/100, Training Loss: 2.4515358745702542e-05 Validation Loss: 0.0018641751957433408\n",
      "Epoch 38/100, Training Loss: 2.5100764105445705e-05 Validation Loss: 0.0018630562623286977\n",
      "Epoch 39/100, Training Loss: 2.471718289598357e-05 Validation Loss: 0.0018637248470012742\n",
      "Epoch 40/100, Training Loss: 2.4516062694601715e-05 Validation Loss: 0.0018641746499763337\n",
      "Epoch 41/100, Training Loss: 2.4593826310592704e-05 Validation Loss: 0.0018639897424548003\n",
      "Epoch 42/100, Training Loss: 2.509815567464102e-05 Validation Loss: 0.001863060193934896\n",
      "Epoch 43/100, Training Loss: 2.454418790875934e-05 Validation Loss: 0.0018641062479891435\n",
      "Epoch 44/100, Training Loss: 2.463776218064595e-05 Validation Loss: 0.0018638916304165303\n",
      "Epoch 45/100, Training Loss: 2.491270970494952e-05 Validation Loss: 0.0018633591761814563\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00026974157663062215\n",
      "Validation Loss: 0.038385841995477676\n",
      "Validation Loss: 0.000328181340591982\n",
      "Validation Loss: 0.0002893811324611306\n",
      "Validation Loss: 0.0002526725293137133\n",
      "Validation Loss: 0.0002258955646539107\n",
      "Validation Loss: 0.00021814837236888707\n",
      "Validation Loss: 0.002116058487445116\n",
      "Validation Loss: 0.0002952671202365309\n",
      "Validation Loss: 0.0007043150835670531\n",
      "Validation Loss: 0.0009649156709201634\n",
      "Validation Loss: 0.003019612981006503\n",
      "Validation Loss: 0.00030469935154542327\n",
      "Validation Loss: 0.0002460381365381181\n",
      "Validation Loss: 0.00024146554642356932\n",
      "Validation Loss: 0.002950961235910654\n",
      "Validation Loss: 0.0007582615362480283\n",
      "Validation Loss: 0.0003385739400982857\n",
      "Validation Loss: 0.00024360328097827733\n",
      "Validation Loss: 4.27425948146265e-05\n",
      "Trial Number 9 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.0001130624586949125 Validation Loss: 0.0020101473672716756\n",
      "Epoch 2/100, Training Loss: 5.068422251497395e-05 Validation Loss: 0.0019238402994373556\n",
      "Epoch 3/100, Training Loss: 3.674791150842793e-05 Validation Loss: 0.0019009771749590866\n",
      "Epoch 4/100, Training Loss: 3.190991628798656e-05 Validation Loss: 0.0018918881661833602\n",
      "Epoch 5/100, Training Loss: 2.941666025435552e-05 Validation Loss: 0.0018866769202095471\n",
      "Epoch 6/100, Training Loss: 2.797468005155679e-05 Validation Loss: 0.0018833751639496987\n",
      "Epoch 7/100, Training Loss: 2.7224374207435176e-05 Validation Loss: 0.0018815269410741961\n",
      "Epoch 8/100, Training Loss: 2.693005808396265e-05 Validation Loss: 0.0018807666964298923\n",
      "Epoch 9/100, Training Loss: 2.6864569008466788e-05 Validation Loss: 0.0018805910162263354\n",
      "Epoch 10/100, Training Loss: 2.8305848900345154e-05 Validation Loss: 0.0018841106332798607\n",
      "Epoch 11/100, Training Loss: 2.7026642783312127e-05 Validation Loss: 0.0018810006621541278\n",
      "Epoch 12/100, Training Loss: 2.6453544705873355e-05 Validation Loss: 0.0018794822027095237\n",
      "Epoch 13/100, Training Loss: 2.6503603294258937e-05 Validation Loss: 0.0018796120374238903\n",
      "Epoch 14/100, Training Loss: 2.5880925022647716e-05 Validation Loss: 0.0018778460310913806\n",
      "Epoch 15/100, Training Loss: 2.496809247531928e-05 Validation Loss: 0.0018748748085001385\n",
      "Epoch 16/100, Training Loss: 2.455180583638139e-05 Validation Loss: 0.0018732171782815483\n",
      "Epoch 17/100, Training Loss: 2.4226737878052518e-05 Validation Loss: 0.0018716114423450358\n",
      "Epoch 18/100, Training Loss: 2.3993809008970857e-05 Validation Loss: 0.0018699882047436532\n",
      "Epoch 19/100, Training Loss: 2.3881859306129627e-05 Validation Loss: 0.0018683432740267138\n",
      "Epoch 20/100, Training Loss: 2.3915383280836977e-05 Validation Loss: 0.0018669353458143987\n",
      "Epoch 21/100, Training Loss: 2.4071028747130185e-05 Validation Loss: 0.001865727259475031\n",
      "Epoch 22/100, Training Loss: 2.3903699911898002e-05 Validation Loss: 0.001867103728411833\n",
      "Epoch 23/100, Training Loss: 2.4088034479063936e-05 Validation Loss: 0.001865636481664424\n",
      "Epoch 24/100, Training Loss: 2.4516697521903552e-05 Validation Loss: 0.0018641744877061022\n",
      "Epoch 25/100, Training Loss: 2.461464100633748e-05 Validation Loss: 0.0018639434602521839\n",
      "Epoch 26/100, Training Loss: 2.495482658559922e-05 Validation Loss: 0.001863288405772582\n",
      "Epoch 27/100, Training Loss: 2.509549267415423e-05 Validation Loss: 0.0018630643785229435\n",
      "Epoch 28/100, Training Loss: 2.4492617740179412e-05 Validation Loss: 0.0018642327391631334\n",
      "Epoch 29/100, Training Loss: 2.4678764020791277e-05 Validation Loss: 0.0018638038998050465\n",
      "Epoch 30/100, Training Loss: 2.4643664801260456e-05 Validation Loss: 0.0018638789029028326\n",
      "Epoch 31/100, Training Loss: 2.5110586648224853e-05 Validation Loss: 0.001863041523500123\n",
      "Epoch 32/100, Training Loss: 2.456677975715138e-05 Validation Loss: 0.0018640526228245959\n",
      "Epoch 33/100, Training Loss: 2.4577411750215106e-05 Validation Loss: 0.001864027068788028\n",
      "Epoch 34/100, Training Loss: 2.5028240997926332e-05 Validation Loss: 0.0018631682704634055\n",
      "Epoch 35/100, Training Loss: 2.5167339117615484e-05 Validation Loss: 0.0018629581048722316\n",
      "Epoch 36/100, Training Loss: 2.5119863494182937e-05 Validation Loss: 0.0018630276224126745\n",
      "Epoch 37/100, Training Loss: 2.4814100470393896e-05 Validation Loss: 0.0018635363064607503\n",
      "Epoch 38/100, Training Loss: 2.4582826881669462e-05 Validation Loss: 0.001864015273096006\n",
      "Epoch 39/100, Training Loss: 2.444261554046534e-05 Validation Loss: 0.0018643640995046598\n",
      "Epoch 40/100, Training Loss: 2.4373681299039163e-05 Validation Loss: 0.0018645571539151538\n",
      "Epoch 41/100, Training Loss: 2.4568545995862223e-05 Validation Loss: 0.0018640479441888885\n",
      "Epoch 42/100, Training Loss: 2.509400474082213e-05 Validation Loss: 0.0018630664225913272\n",
      "Epoch 43/100, Training Loss: 2.4405389922321774e-05 Validation Loss: 0.001864467150896552\n",
      "Epoch 44/100, Training Loss: 2.466526893840637e-05 Validation Loss: 0.0018638321270268822\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002620434679556638\n",
      "Validation Loss: 0.0385042242705822\n",
      "Validation Loss: 0.00031240194221027195\n",
      "Validation Loss: 0.0002746577374637127\n",
      "Validation Loss: 0.00023914642224553972\n",
      "Validation Loss: 0.0002136851253453642\n",
      "Validation Loss: 0.00020609918283298612\n",
      "Validation Loss: 0.0021320872474461794\n",
      "Validation Loss: 0.00028990398277528584\n",
      "Validation Loss: 0.0006984578794799745\n",
      "Validation Loss: 0.0009654762106947601\n",
      "Validation Loss: 0.003029241692274809\n",
      "Validation Loss: 0.0002896469668485224\n",
      "Validation Loss: 0.00023244823387358338\n",
      "Validation Loss: 0.0002290387055836618\n",
      "Validation Loss: 0.0029645704198628664\n",
      "Validation Loss: 0.0007663959986530244\n",
      "Validation Loss: 0.0003225873224437237\n",
      "Validation Loss: 0.00023108467576093972\n",
      "Validation Loss: 3.813581497524865e-05\n",
      "Trial Number 10 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00017496223154012114 Validation Loss: 0.001949858274988357\n",
      "Epoch 2/100, Training Loss: 7.302638550754637e-05 Validation Loss: 0.0018779584855472095\n",
      "Epoch 3/100, Training Loss: 4.303045352571644e-05 Validation Loss: 0.0018627782369612458\n",
      "Epoch 4/100, Training Loss: 3.322510383441113e-05 Validation Loss: 0.0018603366814999934\n",
      "Epoch 5/100, Training Loss: 2.916352059401106e-05 Validation Loss: 0.0018604963711919094\n",
      "Epoch 6/100, Training Loss: 2.7682390282279812e-05 Validation Loss: 0.0018609522364979724\n",
      "Epoch 7/100, Training Loss: 2.7225800295127556e-05 Validation Loss: 0.001861165695345091\n",
      "Epoch 8/100, Training Loss: 2.7422685889177956e-05 Validation Loss: 0.001861067046211695\n",
      "Epoch 9/100, Training Loss: 2.7310057703289203e-05 Validation Loss: 0.0018611208101263369\n",
      "Epoch 10/100, Training Loss: 2.6785479349200614e-05 Validation Loss: 0.0018614140613826085\n",
      "Epoch 11/100, Training Loss: 2.7408743335399777e-05 Validation Loss: 0.0018610720744676936\n",
      "Epoch 12/100, Training Loss: 2.7394547942094505e-05 Validation Loss: 0.0018610789646201899\n",
      "Epoch 13/100, Training Loss: 2.7116926503367722e-05 Validation Loss: 0.0018612208717157276\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00025272570201195776\n",
      "Validation Loss: 0.03865687921643257\n",
      "Validation Loss: 0.0002927027817349881\n",
      "Validation Loss: 0.00025631519383750856\n",
      "Validation Loss: 0.00022234191419556737\n",
      "Validation Loss: 0.00019857072038576007\n",
      "Validation Loss: 0.000191191938938573\n",
      "Validation Loss: 0.002153248991817236\n",
      "Validation Loss: 0.00028358574490994215\n",
      "Validation Loss: 0.0006915050325915217\n",
      "Validation Loss: 0.0009667676058597863\n",
      "Validation Loss: 0.0030421826522797346\n",
      "Validation Loss: 0.0002708816609811038\n",
      "Validation Loss: 0.00021556176943704486\n",
      "Validation Loss: 0.00021364635904319584\n",
      "Validation Loss: 0.002982624340802431\n",
      "Validation Loss: 0.0007774171535857022\n",
      "Validation Loss: 0.0003026220656465739\n",
      "Validation Loss: 0.00021557444415520877\n",
      "Validation Loss: 3.2789219403639436e-05\n",
      "Trial Number 11 ==========================================================\n",
      "Epoch 1/100, Training Loss: 3.5624776501208544e-05 Validation Loss: 0.0018606451373085148\n",
      "Epoch 2/100, Training Loss: 2.966812280646991e-05 Validation Loss: 0.001860409050030456\n",
      "Epoch 3/100, Training Loss: 2.7923091693082824e-05 Validation Loss: 0.0018608617501623833\n",
      "Epoch 4/100, Training Loss: 2.725446756812744e-05 Validation Loss: 0.0018611559784089764\n",
      "Epoch 5/100, Training Loss: 2.723351281019859e-05 Validation Loss: 0.0018611643280577422\n",
      "Epoch 6/100, Training Loss: 2.7527206839295104e-05 Validation Loss: 0.001861020166086011\n",
      "Epoch 7/100, Training Loss: 2.7861840862897225e-05 Validation Loss: 0.0018608776746055218\n",
      "Epoch 8/100, Training Loss: 2.8093872970202938e-05 Validation Loss: 0.0018607906409886658\n",
      "Epoch 9/100, Training Loss: 2.814732761180494e-05 Validation Loss: 0.0018607715430114485\n",
      "Epoch 10/100, Training Loss: 2.8293867217143998e-05 Validation Loss: 0.0018607225330343864\n",
      "Epoch 11/100, Training Loss: 2.8003802071907558e-05 Validation Loss: 0.001860822166270294\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002480383845977485\n",
      "Validation Loss: 0.03873820975422859\n",
      "Validation Loss: 0.00028250194736756384\n",
      "Validation Loss: 0.0002468348538968712\n",
      "Validation Loss: 0.0002136784023605287\n",
      "Validation Loss: 0.00019080484344158322\n",
      "Validation Loss: 0.0001835360744735226\n",
      "Validation Loss: 0.0021647491957992315\n",
      "Validation Loss: 0.00028049139655195177\n",
      "Validation Loss: 0.0006880736327730119\n",
      "Validation Loss: 0.000967714935541153\n",
      "Validation Loss: 0.003049316583201289\n",
      "Validation Loss: 0.00026117684319615364\n",
      "Validation Loss: 0.00020685471827164292\n",
      "Validation Loss: 0.00020573283836711198\n",
      "Validation Loss: 0.0029924737755209208\n",
      "Validation Loss: 0.0007835316937416792\n",
      "Validation Loss: 0.00029227984487079084\n",
      "Validation Loss: 0.000207598292035982\n",
      "Validation Loss: 3.0210925615392625e-05\n",
      "Trial Number 12 ==========================================================\n",
      "Epoch 1/100, Training Loss: 8.683345367899165e-05 Validation Loss: 0.0018864707193212106\n",
      "Epoch 2/100, Training Loss: 5.4878204537089914e-05 Validation Loss: 0.0018678987922175865\n",
      "Epoch 3/100, Training Loss: 4.4178410462336615e-05 Validation Loss: 0.0018631868185560315\n",
      "Epoch 4/100, Training Loss: 3.723871486727148e-05 Validation Loss: 0.001861011423769488\n",
      "Epoch 5/100, Training Loss: 3.337987436680123e-05 Validation Loss: 0.001860350414078447\n",
      "Epoch 6/100, Training Loss: 3.162805660394952e-05 Validation Loss: 0.001860262789713101\n",
      "Epoch 7/100, Training Loss: 3.102718619629741e-05 Validation Loss: 0.0018602747495469836\n",
      "Epoch 8/100, Training Loss: 3.086292781517841e-05 Validation Loss: 0.0018602822346903288\n",
      "Epoch 9/100, Training Loss: 3.078140434809029e-05 Validation Loss: 0.0018602866483033741\n",
      "Epoch 10/100, Training Loss: 3.0760733352508396e-05 Validation Loss: 0.001860287765728256\n",
      "Epoch 11/100, Training Loss: 3.042920070583932e-05 Validation Loss: 0.0018603118388695963\n",
      "Epoch 12/100, Training Loss: 2.9695318517042324e-05 Validation Loss: 0.0018603996878304187\n",
      "Epoch 13/100, Training Loss: 2.9754681236227043e-05 Validation Loss: 0.001860389616844491\n",
      "Epoch 14/100, Training Loss: 2.936129385489039e-05 Validation Loss: 0.001860455025909798\n",
      "Epoch 15/100, Training Loss: 2.8965037927264348e-05 Validation Loss: 0.0018605377328195804\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002450251777190715\n",
      "Validation Loss: 0.038792338222265244\n",
      "Validation Loss: 0.00027582678012549877\n",
      "Validation Loss: 0.00024063819728326052\n",
      "Validation Loss: 0.00020802428480237722\n",
      "Validation Loss: 0.0001857469032984227\n",
      "Validation Loss: 0.00017855121404863894\n",
      "Validation Loss: 0.002172487787902355\n",
      "Validation Loss: 0.0002785363176371902\n",
      "Validation Loss: 0.0006858947454020381\n",
      "Validation Loss: 0.0009684443357400596\n",
      "Validation Loss: 0.003054155269637704\n",
      "Validation Loss: 0.0002548310731071979\n",
      "Validation Loss: 0.0002011717006098479\n",
      "Validation Loss: 0.00020057686197105795\n",
      "Validation Loss: 0.002999116200953722\n",
      "Validation Loss: 0.0007876930758357048\n",
      "Validation Loss: 0.0002855107595678419\n",
      "Validation Loss: 0.00020240074081812054\n",
      "Validation Loss: 2.859859523596242e-05\n",
      "Trial Number 13 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.4514667529729195e-05 Validation Loss: 0.0018730339463670425\n",
      "Epoch 2/100, Training Loss: 2.4617833332740702e-05 Validation Loss: 0.0018639464032015737\n",
      "Epoch 3/100, Training Loss: 2.5235127395717427e-05 Validation Loss: 0.001862869058692746\n",
      "Epoch 4/100, Training Loss: 2.5574792744009756e-05 Validation Loss: 0.0018624424330350151\n",
      "Epoch 5/100, Training Loss: 2.5907846065820195e-05 Validation Loss: 0.0018620960228369718\n",
      "Epoch 6/100, Training Loss: 2.6189631171291694e-05 Validation Loss: 0.0018618456560280873\n",
      "Epoch 7/100, Training Loss: 2.6487308787181973e-05 Validation Loss: 0.0018616156408758904\n",
      "Epoch 8/100, Training Loss: 2.6371892090537585e-05 Validation Loss: 0.0018617003917916692\n",
      "Epoch 9/100, Training Loss: 2.6237605197820812e-05 Validation Loss: 0.00186180515460109\n",
      "Epoch 10/100, Training Loss: 2.5031382392626256e-05 Validation Loss: 0.001863163493941503\n",
      "Epoch 11/100, Training Loss: 2.5469382308074273e-05 Validation Loss: 0.0018625609683509744\n",
      "Epoch 12/100, Training Loss: 2.5453926355112344e-05 Validation Loss: 0.0018625796627425983\n",
      "Epoch 13/100, Training Loss: 2.539236629672814e-05 Validation Loss: 0.0018626550039189636\n",
      "Epoch 14/100, Training Loss: 2.5266532247769646e-05 Validation Loss: 0.0018628196118774824\n",
      "Epoch 15/100, Training Loss: 2.519501322240103e-05 Validation Loss: 0.0018629185644019343\n",
      "Epoch 16/100, Training Loss: 2.51805831794627e-05 Validation Loss: 0.0018629391510104451\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002630232775118202\n",
      "Validation Loss: 0.03848879411816597\n",
      "Validation Loss: 0.0003144334477838129\n",
      "Validation Loss: 0.0002765518147498369\n",
      "Validation Loss: 0.00024088470672722906\n",
      "Validation Loss: 0.00021525216288864613\n",
      "Validation Loss: 0.00020764520741067827\n",
      "Validation Loss: 0.002129979431629181\n",
      "Validation Loss: 0.000290579890133813\n",
      "Validation Loss: 0.000699198164511472\n",
      "Validation Loss: 0.0009653813322074711\n",
      "Validation Loss: 0.0030279664788395166\n",
      "Validation Loss: 0.0002915837976615876\n",
      "Validation Loss: 0.00023419479839503765\n",
      "Validation Loss: 0.0002306339010829106\n",
      "Validation Loss: 0.0029627771582454443\n",
      "Validation Loss: 0.0007653153734281659\n",
      "Validation Loss: 0.00032464583637192845\n",
      "Validation Loss: 0.000232691818382591\n",
      "Validation Loss: 3.871330409310758e-05\n",
      "Trial Number 14 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00016815740673337132 Validation Loss: 0.0019448783700287018\n",
      "Epoch 2/100, Training Loss: 7.608262239955366e-05 Validation Loss: 0.001879790312110717\n",
      "Epoch 3/100, Training Loss: 4.868080577580258e-05 Validation Loss: 0.001865040996830136\n",
      "Epoch 4/100, Training Loss: 3.7959169276291505e-05 Validation Loss: 0.001861191223428404\n",
      "Epoch 5/100, Training Loss: 3.33614407281857e-05 Validation Loss: 0.0018603492567878684\n",
      "Epoch 6/100, Training Loss: 3.1272116757463664e-05 Validation Loss: 0.0018602668507112488\n",
      "Epoch 7/100, Training Loss: 3.0344892365974374e-05 Validation Loss: 0.001860319941837664\n",
      "Epoch 8/100, Training Loss: 2.9943446861580014e-05 Validation Loss: 0.0018603642801040463\n",
      "Epoch 9/100, Training Loss: 2.9751097827102058e-05 Validation Loss: 0.0018603904820985629\n",
      "Epoch 10/100, Training Loss: 2.9588787583634257e-05 Validation Loss: 0.0018604153931058049\n",
      "Epoch 11/100, Training Loss: 2.933547875727527e-05 Validation Loss: 0.0018604599048098048\n",
      "Epoch 12/100, Training Loss: 2.8986514735152014e-05 Validation Loss: 0.0018605327984569683\n",
      "Epoch 13/100, Training Loss: 2.8572430892381817e-05 Validation Loss: 0.0018606383738029122\n",
      "Epoch 14/100, Training Loss: 2.8155645850347355e-05 Validation Loss: 0.0018607682238022487\n",
      "Epoch 15/100, Training Loss: 2.7811702238977887e-05 Validation Loss: 0.0018608958788799063\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00024932160158641636\n",
      "Validation Loss: 0.03871561214327812\n",
      "Validation Loss: 0.0002853159967344254\n",
      "Validation Loss: 0.0002494488435331732\n",
      "Validation Loss: 0.0002160656003979966\n",
      "Validation Loss: 0.0001929428253788501\n",
      "Validation Loss: 0.00018564351194072515\n",
      "Validation Loss: 0.0021615379955619574\n",
      "Validation Loss: 0.0002813323517329991\n",
      "Validation Loss: 0.0006890080985613167\n",
      "Validation Loss: 0.0009674336761236191\n",
      "Validation Loss: 0.0030473177321255207\n",
      "Validation Loss: 0.00026385317323729396\n",
      "Validation Loss: 0.0002092540089506656\n",
      "Validation Loss: 0.0002079118276014924\n",
      "Validation Loss: 0.002989721018821001\n",
      "Validation Loss: 0.000781815848313272\n",
      "Validation Loss: 0.0002951331262011081\n",
      "Validation Loss: 0.0002097946562571451\n",
      "Validation Loss: 3.0908602639101446e-05\n",
      "Trial Number 15 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00015992364205885679 Validation Loss: 0.0020685284104650405\n",
      "Epoch 2/100, Training Loss: 4.9587029934627935e-05 Validation Loss: 0.0019219518404070682\n",
      "Epoch 3/100, Training Loss: 3.769643808482215e-05 Validation Loss: 0.0019024989119379523\n",
      "Epoch 4/100, Training Loss: 3.289029700681567e-05 Validation Loss: 0.0018937022124900255\n",
      "Epoch 5/100, Training Loss: 3.028390528925229e-05 Validation Loss: 0.0018884687819108886\n",
      "Epoch 6/100, Training Loss: 2.84970647044247e-05 Validation Loss: 0.0018845551260692115\n",
      "Epoch 7/100, Training Loss: 2.7248022888670675e-05 Validation Loss: 0.001881561088320846\n",
      "Epoch 8/100, Training Loss: 2.636668796185404e-05 Validation Loss: 0.0018792430704159088\n",
      "Epoch 9/100, Training Loss: 2.5707329768920317e-05 Validation Loss: 0.001877332617624065\n",
      "Epoch 10/100, Training Loss: 2.62840239884099e-05 Validation Loss: 0.0018790075182365722\n",
      "Epoch 11/100, Training Loss: 2.5250163162127137e-05 Validation Loss: 0.001875865322526241\n",
      "Epoch 12/100, Training Loss: 2.47603929892648e-05 Validation Loss: 0.0018740860131541256\n",
      "Epoch 13/100, Training Loss: 2.5053044737433083e-05 Validation Loss: 0.001875177553407189\n",
      "Epoch 14/100, Training Loss: 2.4335633497685194e-05 Validation Loss: 0.0018721986602565587\n",
      "Epoch 15/100, Training Loss: 2.4076518457150087e-05 Validation Loss: 0.0018706572946936635\n",
      "Epoch 16/100, Training Loss: 2.4055474568740465e-05 Validation Loss: 0.0018704985340159798\n",
      "Epoch 17/100, Training Loss: 2.3971811970113777e-05 Validation Loss: 0.0018697790038524373\n",
      "Epoch 18/100, Training Loss: 2.388214306847658e-05 Validation Loss: 0.0018683568319197972\n",
      "Epoch 19/100, Training Loss: 2.38779466599226e-05 Validation Loss: 0.0018678806488959185\n",
      "Epoch 20/100, Training Loss: 2.3880509615992196e-05 Validation Loss: 0.0018676818190751416\n",
      "Epoch 21/100, Training Loss: 2.3951852199388668e-05 Validation Loss: 0.0018665393486160821\n",
      "Epoch 22/100, Training Loss: 2.401977872068528e-05 Validation Loss: 0.001866025915371887\n",
      "Epoch 23/100, Training Loss: 2.398509423073847e-05 Validation Loss: 0.0018662622755334492\n",
      "Epoch 24/100, Training Loss: 2.4202277927543037e-05 Validation Loss: 0.00186513678923385\n",
      "Epoch 25/100, Training Loss: 2.40137360378867e-05 Validation Loss: 0.0018660614251020382\n",
      "Epoch 26/100, Training Loss: 2.435773603792768e-05 Validation Loss: 0.0018646068610116192\n",
      "Epoch 27/100, Training Loss: 2.4356375433853827e-05 Validation Loss: 0.0018646098450744587\n",
      "Epoch 28/100, Training Loss: 2.4806702640489675e-05 Validation Loss: 0.0018635512279603123\n",
      "Epoch 29/100, Training Loss: 2.4585151550127193e-05 Validation Loss: 0.0018640098187948666\n",
      "Epoch 30/100, Training Loss: 2.4546514396206476e-05 Validation Loss: 0.0018640998287384957\n",
      "Epoch 31/100, Training Loss: 2.452095941407606e-05 Validation Loss: 0.0018641612378207594\n",
      "Epoch 32/100, Training Loss: 2.511745879019145e-05 Validation Loss: 0.001863031235717173\n",
      "Epoch 33/100, Training Loss: 2.5159475626423955e-05 Validation Loss: 0.0018629694575496769\n",
      "Epoch 34/100, Training Loss: 2.4487739210599102e-05 Validation Loss: 0.001864245313639969\n",
      "Epoch 35/100, Training Loss: 2.5057923267013393e-05 Validation Loss: 0.0018631218289227763\n",
      "Epoch 36/100, Training Loss: 2.5167468265863135e-05 Validation Loss: 0.0018629579296253725\n",
      "Epoch 37/100, Training Loss: 2.515155210858211e-05 Validation Loss: 0.0018629810283966723\n",
      "Epoch 38/100, Training Loss: 2.4657751055201516e-05 Validation Loss: 0.001863848837242249\n",
      "Epoch 39/100, Training Loss: 2.4745189875829965e-05 Validation Loss: 0.0018636685517708404\n",
      "Epoch 40/100, Training Loss: 2.492732892278582e-05 Validation Loss: 0.0018633341581170374\n",
      "Epoch 41/100, Training Loss: 2.452002627251204e-05 Validation Loss: 0.0018641643434147064\n",
      "Epoch 42/100, Training Loss: 2.504083931853529e-05 Validation Loss: 0.0018631484726841171\n",
      "Epoch 43/100, Training Loss: 2.5109029593295418e-05 Validation Loss: 0.0018630437841159785\n",
      "Epoch 44/100, Training Loss: 2.517308712413069e-05 Validation Loss: 0.0018629498552308404\n",
      "Epoch 45/100, Training Loss: 2.5083456421270967e-05 Validation Loss: 0.001863082436747881\n",
      "Epoch 46/100, Training Loss: 2.4511358788004145e-05 Validation Loss: 0.0018641860488830145\n",
      "Epoch 47/100, Training Loss: 2.5056175218196586e-05 Validation Loss: 0.0018631245595046658\n",
      "Epoch 48/100, Training Loss: 2.5166920750052668e-05 Validation Loss: 0.0018629587321174865\n",
      "Epoch 49/100, Training Loss: 2.5142910089925863e-05 Validation Loss: 0.0018629936590847647\n",
      "Epoch 50/100, Training Loss: 2.4290653527714312e-05 Validation Loss: 0.0018648152778150727\n",
      "Epoch 51/100, Training Loss: 2.5053403078345582e-05 Validation Loss: 0.0018631287910082124\n",
      "Epoch 52/100, Training Loss: 2.4343795303138904e-05 Validation Loss: 0.0018646471732793155\n",
      "Epoch 53/100, Training Loss: 2.4522922103642486e-05 Validation Loss: 0.0018641572830687494\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.000262048706645146\n",
      "Validation Loss: 0.03850414231419563\n",
      "Validation Loss: 0.0003124127397313714\n",
      "Validation Loss: 0.00027466777828522027\n",
      "Validation Loss: 0.00023915566271170974\n",
      "Validation Loss: 0.0002136934344889596\n",
      "Validation Loss: 0.00020610739011317492\n",
      "Validation Loss: 0.002132076071575284\n",
      "Validation Loss: 0.000289907562546432\n",
      "Validation Loss: 0.0006984618958085775\n",
      "Validation Loss: 0.0009654757450334728\n",
      "Validation Loss: 0.003029234940186143\n",
      "Validation Loss: 0.0002896572113968432\n",
      "Validation Loss: 0.00023245751799549907\n",
      "Validation Loss: 0.0002290471747983247\n",
      "Validation Loss: 0.0029645608738064766\n",
      "Validation Loss: 0.0007663902943022549\n",
      "Validation Loss: 0.0003225982654839754\n",
      "Validation Loss: 0.00023109321773517877\n",
      "Validation Loss: 3.81388672394678e-05\n",
      "Trial Number 16 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.0002291967539349571 Validation Loss: 0.001992721125430608\n",
      "Epoch 2/100, Training Loss: 8.335815073223785e-05 Validation Loss: 0.001884406955972851\n",
      "Epoch 3/100, Training Loss: 5.5984652135521173e-05 Validation Loss: 0.0018685335300399624\n",
      "Epoch 4/100, Training Loss: 4.426868326845579e-05 Validation Loss: 0.0018632505084677437\n",
      "Epoch 5/100, Training Loss: 3.7301044358173385e-05 Validation Loss: 0.0018610360574637043\n",
      "Epoch 6/100, Training Loss: 3.332006963319145e-05 Validation Loss: 0.0018603470120184715\n",
      "Epoch 7/100, Training Loss: 3.1237188522936776e-05 Validation Loss: 0.001860267531173154\n",
      "Epoch 8/100, Training Loss: 3.0262997825047933e-05 Validation Loss: 0.001860327257349037\n",
      "Epoch 9/100, Training Loss: 2.9872655431972817e-05 Validation Loss: 0.001860373072892164\n",
      "Epoch 10/100, Training Loss: 2.9553186323028058e-05 Validation Loss: 0.0018604211534806682\n",
      "Epoch 11/100, Training Loss: 2.9497034120140597e-05 Validation Loss: 0.001860430660669563\n",
      "Epoch 12/100, Training Loss: 2.9005526812397875e-05 Validation Loss: 0.0018605284250839459\n",
      "Epoch 13/100, Training Loss: 2.9013170205871575e-05 Validation Loss: 0.0018605267884068007\n",
      "Epoch 14/100, Training Loss: 2.8317595933913253e-05 Validation Loss: 0.0018607146235303663\n",
      "Epoch 15/100, Training Loss: 2.8119300623075105e-05 Validation Loss: 0.0018607807812472607\n",
      "Epoch 16/100, Training Loss: 2.7832560590468347e-05 Validation Loss: 0.001860887429598092\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002488108293619007\n",
      "Validation Loss: 0.03872457891702652\n",
      "Validation Loss: 0.0002841978566721082\n",
      "Validation Loss: 0.0002484100987203419\n",
      "Validation Loss: 0.00021511684462893754\n",
      "Validation Loss: 0.00019209293532185256\n",
      "Validation Loss: 0.0001848057290771976\n",
      "Validation Loss: 0.0021628104150295258\n",
      "Validation Loss: 0.0002809970173984766\n",
      "Validation Loss: 0.0006886357441544533\n",
      "Validation Loss: 0.0009675435139797628\n",
      "Validation Loss: 0.0030481091234833\n",
      "Validation Loss: 0.00026278969016857445\n",
      "Validation Loss: 0.0002083004656014964\n",
      "Validation Loss: 0.00020704566850326955\n",
      "Validation Loss: 0.002990811364725232\n",
      "Validation Loss: 0.0007824948988854885\n",
      "Validation Loss: 0.0002939994737971574\n",
      "Validation Loss: 0.00020892161410301924\n",
      "Validation Loss: 3.063013718929142e-05\n",
      "Trial Number 17 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.9928130970802158e-05 Validation Loss: 0.0018878126927324055\n",
      "Epoch 2/100, Training Loss: 2.42695114138769e-05 Validation Loss: 0.0018719005660355562\n",
      "Epoch 3/100, Training Loss: 2.392995884292759e-05 Validation Loss: 0.0018693338602976943\n",
      "Epoch 4/100, Training Loss: 2.3885711925686337e-05 Validation Loss: 0.0018685095875383237\n",
      "Epoch 5/100, Training Loss: 2.3881637389422394e-05 Validation Loss: 0.0018683524256061048\n",
      "Epoch 6/100, Training Loss: 2.3881813831394538e-05 Validation Loss: 0.0018683564815508544\n",
      "Epoch 7/100, Training Loss: 2.388101893302519e-05 Validation Loss: 0.0018683164805350556\n",
      "Epoch 8/100, Training Loss: 2.3878934371168725e-05 Validation Loss: 0.0018681889893497079\n",
      "Epoch 9/100, Training Loss: 2.3877631974755786e-05 Validation Loss: 0.0018679850574059758\n",
      "Epoch 10/100, Training Loss: 2.388011671428103e-05 Validation Loss: 0.0018677082951385267\n",
      "Epoch 11/100, Training Loss: 2.3890752345323563e-05 Validation Loss: 0.0018673579981909928\n",
      "Epoch 12/100, Training Loss: 2.3915039491839707e-05 Validation Loss: 0.0018669428526062522\n",
      "Epoch 13/100, Training Loss: 2.39390956267016e-05 Validation Loss: 0.0018666620444384625\n",
      "Epoch 14/100, Training Loss: 2.39082128246082e-05 Validation Loss: 0.0018670337484542597\n",
      "Epoch 15/100, Training Loss: 2.416678580630105e-05 Validation Loss: 0.0018652784920098922\n",
      "Epoch 16/100, Training Loss: 2.4414788640569896e-05 Validation Loss: 0.0018644422901737416\n",
      "Epoch 17/100, Training Loss: 2.4691345970495604e-05 Validation Loss: 0.0018637794388621839\n",
      "Epoch 18/100, Training Loss: 2.4897108232835308e-05 Validation Loss: 0.0018633872884228858\n",
      "Epoch 19/100, Training Loss: 2.4472372388117947e-05 Validation Loss: 0.0018642854129162886\n",
      "Epoch 20/100, Training Loss: 2.501600465620868e-05 Validation Loss: 0.0018631878926838965\n",
      "Epoch 21/100, Training Loss: 2.516046879463829e-05 Validation Loss: 0.00186296809363126\n",
      "Epoch 22/100, Training Loss: 2.5177245333907194e-05 Validation Loss: 0.0018629438720697423\n",
      "Epoch 23/100, Training Loss: 2.517851498851087e-05 Validation Loss: 0.0018629420796106794\n",
      "Epoch 24/100, Training Loss: 2.525066156522371e-05 Validation Loss: 0.0018628411903869589\n",
      "Epoch 25/100, Training Loss: 2.462622978782747e-05 Validation Loss: 0.0018639173949489695\n",
      "Epoch 26/100, Training Loss: 2.4932385713327676e-05 Validation Loss: 0.0018633254943465432\n",
      "Epoch 27/100, Training Loss: 2.4652918000356294e-05 Validation Loss: 0.0018638591424313484\n",
      "Epoch 28/100, Training Loss: 2.5100938728428446e-05 Validation Loss: 0.0018630560293108913\n",
      "Epoch 29/100, Training Loss: 2.4629855033708736e-05 Validation Loss: 0.0018639094319787487\n",
      "Epoch 30/100, Training Loss: 2.4969160222099163e-05 Validation Loss: 0.0018632640270566186\n",
      "Epoch 31/100, Training Loss: 2.5037692466867156e-05 Validation Loss: 0.0018631533209517966\n",
      "Epoch 32/100, Training Loss: 2.5158915377687663e-05 Validation Loss: 0.001862970262911622\n",
      "Epoch 33/100, Training Loss: 2.471367406542413e-05 Validation Loss: 0.0018637320072298306\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00026171506033279\n",
      "Validation Loss: 0.03850942477583885\n",
      "Validation Loss: 0.00031171939917840064\n",
      "Validation Loss: 0.0002740214695222676\n",
      "Validation Loss: 0.00023856261395849288\n",
      "Validation Loss: 0.00021315897174645215\n",
      "Validation Loss: 0.00020558011601679027\n",
      "Validation Loss: 0.0021327983122318983\n",
      "Validation Loss: 0.0002896778751164675\n",
      "Validation Loss: 0.0006982102058827877\n",
      "Validation Loss: 0.0009655096800997853\n",
      "Validation Loss: 0.003029672894626856\n",
      "Validation Loss: 0.00028899620519950986\n",
      "Validation Loss: 0.00023186166072264314\n",
      "Validation Loss: 0.00022850307868793607\n",
      "Validation Loss: 0.002965175546705723\n",
      "Validation Loss: 0.0007667613681405783\n",
      "Validation Loss: 0.0003218957281205803\n",
      "Validation Loss: 0.0002305450470885262\n",
      "Validation Loss: 3.794284930336289e-05\n",
      "Trial Number 18 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.0001350731763523072 Validation Loss: 0.0020386534498451478\n",
      "Epoch 2/100, Training Loss: 6.189047417137772e-05 Validation Loss: 0.001940879479923065\n",
      "Epoch 3/100, Training Loss: 4.345969864516519e-05 Validation Loss: 0.0019125214778261193\n",
      "Epoch 4/100, Training Loss: 3.62421887984965e-05 Validation Loss: 0.0019001336913008846\n",
      "Epoch 5/100, Training Loss: 3.347842357470654e-05 Validation Loss: 0.001894970625486237\n",
      "Epoch 6/100, Training Loss: 3.2222851586993784e-05 Validation Loss: 0.0018924919556849312\n",
      "Epoch 7/100, Training Loss: 3.137930252705701e-05 Validation Loss: 0.0018907772191966877\n",
      "Epoch 8/100, Training Loss: 3.067018405999988e-05 Validation Loss: 0.0018893088353722927\n",
      "Epoch 9/100, Training Loss: 3.0080709620960988e-05 Validation Loss: 0.0018880650785922334\n",
      "Epoch 10/100, Training Loss: 2.9590904887299985e-05 Validation Loss: 0.0018870102592004265\n",
      "Epoch 11/100, Training Loss: 2.995406430272851e-05 Validation Loss: 0.0018877881254186297\n",
      "Epoch 12/100, Training Loss: 3.0097678973106667e-05 Validation Loss: 0.0018880847623893174\n",
      "Epoch 13/100, Training Loss: 2.9677174097741954e-05 Validation Loss: 0.001887181662045527\n",
      "Epoch 14/100, Training Loss: 2.8483604182838462e-05 Validation Loss: 0.0018845290967012733\n",
      "Epoch 15/100, Training Loss: 2.771159961412195e-05 Validation Loss: 0.0018827076731818665\n",
      "Epoch 16/100, Training Loss: 2.7006259188055992e-05 Validation Loss: 0.0018809475788885543\n",
      "Epoch 17/100, Training Loss: 2.648161716933828e-05 Validation Loss: 0.0018795573095579422\n",
      "Epoch 18/100, Training Loss: 2.584183130238671e-05 Validation Loss: 0.0018777378560528\n",
      "Epoch 19/100, Training Loss: 2.5363246095366776e-05 Validation Loss: 0.001876242639182988\n",
      "Epoch 20/100, Training Loss: 2.6234121833113022e-05 Validation Loss: 0.0018788604088859294\n",
      "Epoch 21/100, Training Loss: 2.4915230824262835e-05 Validation Loss: 0.0018746758346263187\n",
      "Epoch 22/100, Training Loss: 2.463468445057515e-05 Validation Loss: 0.001873567657712907\n",
      "Epoch 23/100, Training Loss: 2.4090153601719067e-05 Validation Loss: 0.0018707527371027606\n",
      "Epoch 24/100, Training Loss: 2.392389615124557e-05 Validation Loss: 0.001869220074427405\n",
      "Epoch 25/100, Training Loss: 2.3877713829278946e-05 Validation Loss: 0.0018680291743210385\n",
      "Epoch 26/100, Training Loss: 2.391507223364897e-05 Validation Loss: 0.001866940047845468\n",
      "Epoch 27/100, Training Loss: 2.389321889495477e-05 Validation Loss: 0.0018672967696585368\n",
      "Epoch 28/100, Training Loss: 2.3974746000021696e-05 Validation Loss: 0.0018663401865187757\n",
      "Epoch 29/100, Training Loss: 2.4317852876265533e-05 Validation Loss: 0.0018647312109172944\n",
      "Epoch 30/100, Training Loss: 2.4388673409703188e-05 Validation Loss: 0.0018645156195985168\n",
      "Epoch 31/100, Training Loss: 2.455178037052974e-05 Validation Loss: 0.0018640891245181002\n",
      "Epoch 32/100, Training Loss: 2.485361801518593e-05 Validation Loss: 0.0018634647140324332\n",
      "Epoch 33/100, Training Loss: 2.4520633814972825e-05 Validation Loss: 0.0018641633554441616\n",
      "Epoch 34/100, Training Loss: 2.4420878617092967e-05 Validation Loss: 0.0018644218303368997\n",
      "Epoch 35/100, Training Loss: 2.5034063583007082e-05 Validation Loss: 0.0018631590416481258\n",
      "Epoch 36/100, Training Loss: 2.5167755666188896e-05 Validation Loss: 0.0018629575291591609\n",
      "Epoch 37/100, Training Loss: 2.517766370147001e-05 Validation Loss: 0.001862943287310465\n",
      "Epoch 38/100, Training Loss: 2.505768497940153e-05 Validation Loss: 0.001863122009472584\n",
      "Epoch 39/100, Training Loss: 2.43032154685352e-05 Validation Loss: 0.0018647742260667712\n",
      "Epoch 40/100, Training Loss: 2.487647725502029e-05 Validation Loss: 0.0018634225820422787\n",
      "Epoch 41/100, Training Loss: 2.483961907273624e-05 Validation Loss: 0.001863489059932502\n",
      "Epoch 42/100, Training Loss: 2.514796688046772e-05 Validation Loss: 0.0018629862246001769\n",
      "Epoch 43/100, Training Loss: 2.4730441509746015e-05 Validation Loss: 0.0018636980475827908\n",
      "Epoch 44/100, Training Loss: 2.451993350405246e-05 Validation Loss: 0.0018641650899451142\n",
      "Epoch 45/100, Training Loss: 2.4289671273436397e-05 Validation Loss: 0.0018648187062650168\n",
      "Epoch 46/100, Training Loss: 2.435690475977026e-05 Validation Loss: 0.0018646060941428432\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002642777981236577\n",
      "Validation Loss: 0.03846919909119606\n",
      "Validation Loss: 0.0003170244162902236\n",
      "Validation Loss: 0.0002789681020658463\n",
      "Validation Loss: 0.00024310298613272607\n",
      "Validation Loss: 0.0002172529202653095\n",
      "Validation Loss: 0.00020961929112672806\n",
      "Validation Loss: 0.002127310261130333\n",
      "Validation Loss: 0.0002914483193308115\n",
      "Validation Loss: 0.0007001482881605625\n",
      "Validation Loss: 0.0009652700391598046\n",
      "Validation Loss: 0.003026355756446719\n",
      "Validation Loss: 0.000294054567348212\n",
      "Validation Loss: 0.0002364236570429057\n",
      "Validation Loss: 0.0002326704270672053\n",
      "Validation Loss: 0.0029605079907923937\n",
      "Validation Loss: 0.0007639516261406243\n",
      "Validation Loss: 0.0003272709727752954\n",
      "Validation Loss: 0.0002347435220144689\n",
      "Validation Loss: 3.9456634112866595e-05\n",
      "Trial Number 19 ==========================================================\n",
      "Epoch 1/100, Training Loss: 8.720302867004648e-05 Validation Loss: 0.001886828788987881\n",
      "Epoch 2/100, Training Loss: 3.7722249544458464e-05 Validation Loss: 0.001861132488217672\n",
      "Epoch 3/100, Training Loss: 3.006986662512645e-05 Validation Loss: 0.00186035013127531\n",
      "Epoch 4/100, Training Loss: 2.744045377767179e-05 Validation Loss: 0.0018610624604087846\n",
      "Epoch 5/100, Training Loss: 2.6497502403799444e-05 Validation Loss: 0.0018616125558074744\n",
      "Epoch 6/100, Training Loss: 2.637948637129739e-05 Validation Loss: 0.0018616976890346636\n",
      "Epoch 7/100, Training Loss: 2.662676342879422e-05 Validation Loss: 0.0018615197331798318\n",
      "Epoch 8/100, Training Loss: 2.6919287847704254e-05 Validation Loss: 0.0018613338038736568\n",
      "Epoch 9/100, Training Loss: 2.7078083803644404e-05 Validation Loss: 0.0018612425949645107\n",
      "Epoch 10/100, Training Loss: 2.7072968805441633e-05 Validation Loss: 0.0018612450779922753\n",
      "Epoch 11/100, Training Loss: 2.693021815503016e-05 Validation Loss: 0.0018613265552418256\n",
      "Epoch 12/100, Training Loss: 2.6661238734959625e-05 Validation Loss: 0.0018614946615244366\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002534842351451516\n",
      "Validation Loss: 0.03864402696490288\n",
      "Validation Loss: 0.000294334051432088\n",
      "Validation Loss: 0.00025783240562304854\n",
      "Validation Loss: 0.00022372983221430331\n",
      "Validation Loss: 0.00019981656805612147\n",
      "Validation Loss: 0.00019242036796640605\n",
      "Validation Loss: 0.0021514457184821367\n",
      "Validation Loss: 0.0002840921515598893\n",
      "Validation Loss: 0.0006920647574588656\n",
      "Validation Loss: 0.0009666343685239553\n",
      "Validation Loss: 0.003041070420295\n",
      "Validation Loss: 0.0002724344376474619\n",
      "Validation Loss: 0.00021695658506359905\n",
      "Validation Loss: 0.000214915577089414\n",
      "Validation Loss: 0.0029810818377882242\n",
      "Validation Loss: 0.0007764659239910543\n",
      "Validation Loss: 0.00030427565798163414\n",
      "Validation Loss: 0.00021685357205569744\n",
      "Validation Loss: 3.3213938877452165e-05\n",
      "Trial Number 20 ==========================================================\n",
      "Epoch 1/100, Training Loss: 5.658974623656832e-05 Validation Loss: 0.0019324531764094493\n",
      "Epoch 2/100, Training Loss: 3.0946743208914995e-05 Validation Loss: 0.0018898745589352552\n",
      "Epoch 3/100, Training Loss: 2.7667350877891295e-05 Validation Loss: 0.0018826145457906402\n",
      "Epoch 4/100, Training Loss: 2.584929097793065e-05 Validation Loss: 0.0018777707159306578\n",
      "Epoch 5/100, Training Loss: 2.4982813556562178e-05 Validation Loss: 0.0018749388329343348\n",
      "Epoch 6/100, Training Loss: 2.4651137209730223e-05 Validation Loss: 0.0018736504079820938\n",
      "Epoch 7/100, Training Loss: 2.4534654585295357e-05 Validation Loss: 0.0018731488439225515\n",
      "Epoch 8/100, Training Loss: 2.444224082864821e-05 Validation Loss: 0.0018727254211210864\n",
      "Epoch 9/100, Training Loss: 2.4320090233231895e-05 Validation Loss: 0.0018721219755625323\n",
      "Epoch 10/100, Training Loss: 2.4144781491486356e-05 Validation Loss: 0.001871121705296107\n",
      "Epoch 11/100, Training Loss: 2.4383480194956064e-05 Validation Loss: 0.0018724343427636591\n",
      "Epoch 12/100, Training Loss: 2.4543205654481426e-05 Validation Loss: 0.0018731736149319466\n",
      "Epoch 13/100, Training Loss: 2.395819865341764e-05 Validation Loss: 0.0018696347197257222\n",
      "Epoch 14/100, Training Loss: 2.3878266802057624e-05 Validation Loss: 0.0018681170531655367\n",
      "Epoch 15/100, Training Loss: 2.396203672105912e-05 Validation Loss: 0.001866447563369989\n",
      "Epoch 16/100, Training Loss: 2.391472298768349e-05 Validation Loss: 0.0018669417827831339\n",
      "Epoch 17/100, Training Loss: 2.4216908059315756e-05 Validation Loss: 0.0018650800478070702\n",
      "Epoch 18/100, Training Loss: 2.3970171241671778e-05 Validation Loss: 0.0018663755740315553\n",
      "Epoch 19/100, Training Loss: 2.4469394702464342e-05 Validation Loss: 0.001864294462491901\n",
      "Epoch 20/100, Training Loss: 2.406306521152146e-05 Validation Loss: 0.0018657679873691334\n",
      "Epoch 21/100, Training Loss: 2.4610602849861607e-05 Validation Loss: 0.0018639528375500283\n",
      "Epoch 22/100, Training Loss: 2.4968516299850307e-05 Validation Loss: 0.0018632654347711533\n",
      "Epoch 23/100, Training Loss: 2.5136123440461233e-05 Validation Loss: 0.001863003663627864\n",
      "Epoch 24/100, Training Loss: 2.4621060219942592e-05 Validation Loss: 0.001863928759979166\n",
      "Epoch 25/100, Training Loss: 2.4220345949288458e-05 Validation Loss: 0.0018650627525204878\n",
      "Epoch 26/100, Training Loss: 2.483999742253218e-05 Validation Loss: 0.0018634882065944676\n",
      "Epoch 27/100, Training Loss: 2.4839446268742904e-05 Validation Loss: 0.0018634894198467525\n",
      "Epoch 28/100, Training Loss: 2.4263106752187014e-05 Validation Loss: 0.0018649093862518164\n",
      "Epoch 29/100, Training Loss: 2.435133865219541e-05 Validation Loss: 0.001864624551711015\n",
      "Epoch 30/100, Training Loss: 2.4967985154944472e-05 Validation Loss: 0.0018632660872833994\n",
      "Epoch 31/100, Training Loss: 2.4956761990324594e-05 Validation Loss: 0.001863284510725536\n",
      "Epoch 32/100, Training Loss: 2.4805358407320455e-05 Validation Loss: 0.0018635525794635903\n",
      "Epoch 33/100, Training Loss: 2.514098559913691e-05 Validation Loss: 0.0018629964457531356\n",
      "Epoch 34/100, Training Loss: 2.4596034563728608e-05 Validation Loss: 0.0018639849289370319\n",
      "Epoch 35/100, Training Loss: 2.5071949494304135e-05 Validation Loss: 0.0018631001061107065\n",
      "Epoch 36/100, Training Loss: 2.4912487788242288e-05 Validation Loss: 0.0018633595812643528\n",
      "Epoch 37/100, Training Loss: 2.458197923260741e-05 Validation Loss: 0.0018640171882715415\n",
      "Epoch 38/100, Training Loss: 2.4430335543002002e-05 Validation Loss: 0.001864397366337043\n",
      "Epoch 39/100, Training Loss: 2.4998478693305515e-05 Validation Loss: 0.0018632161212158724\n",
      "Epoch 40/100, Training Loss: 2.47141724685207e-05 Validation Loss: 0.001863730939153566\n",
      "Epoch 41/100, Training Loss: 2.478217902535107e-05 Validation Loss: 0.0018635961774383278\n",
      "Epoch 42/100, Training Loss: 2.4694481908227317e-05 Validation Loss: 0.0018637713890483788\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002622633473947644\n",
      "Validation Loss: 0.03850075230002403\n",
      "Validation Loss: 0.00031285849399864674\n",
      "Validation Loss: 0.0002750833809841424\n",
      "Validation Loss: 0.00023953702475409955\n",
      "Validation Loss: 0.00021403715072665364\n",
      "Validation Loss: 0.00020644650794565678\n",
      "Validation Loss: 0.0021316122729331255\n",
      "Validation Loss: 0.00029005552642047405\n",
      "Validation Loss: 0.0006986240041442215\n",
      "Validation Loss: 0.0009654542081989348\n",
      "Validation Loss: 0.0030289541464298964\n",
      "Validation Loss: 0.00029008224373683333\n",
      "Validation Loss: 0.0002328406844753772\n",
      "Validation Loss: 0.00022939710470382124\n",
      "Validation Loss: 0.0029641662258654833\n",
      "Validation Loss: 0.0007661523413844407\n",
      "Validation Loss: 0.0003230499569326639\n",
      "Validation Loss: 0.00023144573788158596\n",
      "Validation Loss: 3.826518513960764e-05\n",
      "Trial Number 21 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.747196595009882e-05 Validation Loss: 0.0018610478215251968\n",
      "Epoch 2/100, Training Loss: 2.7276857508695684e-05 Validation Loss: 0.0018611394925395351\n",
      "Epoch 3/100, Training Loss: 2.7554253392736427e-05 Validation Loss: 0.0018610068078957387\n",
      "Epoch 4/100, Training Loss: 2.7762434910982847e-05 Validation Loss: 0.001860916332665148\n",
      "Epoch 5/100, Training Loss: 2.778912676149048e-05 Validation Loss: 0.0018609050693267923\n",
      "Epoch 6/100, Training Loss: 2.767361729638651e-05 Validation Loss: 0.0018609528765950794\n",
      "Epoch 7/100, Training Loss: 2.743591358012054e-05 Validation Loss: 0.0018610593945557594\n",
      "Epoch 8/100, Training Loss: 2.6950545361614786e-05 Validation Loss: 0.001861316332217238\n",
      "Epoch 9/100, Training Loss: 2.6782494387589395e-05 Validation Loss: 0.0018614164416579427\n",
      "Epoch 10/100, Training Loss: 2.5816489142016508e-05 Validation Loss: 0.0018621811352890795\n",
      "Epoch 11/100, Training Loss: 2.6227931812172756e-05 Validation Loss: 0.0018618128652133383\n",
      "Epoch 12/100, Training Loss: 2.6153031285502948e-05 Validation Loss: 0.0018618747689047508\n",
      "Epoch 13/100, Training Loss: 2.5371606170665473e-05 Validation Loss: 0.0018626812404762287\n",
      "Epoch 14/100, Training Loss: 2.5693369025248103e-05 Validation Loss: 0.0018623084252119256\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.000258360814768821\n",
      "Validation Loss: 0.038563232868909836\n",
      "Validation Loss: 0.00030470118508674204\n",
      "Validation Loss: 0.00026748207164928317\n",
      "Validation Loss: 0.0002325660752831027\n",
      "Validation Loss: 0.00020775891607627273\n",
      "Validation Loss: 0.00020025315461680293\n",
      "Validation Loss: 0.0021402016282081604\n",
      "Validation Loss: 0.0002873823104891926\n",
      "Validation Loss: 0.0006956906872801483\n",
      "Validation Loss: 0.0009658999624662101\n",
      "Validation Loss: 0.0030341744422912598\n",
      "Validation Loss: 0.0002823076501954347\n",
      "Validation Loss: 0.00022583614918403327\n",
      "Validation Loss: 0.00022300489945337176\n",
      "Validation Loss: 0.002971482230350375\n",
      "Validation Loss: 0.0007705855532549322\n",
      "Validation Loss: 0.0003147835668642074\n",
      "Validation Loss: 0.00022500524937640876\n",
      "Validation Loss: 3.599020419642329e-05\n",
      "Trial Number 22 ==========================================================\n",
      "Epoch 1/100, Training Loss: 5.634064655168913e-05 Validation Loss: 0.0019318628011846322\n",
      "Epoch 2/100, Training Loss: 3.409614146221429e-05 Validation Loss: 0.0018959413400485847\n",
      "Epoch 3/100, Training Loss: 2.6874862669501454e-05 Validation Loss: 0.0018805902731272473\n",
      "Epoch 4/100, Training Loss: 2.4942295567598194e-05 Validation Loss: 0.001874776924800603\n",
      "Epoch 5/100, Training Loss: 2.4359696908504702e-05 Validation Loss: 0.0018723195407236153\n",
      "Epoch 6/100, Training Loss: 2.4072931410046294e-05 Validation Loss: 0.001870629912512307\n",
      "Epoch 7/100, Training Loss: 2.39215514739044e-05 Validation Loss: 0.00186918716757166\n",
      "Epoch 8/100, Training Loss: 2.3877631974755786e-05 Validation Loss: 0.0018679860377652294\n",
      "Epoch 9/100, Training Loss: 2.3908289222163148e-05 Validation Loss: 0.0018670362315444118\n",
      "Epoch 10/100, Training Loss: 2.401023630227428e-05 Validation Loss: 0.001866088081403293\n",
      "Epoch 11/100, Training Loss: 2.4125647541950457e-05 Validation Loss: 0.001865458498359034\n",
      "Epoch 12/100, Training Loss: 2.4214148652390577e-05 Validation Loss: 0.0018650915673714319\n",
      "Epoch 13/100, Training Loss: 2.4337592549272813e-05 Validation Loss: 0.0018646697156153229\n",
      "Epoch 14/100, Training Loss: 2.424142621748615e-05 Validation Loss: 0.0018649877346992205\n",
      "Epoch 15/100, Training Loss: 2.463797682139557e-05 Validation Loss: 0.001863893082925385\n",
      "Epoch 16/100, Training Loss: 2.4898610718082637e-05 Validation Loss: 0.0018633843853391931\n",
      "Epoch 17/100, Training Loss: 2.4148737793439068e-05 Validation Loss: 0.0018653535732793814\n",
      "Epoch 18/100, Training Loss: 2.4719010980334133e-05 Validation Loss: 0.0018637218481525658\n",
      "Epoch 19/100, Training Loss: 2.467637932568323e-05 Validation Loss: 0.0018638088198808908\n",
      "Epoch 20/100, Training Loss: 2.5054518118849955e-05 Validation Loss: 0.0018631270251886687\n",
      "Epoch 21/100, Training Loss: 2.516918357287068e-05 Validation Loss: 0.001862955436428424\n",
      "Epoch 22/100, Training Loss: 2.5177938368869945e-05 Validation Loss: 0.0018629428854093379\n",
      "Epoch 23/100, Training Loss: 2.5178585929097608e-05 Validation Loss: 0.0018629419446662308\n",
      "Epoch 24/100, Training Loss: 2.5028899472090416e-05 Validation Loss: 0.0018631673529908852\n",
      "Epoch 25/100, Training Loss: 2.5166862542391755e-05 Validation Loss: 0.0018629588308147207\n",
      "Epoch 26/100, Training Loss: 2.5169283617287874e-05 Validation Loss: 0.0018629553542639122\n",
      "Epoch 27/100, Training Loss: 2.5177992938552052e-05 Validation Loss: 0.0018629428565862518\n",
      "Epoch 28/100, Training Loss: 2.5178087526001036e-05 Validation Loss: 0.0018629426832734092\n",
      "Epoch 29/100, Training Loss: 2.460605901433155e-05 Validation Loss: 0.001863962287279914\n",
      "Epoch 30/100, Training Loss: 2.477600719430484e-05 Validation Loss: 0.0018636083593097938\n",
      "Epoch 31/100, Training Loss: 2.5080398700083606e-05 Validation Loss: 0.0018630870907404735\n",
      "Epoch 32/100, Training Loss: 2.4938801288953982e-05 Validation Loss: 0.0018633146711529332\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002614060649648309\n",
      "Validation Loss: 0.03851432353258133\n",
      "Validation Loss: 0.0003110765537712723\n",
      "Validation Loss: 0.0002734222507569939\n",
      "Validation Loss: 0.0002380128571530804\n",
      "Validation Loss: 0.00021266356634441763\n",
      "Validation Loss: 0.00020509134628809988\n",
      "Validation Loss: 0.002133469795808196\n",
      "Validation Loss: 0.0002894653589464724\n",
      "Validation Loss: 0.0006979773170314729\n",
      "Validation Loss: 0.0009655419271439314\n",
      "Validation Loss: 0.003030079649761319\n",
      "Validation Loss: 0.00028838342404924333\n",
      "Validation Loss: 0.00023130924091674387\n",
      "Validation Loss: 0.00022799875296186656\n",
      "Validation Loss: 0.0029657469131052494\n",
      "Validation Loss: 0.0007671064813621342\n",
      "Validation Loss: 0.00032124429708346725\n",
      "Validation Loss: 0.0002300368796568364\n",
      "Validation Loss: 3.7761601561214775e-05\n",
      "Trial Number 23 ==========================================================\n",
      "Epoch 1/100, Training Loss: 9.933505498338491e-05 Validation Loss: 0.0019919245579581414\n",
      "Epoch 2/100, Training Loss: 4.997639189241454e-05 Validation Loss: 0.0019226212430438914\n",
      "Epoch 3/100, Training Loss: 3.9542544982396066e-05 Validation Loss: 0.001905764797867135\n",
      "Epoch 4/100, Training Loss: 3.554279828676954e-05 Validation Loss: 0.0018987206298341857\n",
      "Epoch 5/100, Training Loss: 3.3269392588408664e-05 Validation Loss: 0.0018944717620625205\n",
      "Epoch 6/100, Training Loss: 3.1674244382884353e-05 Validation Loss: 0.0018913446034560337\n",
      "Epoch 7/100, Training Loss: 3.0438097383012064e-05 Validation Loss: 0.0018888134144981255\n",
      "Epoch 8/100, Training Loss: 2.9557615562225692e-05 Validation Loss: 0.001886936756649972\n",
      "Epoch 9/100, Training Loss: 2.8999258574913256e-05 Validation Loss: 0.0018857062829454757\n",
      "Epoch 10/100, Training Loss: 2.8598498829524033e-05 Validation Loss: 0.0018847994511973988\n",
      "Epoch 11/100, Training Loss: 2.8184107577544637e-05 Validation Loss: 0.0018838395700602325\n",
      "Epoch 12/100, Training Loss: 2.7795680580311455e-05 Validation Loss: 0.001882914838239866\n",
      "Epoch 13/100, Training Loss: 2.7842481358675286e-05 Validation Loss: 0.0018830232311984462\n",
      "Epoch 14/100, Training Loss: 2.8015410862280987e-05 Validation Loss: 0.0018834268957743819\n",
      "Epoch 15/100, Training Loss: 2.6716907086665742e-05 Validation Loss: 0.0018801904103288472\n",
      "Epoch 16/100, Training Loss: 2.604253859317396e-05 Validation Loss: 0.0018783270905821485\n",
      "Epoch 17/100, Training Loss: 2.6994857762474567e-05 Validation Loss: 0.0018809042212309761\n",
      "Epoch 18/100, Training Loss: 2.563559428381268e-05 Validation Loss: 0.0018771042585515668\n",
      "Epoch 19/100, Training Loss: 2.481019873812329e-05 Validation Loss: 0.0018742794052688733\n",
      "Epoch 20/100, Training Loss: 2.4398943423875608e-05 Validation Loss: 0.0018725130044593649\n",
      "Epoch 21/100, Training Loss: 2.4133176339091733e-05 Validation Loss: 0.0018710427727756896\n",
      "Epoch 22/100, Training Loss: 2.393830072833225e-05 Validation Loss: 0.0018694080839867905\n",
      "Epoch 23/100, Training Loss: 2.38790235016495e-05 Validation Loss: 0.0018681872941529636\n",
      "Epoch 24/100, Training Loss: 2.3898252038634382e-05 Validation Loss: 0.0018672010630378171\n",
      "Epoch 25/100, Training Loss: 2.3971022528712638e-05 Validation Loss: 0.0018663734922186538\n",
      "Epoch 26/100, Training Loss: 2.4069828214123845e-05 Validation Loss: 0.0018657339678923544\n",
      "Epoch 27/100, Training Loss: 2.4099124857457355e-05 Validation Loss: 0.001865582581869519\n",
      "Epoch 28/100, Training Loss: 2.3983931896509603e-05 Validation Loss: 0.0018662696642879025\n",
      "Epoch 29/100, Training Loss: 2.4268103516078554e-05 Validation Loss: 0.0018648939672108872\n",
      "Epoch 30/100, Training Loss: 2.4673514417372644e-05 Validation Loss: 0.0018638166288160475\n",
      "Epoch 31/100, Training Loss: 2.4510107323294505e-05 Validation Loss: 0.0018641893648480576\n",
      "Epoch 32/100, Training Loss: 2.4992488761199638e-05 Validation Loss: 0.001863226047649656\n",
      "Epoch 33/100, Training Loss: 2.513235631340649e-05 Validation Loss: 0.0018630092110737385\n",
      "Epoch 34/100, Training Loss: 2.465610668878071e-05 Validation Loss: 0.0018638515967096124\n",
      "Epoch 36/100, Training Loss: 2.4744742404436693e-05 Validation Loss: 0.0018636696491692922\n",
      "Epoch 37/100, Training Loss: 2.424676677037496e-05 Validation Loss: 0.001864965550717773\n",
      "Epoch 38/100, Training Loss: 2.4515258701285347e-05 Validation Loss: 0.0018641763292642317\n",
      "Epoch 39/100, Training Loss: 2.505078555259388e-05 Validation Loss: 0.001863132921630958\n",
      "Epoch 40/100, Training Loss: 2.5164983526337892e-05 Validation Loss: 0.001862961481852379\n",
      "Epoch 41/100, Training Loss: 2.4810504328343086e-05 Validation Loss: 0.0018635430092007993\n",
      "Epoch 42/100, Training Loss: 2.4572891561547294e-05 Validation Loss: 0.0018640381934386773\n",
      "Epoch 43/100, Training Loss: 2.5072071366594173e-05 Validation Loss: 0.0018630998871301172\n",
      "Epoch 44/100, Training Loss: 2.4311406377819367e-05 Validation Loss: 0.0018647486180649014\n",
      "Epoch 45/100, Training Loss: 2.45254432229558e-05 Validation Loss: 0.0018641504473806561\n",
      "Epoch 46/100, Training Loss: 2.4480650608893484e-05 Validation Loss: 0.0018642634154738616\n",
      "Epoch 47/100, Training Loss: 2.481469709891826e-05 Validation Loss: 0.0018635350158478015\n",
      "Epoch 48/100, Training Loss: 2.4814647986204363e-05 Validation Loss: 0.001863535228152914\n",
      "Epoch 49/100, Training Loss: 2.5145356630673632e-05 Validation Loss: 0.001862990094816945\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002643232583068311\n",
      "Validation Loss: 0.03846849128603935\n",
      "Validation Loss: 0.0003171181306242943\n",
      "Validation Loss: 0.00027905552997253835\n",
      "Validation Loss: 0.00024318326904904097\n",
      "Validation Loss: 0.00021732533059548587\n",
      "Validation Loss: 0.0002096907701343298\n",
      "Validation Loss: 0.002127214102074504\n",
      "Validation Loss: 0.00029147983877919614\n",
      "Validation Loss: 0.0007001828053034842\n",
      "Validation Loss: 0.0009652661392465234\n",
      "Validation Loss: 0.0030262977816164494\n",
      "Validation Loss: 0.0002941438870038837\n",
      "Validation Loss: 0.00023650431830901653\n",
      "Validation Loss: 0.000232744132517837\n",
      "Validation Loss: 0.0029604267328977585\n",
      "Validation Loss: 0.0007639026152901351\n",
      "Validation Loss: 0.0003273659385740757\n",
      "Validation Loss: 0.00023481778043787926\n",
      "Validation Loss: 3.9483667933382094e-05\n",
      "Trial Number 24 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00016219046665355563 Validation Loss: 0.0019400425172697389\n",
      "Epoch 2/100, Training Loss: 7.272884249687195e-05 Validation Loss: 0.0018777245796839073\n",
      "Epoch 3/100, Training Loss: 5.3778294386575e-05 Validation Loss: 0.001867364994340076\n",
      "Epoch 4/100, Training Loss: 4.57193746115081e-05 Validation Loss: 0.0018637819018635184\n",
      "Epoch 5/100, Training Loss: 4.065464963787235e-05 Validation Loss: 0.0018619611573711013\n",
      "Epoch 6/100, Training Loss: 3.7100075132912025e-05 Validation Loss: 0.0018609806731550895\n",
      "Epoch 7/100, Training Loss: 3.463013854343444e-05 Validation Loss: 0.0018605055460418872\n",
      "Epoch 8/100, Training Loss: 3.307514634798281e-05 Validation Loss: 0.0018603243481513565\n",
      "Epoch 9/100, Training Loss: 3.220298094674945e-05 Validation Loss: 0.0018602731365771387\n",
      "Epoch 10/100, Training Loss: 3.172272045048885e-05 Validation Loss: 0.0018602630692720811\n",
      "Epoch 11/100, Training Loss: 3.140153057756834e-05 Validation Loss: 0.0018602643089767222\n",
      "Epoch 12/100, Training Loss: 3.1076018785825e-05 Validation Loss: 0.0018602726394100972\n",
      "Epoch 13/100, Training Loss: 3.082829061895609e-05 Validation Loss: 0.0018602839157874681\n",
      "Epoch 14/100, Training Loss: 3.053696491406299e-05 Validation Loss: 0.0018603030800203506\n",
      "Epoch 15/100, Training Loss: 3.0276958568720147e-05 Validation Loss: 0.0018603259914415906\n",
      "Epoch 16/100, Training Loss: 2.998261152242776e-05 Validation Loss: 0.0018603588234945645\n",
      "Epoch 17/100, Training Loss: 2.9857694244128652e-05 Validation Loss: 0.0018603751846511289\n",
      "Epoch 18/100, Training Loss: 2.9415632525342517e-05 Validation Loss: 0.001860445069467563\n",
      "Epoch 19/100, Training Loss: 2.9185786843299866e-05 Validation Loss: 0.001860489474613487\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00024311667948495597\n",
      "Validation Loss: 0.038827426731586456\n",
      "Validation Loss: 0.00027154688723385334\n",
      "Validation Loss: 0.00023666823108214885\n",
      "Validation Loss: 0.00020440561638679355\n",
      "Validation Loss: 0.00018251431174576283\n",
      "Validation Loss: 0.0001753659307723865\n",
      "Validation Loss: 0.002177541609853506\n",
      "Validation Loss: 0.00027731290902011096\n",
      "Validation Loss: 0.0006845262250863016\n",
      "Validation Loss: 0.0009689590660855174\n",
      "Validation Loss: 0.0030573310796171427\n",
      "Validation Loss: 0.0002507645112928003\n",
      "Validation Loss: 0.00019753430387936532\n",
      "Validation Loss: 0.0001972807658603415\n",
      "Validation Loss: 0.0030034601222723722\n",
      "Validation Loss: 0.0007904304075054824\n",
      "Validation Loss: 0.0002811700978782028\n",
      "Validation Loss: 0.00019907770911231637\n",
      "Validation Loss: 2.7597177904681303e-05\n",
      "Trial Number 25 ==========================================================\n",
      "Epoch 1/100, Training Loss: 3.230803122278303e-05 Validation Loss: 0.0018602827004763913\n",
      "Epoch 2/100, Training Loss: 2.6740035536931828e-05 Validation Loss: 0.0018614563132817923\n",
      "Epoch 3/100, Training Loss: 2.5957166144507937e-05 Validation Loss: 0.0018620586241964781\n",
      "Epoch 4/100, Training Loss: 2.5801997253438458e-05 Validation Loss: 0.0018622059190879392\n",
      "Epoch 5/100, Training Loss: 2.5846655262284912e-05 Validation Loss: 0.0018621596857972267\n",
      "Epoch 6/100, Training Loss: 2.6032337700598873e-05 Validation Loss: 0.0018619844789910234\n",
      "Epoch 7/100, Training Loss: 2.6282363251084462e-05 Validation Loss: 0.0018617731591038774\n",
      "Epoch 8/100, Training Loss: 2.652412513270974e-05 Validation Loss: 0.0018615913163123918\n",
      "Epoch 9/100, Training Loss: 2.6714080377132632e-05 Validation Loss: 0.0018614618209243572\n",
      "Epoch 10/100, Training Loss: 2.678626697161235e-05 Validation Loss: 0.0018614151204750541\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.0002515380911063403\n",
      "Validation Loss: 0.03867717832326889\n",
      "Validation Loss: 0.0002901382395066321\n",
      "Validation Loss: 0.0002539305714890361\n",
      "Validation Loss: 0.0002201612660428509\n",
      "Validation Loss: 0.00019661425903905183\n",
      "Validation Loss: 0.00018926296615973115\n",
      "Validation Loss: 0.0021561041940003633\n",
      "Validation Loss: 0.0002827959833666682\n",
      "Validation Loss: 0.000690631044562906\n",
      "Validation Loss: 0.0009669872815720737\n",
      "Validation Loss: 0.0030439470428973436\n",
      "Validation Loss: 0.0002684409555513412\n",
      "Validation Loss: 0.0002133702510036528\n",
      "Validation Loss: 0.00021165302314329892\n",
      "Validation Loss: 0.0029850671999156475\n",
      "Validation Loss: 0.0007789272349327803\n",
      "Validation Loss: 0.00030002216226421297\n",
      "Validation Loss: 0.00021356546494644135\n",
      "Validation Loss: 3.212832234567031e-05\n",
      "Trial Number 26 ==========================================================\n",
      "Epoch 1/100, Training Loss: 5.150339347892441e-05 Validation Loss: 0.0019249474587460973\n",
      "Epoch 2/100, Training Loss: 3.526536238496192e-05 Validation Loss: 0.0018982442809839803\n",
      "Epoch 3/100, Training Loss: 3.22394298564177e-05 Validation Loss: 0.0018924964463467025\n",
      "Epoch 4/100, Training Loss: 3.09652132273186e-05 Validation Loss: 0.0018899212235740574\n",
      "Epoch 5/100, Training Loss: 3.0122489988571033e-05 Validation Loss: 0.001888158157196331\n",
      "Epoch 6/100, Training Loss: 2.9620434361277148e-05 Validation Loss: 0.001887078546955862\n",
      "Epoch 7/100, Training Loss: 2.929452784883324e-05 Validation Loss: 0.0018863637717904888\n",
      "Epoch 8/100, Training Loss: 2.90063089778414e-05 Validation Loss: 0.0018857220558490552\n",
      "Epoch 9/100, Training Loss: 2.8664320780080743e-05 Validation Loss: 0.0018849489667766308\n",
      "Epoch 10/100, Training Loss: 2.8095691959606484e-05 Validation Loss: 0.0018836835458912633\n",
      "Epoch 11/100, Training Loss: 2.7996064090984873e-05 Validation Loss: 0.0018833917022871387\n",
      "Epoch 12/100, Training Loss: 2.820810732373502e-05 Validation Loss: 0.0018838840508823545\n",
      "Epoch 13/100, Training Loss: 2.841185050783679e-05 Validation Loss: 0.0018843501995418456\n",
      "Epoch 14/100, Training Loss: 2.7460919227451086e-05 Validation Loss: 0.001882086475596874\n",
      "Epoch 15/100, Training Loss: 2.6246687411912717e-05 Validation Loss: 0.0018789061198673847\n",
      "Epoch 16/100, Training Loss: 2.550759381847456e-05 Validation Loss: 0.0018767069400455862\n",
      "Epoch 17/100, Training Loss: 2.4880982891772874e-05 Validation Loss: 0.0018745517569178833\n",
      "Epoch 18/100, Training Loss: 2.4516215489711612e-05 Validation Loss: 0.0018730593758155088\n",
      "Epoch 19/100, Training Loss: 2.420338205411099e-05 Validation Loss: 0.0018714764023185703\n",
      "Epoch 20/100, Training Loss: 2.4238746846094728e-05 Validation Loss: 0.0018716749757261785\n",
      "Epoch 21/100, Training Loss: 2.3900762244011275e-05 Validation Loss: 0.0018688450827081622\n",
      "Epoch 22/100, Training Loss: 2.389861037954688e-05 Validation Loss: 0.0018671931575889931\n",
      "Epoch 23/100, Training Loss: 2.3971329937921837e-05 Validation Loss: 0.0018663679233738217\n",
      "Epoch 24/100, Training Loss: 2.3981101548997685e-05 Validation Loss: 0.001866290027860628\n",
      "Epoch 25/100, Training Loss: 2.3995029550860636e-05 Validation Loss: 0.0018661860395936616\n",
      "Epoch 26/100, Training Loss: 2.4437384126940742e-05 Validation Loss: 0.0018643774884520349\n",
      "Epoch 27/100, Training Loss: 2.5033776182681322e-05 Validation Loss: 0.0018631596342682448\n",
      "Epoch 28/100, Training Loss: 2.512914579710923e-05 Validation Loss: 0.0018630138676242237\n",
      "Epoch 29/100, Training Loss: 2.4626981030451134e-05 Validation Loss: 0.0018639157605177792\n",
      "Epoch 30/100, Training Loss: 2.5086841560550965e-05 Validation Loss: 0.0018630773779843301\n",
      "Epoch 31/100, Training Loss: 2.434392081340775e-05 Validation Loss: 0.00186464743137695\n",
      "Epoch 32/100, Training Loss: 2.472097912686877e-05 Validation Loss: 0.0018637169557768304\n",
      "Epoch 33/100, Training Loss: 2.5115410608123057e-05 Validation Loss: 0.001863034339626654\n",
      "Epoch 34/100, Training Loss: 2.4927130652940832e-05 Validation Loss: 0.0018633344869622471\n",
      "Epoch 35/100, Training Loss: 2.4719380235183053e-05 Validation Loss: 0.0018637204151710403\n",
      "Epoch 36/100, Training Loss: 2.4591661713202484e-05 Validation Loss: 0.0018639948614848037\n",
      "Epoch 37/100, Training Loss: 2.4459925043629482e-05 Validation Loss: 0.001864317899154617\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00026956337387673557\n",
      "Validation Loss: 0.03838850557804108\n",
      "Validation Loss: 0.00032782056950964034\n",
      "Validation Loss: 0.000289044197415933\n",
      "Validation Loss: 0.0002523626899346709\n",
      "Validation Loss: 0.0002256154257338494\n",
      "Validation Loss: 0.0002178718859795481\n",
      "Validation Loss: 0.002116416348144412\n",
      "Validation Loss: 0.0002951416827272624\n",
      "Validation Loss: 0.000704178586602211\n",
      "Validation Loss: 0.0009649239364080131\n",
      "Validation Loss: 0.0030198260210454464\n",
      "Validation Loss: 0.0003043549950234592\n",
      "Validation Loss: 0.0002457268419675529\n",
      "Validation Loss: 0.00024118054716382176\n",
      "Validation Loss: 0.0029512643814086914\n",
      "Validation Loss: 0.0007584409322589636\n",
      "Validation Loss: 0.0003382085124030709\n",
      "Validation Loss: 0.00024331618624273688\n",
      "Validation Loss: 4.2634233977878466e-05\n",
      "Trial Number 27 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00022052264830563217 Validation Loss: 0.0019854735975485148\n",
      "Epoch 2/100, Training Loss: 0.00010737957200035453 Validation Loss: 0.0019001104469792388\n",
      "Epoch 3/100, Training Loss: 7.236085366457701e-05 Validation Loss: 0.0018774823127237087\n",
      "Epoch 4/100, Training Loss: 5.618455907097086e-05 Validation Loss: 0.0018685474081063746\n",
      "Epoch 5/100, Training Loss: 4.854895087191835e-05 Validation Loss: 0.0018649540501192515\n",
      "Epoch 6/100, Training Loss: 4.400235411594622e-05 Validation Loss: 0.0018631170553954804\n",
      "Epoch 7/100, Training Loss: 4.079244172316976e-05 Validation Loss: 0.001862004152369373\n",
      "Epoch 8/100, Training Loss: 3.768604074139148e-05 Validation Loss: 0.00186112112175256\n",
      "Epoch 9/100, Training Loss: 3.561554694897495e-05 Validation Loss: 0.001860670919309491\n",
      "Epoch 10/100, Training Loss: 3.4179181966464967e-05 Validation Loss: 0.00186044255530837\n",
      "Epoch 11/100, Training Loss: 3.317751907161437e-05 Validation Loss: 0.0018603330068684522\n",
      "Epoch 12/100, Training Loss: 3.1657666113460436e-05 Validation Loss: 0.0018602628360671117\n",
      "Epoch 13/100, Training Loss: 3.200536229996942e-05 Validation Loss: 0.001860267368466209\n",
      "Epoch 14/100, Training Loss: 3.155750528094359e-05 Validation Loss: 0.001860262899827272\n",
      "Epoch 15/100, Training Loss: 3.115497383987531e-05 Validation Loss: 0.0018602699087034325\n",
      "Epoch 16/100, Training Loss: 3.073358675464988e-05 Validation Loss: 0.0018602894330377285\n",
      "Epoch 17/100, Training Loss: 3.0322591555886902e-05 Validation Loss: 0.0018603215849407352\n",
      "Epoch 18/100, Training Loss: 2.9986073059262708e-05 Validation Loss: 0.0018603583685639499\n",
      "Epoch 19/100, Training Loss: 2.965288695122581e-05 Validation Loss: 0.0018604051754465536\n",
      "Epoch 20/100, Training Loss: 2.9096414436935447e-05 Validation Loss: 0.0018605084051422991\n",
      "Epoch 21/100, Training Loss: 2.809574652928859e-05 Validation Loss: 0.0018607888615686178\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.000245595263550058\n",
      "Validation Loss: 0.03878198191523552\n",
      "Validation Loss: 0.00027709713322110474\n",
      "Validation Loss: 0.00024181706248782575\n",
      "Validation Loss: 0.00020909938029944897\n",
      "Validation Loss: 0.00018670798453968018\n",
      "Validation Loss: 0.0001794983254512772\n",
      "Validation Loss: 0.0021710016299039125\n",
      "Validation Loss: 0.00027890398632735014\n",
      "Validation Loss: 0.000686305167619139\n",
      "Validation Loss: 0.0009682986419647932\n",
      "Validation Loss: 0.0030532239470630884\n",
      "Validation Loss: 0.0002560384455136955\n",
      "Validation Loss: 0.00020225232583470643\n",
      "Validation Loss: 0.00020155671518296003\n",
      "Validation Loss: 0.002997839590534568\n",
      "Validation Loss: 0.0007868910324759781\n",
      "Validation Loss: 0.00028679906972683966\n",
      "Validation Loss: 0.00020338852482382208\n",
      "Validation Loss: 2.890075120376423e-05\n",
      "Trial Number 28 ==========================================================\n",
      "Epoch 1/100, Training Loss: 0.00021156355796847492 Validation Loss: 0.001978265010186053\n",
      "Epoch 2/100, Training Loss: 0.00011579466809052974 Validation Loss: 0.001906209017305075\n",
      "Epoch 3/100, Training Loss: 6.960371683817357e-05 Validation Loss: 0.0018759790679381508\n",
      "Epoch 4/100, Training Loss: 5.438898369902745e-05 Validation Loss: 0.001867730473255537\n",
      "Epoch 5/100, Training Loss: 4.652187635656446e-05 Validation Loss: 0.0018641492361871632\n",
      "Epoch 6/100, Training Loss: 4.070748764206655e-05 Validation Loss: 0.0018619991370276143\n",
      "Epoch 7/100, Training Loss: 3.582105637178756e-05 Validation Loss: 0.0018607159173250846\n",
      "Epoch 8/100, Training Loss: 3.241773811168969e-05 Validation Loss: 0.0018602826361547426\n",
      "Epoch 9/100, Training Loss: 3.0314615287352353e-05 Validation Loss: 0.0018603220073050057\n",
      "Epoch 10/100, Training Loss: 2.921111081377603e-05 Validation Loss: 0.0018604841531980043\n",
      "Epoch 11/100, Training Loss: 2.8864598789368756e-05 Validation Loss: 0.0018605614538451244\n",
      "Epoch 12/100, Training Loss: 2.8315238523646258e-05 Validation Loss: 0.0018607154491059044\n",
      "Epoch 13/100, Training Loss: 2.7768575819209218e-05 Validation Loss: 0.0018609129493838497\n",
      "Epoch 14/100, Training Loss: 2.7707763365469873e-05 Validation Loss: 0.0018609383599288633\n",
      "Epoch 15/100, Training Loss: 2.731931090238504e-05 Validation Loss: 0.0018611157509884602\n",
      "Epoch 16/100, Training Loss: 2.7066727852798067e-05 Validation Loss: 0.0018612483845991737\n",
      "Epoch 17/100, Training Loss: 2.682707054191269e-05 Validation Loss: 0.0018613886637518348\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00025297008687630296\n",
      "Validation Loss: 0.038652729243040085\n",
      "Validation Loss: 0.000293228862574324\n",
      "Validation Loss: 0.00025680448743514717\n",
      "Validation Loss: 0.00022278947290033102\n",
      "Validation Loss: 0.00019897242600563914\n",
      "Validation Loss: 0.00019158799841534346\n",
      "Validation Loss: 0.0021526662167161703\n",
      "Validation Loss: 0.0002837486972566694\n",
      "Validation Loss: 0.0006916852435097098\n",
      "Validation Loss: 0.0009667242411524057\n",
      "Validation Loss: 0.0030418229289352894\n",
      "Validation Loss: 0.0002713824505917728\n",
      "Validation Loss: 0.000216011525481008\n",
      "Validation Loss: 0.00021405557345133275\n",
      "Validation Loss: 0.002982125850394368\n",
      "Validation Loss: 0.0007771095843054354\n",
      "Validation Loss: 0.0003031553060282022\n",
      "Validation Loss: 0.00021598687453661114\n",
      "Validation Loss: 3.292582914582454e-05\n",
      "Trial Number 29 ==========================================================\n",
      "Epoch 1/100, Training Loss: 4.458559487829916e-05 Validation Loss: 0.001863310567481645\n",
      "Epoch 2/100, Training Loss: 3.269629451096989e-05 Validation Loss: 0.0018602944126313737\n",
      "Epoch 3/100, Training Loss: 3.0049524866626598e-05 Validation Loss: 0.0018603546367853376\n",
      "Epoch 4/100, Training Loss: 2.9054841434117407e-05 Validation Loss: 0.0018605223527709213\n",
      "Epoch 5/100, Training Loss: 2.8808553906856105e-05 Validation Loss: 0.0018605796139490654\n",
      "Epoch 6/100, Training Loss: 2.879502972064074e-05 Validation Loss: 0.0018605820539917513\n",
      "Epoch 7/100, Training Loss: 2.8808912247768603e-05 Validation Loss: 0.0018605777610363869\n",
      "Epoch 8/100, Training Loss: 2.880853571696207e-05 Validation Loss: 0.0018605771421510745\n",
      "Epoch 9/100, Training Loss: 2.8810052754124627e-05 Validation Loss: 0.0018605761982261983\n",
      "Epoch 10/100, Training Loss: 2.880614556488581e-05 Validation Loss: 0.0018605767710694378\n",
      "Epoch 11/100, Training Loss: 2.8771992219844833e-05 Validation Loss: 0.0018605850154702677\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00024473731173202395\n",
      "Validation Loss: 0.038797587156295776\n",
      "Validation Loss: 0.00027518384740687907\n",
      "Validation Loss: 0.00024004169972613454\n",
      "Validation Loss: 0.00020748037786688656\n",
      "Validation Loss: 0.0001852607965702191\n",
      "Validation Loss: 0.00017807217955123633\n",
      "Validation Loss: 0.0021732423920184374\n",
      "Validation Loss: 0.0002783510135486722\n",
      "Validation Loss: 0.000685687642544508\n",
      "Validation Loss: 0.0009685191325843334\n",
      "Validation Loss: 0.003054628614336252\n",
      "Validation Loss: 0.00025422009639441967\n",
      "Validation Loss: 0.00020062497060280293\n",
      "Validation Loss: 0.000200081238290295\n",
      "Validation Loss: 0.0029997641686350107\n",
      "Validation Loss: 0.0007881007040850818\n",
      "Validation Loss: 0.0002848587464541197\n",
      "Validation Loss: 0.00020190108625683933\n",
      "Validation Loss: 2.844652954081539e-05\n",
      "Trial Number 30 ==========================================================\n",
      "Epoch 1/100, Training Loss: 2.7940241125179455e-05 Validation Loss: 0.0018833343328664525\n",
      "Epoch 2/100, Training Loss: 2.5257097149733454e-05 Validation Loss: 0.0018759052575983122\n",
      "Epoch 3/100, Training Loss: 2.4047087208600715e-05 Validation Loss: 0.0018704456052221837\n",
      "Epoch 4/100, Training Loss: 2.3878748834249564e-05 Validation Loss: 0.0018681756033345513\n",
      "Epoch 5/100, Training Loss: 2.3884447728050873e-05 Validation Loss: 0.0018675350434983315\n",
      "Epoch 6/100, Training Loss: 2.390174449828919e-05 Validation Loss: 0.0018671467268414244\n",
      "Epoch 7/100, Training Loss: 2.39226792473346e-05 Validation Loss: 0.0018668497982709433\n",
      "Epoch 8/100, Training Loss: 2.3947490262798965e-05 Validation Loss: 0.0018665839586429905\n",
      "Epoch 9/100, Training Loss: 2.397958633082453e-05 Validation Loss: 0.0018663093530536018\n",
      "Epoch 10/100, Training Loss: 2.3994352886802517e-05 Validation Loss: 0.0018661982347536932\n",
      "Epoch 11/100, Training Loss: 2.4040678908932023e-05 Validation Loss: 0.0018658984218781968\n",
      "Epoch 12/100, Training Loss: 2.4133971237461083e-05 Validation Loss: 0.0018654221953077507\n",
      "Epoch 13/100, Training Loss: 2.4214161385316402e-05 Validation Loss: 0.0018650918326436443\n",
      "Epoch 14/100, Training Loss: 2.4305707484018058e-05 Validation Loss: 0.0018647708477140957\n",
      "Epoch 15/100, Training Loss: 2.4451343051623553e-05 Validation Loss: 0.0018643432078830147\n",
      "Epoch 16/100, Training Loss: 2.4513066819054075e-05 Validation Loss: 0.0018641834740206551\n",
      "Epoch 17/100, Training Loss: 2.477307862136513e-05 Validation Loss: 0.001863615556035115\n",
      "Epoch 18/100, Training Loss: 2.4694012608961202e-05 Validation Loss: 0.0018637725312411505\n",
      "Epoch 19/100, Training Loss: 2.47050920734182e-05 Validation Loss: 0.0018637491030007649\n",
      "Epoch 20/100, Training Loss: 2.4897464754758403e-05 Validation Loss: 0.0018633862804882987\n",
      "Epoch 21/100, Training Loss: 2.5138750061159953e-05 Validation Loss: 0.0018629997315225645\n",
      "Epoch 22/100, Training Loss: 2.517580651328899e-05 Validation Loss: 0.0018629459729108716\n",
      "Epoch 23/100, Training Loss: 2.439789750496857e-05 Validation Loss: 0.001864486923096911\n",
      "Epoch 24/100, Training Loss: 2.5046429072972387e-05 Validation Loss: 0.0018631397265619484\n",
      "Epoch 25/100, Training Loss: 2.427758226986043e-05 Validation Loss: 0.0018648599973946633\n",
      "Epoch 26/100, Training Loss: 2.4700682843104005e-05 Validation Loss: 0.0018637585266526202\n",
      "Epoch 27/100, Training Loss: 2.511937782401219e-05 Validation Loss: 0.0018630284269635805\n",
      "Epoch 28/100, Training Loss: 2.4440210836473852e-05 Validation Loss: 0.0018643692658244886\n",
      "Epoch 29/100, Training Loss: 2.4564569685026072e-05 Validation Loss: 0.0018640574685343824\n",
      "Epoch 30/100, Training Loss: 2.5078388716792688e-05 Validation Loss: 0.0018630902199169455\n",
      "Epoch 31/100, Training Loss: 2.510307967895642e-05 Validation Loss: 0.0018630526756013305\n",
      "Early stopping triggered\n",
      "Validation Loss: 0.00026821449864655733\n",
      "Validation Loss: 0.03840882331132889\n",
      "Validation Loss: 0.0003250831214245409\n",
      "Validation Loss: 0.0002864882117137313\n",
      "Validation Loss: 0.0002500123518984765\n",
      "Validation Loss: 0.00022349120990838856\n",
      "Validation Loss: 0.0002157753478968516\n",
      "Validation Loss: 0.002119144657626748\n",
      "Validation Loss: 0.0002941940037999302\n",
      "Validation Loss: 0.0007031458662822843\n",
      "Validation Loss: 0.0009649942512623966\n",
      "Validation Loss: 0.003021454671397805\n",
      "Validation Loss: 0.0003017424896825105\n",
      "Validation Loss: 0.00024336561909876764\n",
      "Validation Loss: 0.00023901910753920674\n",
      "Validation Loss: 0.0029535763897001743\n",
      "Validation Loss: 0.0007598124211654067\n",
      "Validation Loss: 0.00033543541212566197\n",
      "Validation Loss: 0.00024113898689392954\n",
      "Validation Loss: 4.181648182566278e-05\n",
      "Run is Done! Run Time: 70.0 minutese \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create variables\n",
    "test_best_val = float('inf')\n",
    "EvalDF = {}\n",
    "SupplyEvalDF = {}\n",
    "EvalDF_all = np.zeros([len(station_index_list.drop_duplicates()), 10])\n",
    "SupplyEvalDF_all = np.zeros([len(station_index_list.drop_duplicates()), 17])\n",
    "\n",
    "# Start running the model several times. \n",
    "for try_number in range(1, tries+1):\n",
    "\n",
    "    # Create the variables. \n",
    "    EvalDF[try_number] = np.zeros([len(station_index_list.drop_duplicates()), 10])\n",
    "    SupplyEvalDF[try_number] = np.zeros([len(station_index_list.drop_duplicates()), 17])\n",
    "    SitesDict = {}\n",
    "    val_loss_all = 0\n",
    "    print(f'Trial Number {try_number} ==========================================================')\n",
    "    \n",
    "    # Set the optimizer, create the model, and train it. \n",
    "    mlp_model = CustomMLP(layer_sizes, device)\n",
    "    mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "    model_parameters = mlp_model.train_model(train_loader, epochs, mlp_optimizer, early_stopping_patience, path_model_save, validation_loader)\n",
    "    \n",
    "    # Evaluate it for different stations. \n",
    "    for station_index, station_number in enumerate(station_index_list.drop_duplicates()):\n",
    "        index = station_index_list == station_number # Finind the rows that have this station number.\n",
    "        temp_x_scaled_test = torch.Tensor(x_test_1_scaled)\n",
    "        temp_y_scaled_test = torch.Tensor(y_scaled_test_1)\n",
    "        index_np = torch.tensor(index.to_numpy())\n",
    "        test_dataset = TensorDataset(temp_x_scaled_test[index_np], temp_y_scaled_test[index_np])\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "        \n",
    "        # Evaluation\n",
    "        yhat_test, val_loss = mlp_model.evaluate_model(test_loader)\n",
    "        \n",
    "        # Invert scaling for actual and concat it with the rest of the dataset. \n",
    "        inv_yhat_test = scaler.inverse_transform(yhat_test.numpy())\n",
    "        inv_yhat_test[inv_yhat_test<0] = 0 # THIS IS NOT CORRECT !!!!!!!!!!!!!!!\n",
    "        nwm_test = pd.DataFrame(inv_yhat_test, columns=['MLP_flow'])\n",
    "        Dfs = [nwm_test.reset_index(drop=True), x_test_temp[index].reset_index(drop=True)]\n",
    "        Eval_DF_mine = pd.concat(Dfs, axis=1)\n",
    "    \n",
    "        # Get reach id for model eval.\n",
    "        nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "        nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "        SitesDict[nhdreach] = Eval_DF_mine\n",
    "        \n",
    "        # Calculate the results. \n",
    "        prediction_columns = ['NWM_flow', f\"{model_name}_flow\"]\n",
    "        observation_column = 'flow_cfs'\n",
    "        result = evtab(Eval_DF_mine, prediction_columns, nhdreach, observation_column, model_name)\n",
    "        EvalDF[try_number][station_index, :] = result[0]\n",
    "        SupplyEvalDF[try_number][station_index, :] = result[1]\n",
    "\n",
    "    # Finding the best model. \n",
    "    val_loss_all += val_loss\n",
    "    val_loss_all = val_loss_all / len(station_index_list.drop_duplicates())\n",
    "    if val_loss_all < test_best_val:\n",
    "        test_best_val = val_loss_all\n",
    "        best_model_parameters = model_parameters\n",
    "        best_try = try_number\n",
    "        best_output = SitesDict\n",
    "    EvalDF_all = EvalDF[try_number] + EvalDF_all\n",
    "    SupplyEvalDF_all = SupplyEvalDF[try_number] + SupplyEvalDF_all\n",
    "        \n",
    "# Calculate the average results for all of the trials. \n",
    "EvalDF_all = EvalDF_all / tries\n",
    "SupplyEvalDF_all = SupplyEvalDF_all / tries\n",
    "\n",
    "# Sort the outputs of the best model based on date. \n",
    "keys = list(best_output.keys())\n",
    "for key_number in keys:\n",
    "    best_output[key_number] = best_output[key_number].sort_values(by='datetime')\n",
    "    \n",
    "print('Run is Done!' + \" Run Time:\" + \" %s minutese \" % np.round((time.time() - start_time)/60, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4ce655-5d86-413d-9f2a-ec8bb237520c",
   "metadata": {},
   "source": [
    "### 3.4. Create and save the final results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73b841bd-7a81-44f7-9332-245d47b6bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation columns for prediction time series\n",
    "cols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{model_name}_rmse\", 'NWM_pbias', f\"{model_name}_pbias\", \n",
    "        'NWM_kge', f\"{model_name}__kge\", 'NWM_mape',  f\"{model_name}_mape\"]\n",
    "\n",
    "#Evaluation columns for accumulated supply time series\n",
    "supcols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{model_name}_rmse\", 'NWM_pbias', f\"{model_name}_pbias\", \n",
    "        'NWM_kge', f\"{model_name}__kge\", 'NWM_mape',  f\"{model_name}_mape\", 'Obs_vol', 'NWM_vol', f\"{model_name}_vol\",\n",
    "        'NWM_vol_err', f\"{model_name}_vol_err\", 'NWM_vol_Perc_diff', f\"{model_name}_vol_Perc_diff\"]\n",
    "    \n",
    "#save model results\n",
    "EvalDF_all = pd.DataFrame(EvalDF_all, columns=cols)\n",
    "SupplyEvalDF_all = pd.DataFrame(SupplyEvalDF_all, columns=supcols)\n",
    "EvalDF_all.to_csv(f\"{path_save_data}/{model_name}_Performance.csv\")   \n",
    "SupplyEvalDF_all.to_csv(f\"{path_save_data}/{model_name}_Supply_Performance.csv\")\n",
    "torch.save(best_model_parameters, path_model_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2dea70-6237-48e0-ad91-a93de9b01084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for Daily cfs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USGSid</th>\n",
       "      <th>NHDPlusid</th>\n",
       "      <th>NWM_rmse</th>\n",
       "      <th>MLP_rmse</th>\n",
       "      <th>NWM_pbias</th>\n",
       "      <th>MLP_pbias</th>\n",
       "      <th>NWM_kge</th>\n",
       "      <th>MLP__kge</th>\n",
       "      <th>NWM_mape</th>\n",
       "      <th>MLP_mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10105900.0</td>\n",
       "      <td>666170.0</td>\n",
       "      <td>90.667995</td>\n",
       "      <td>149.360619</td>\n",
       "      <td>-16.13</td>\n",
       "      <td>-82.384667</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.61</td>\n",
       "      <td>362.513333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10126000.0</td>\n",
       "      <td>4605050.0</td>\n",
       "      <td>1861.439475</td>\n",
       "      <td>1831.621193</td>\n",
       "      <td>-53.15</td>\n",
       "      <td>88.168000</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>369.37</td>\n",
       "      <td>73.292333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10129900.0</td>\n",
       "      <td>10093082.0</td>\n",
       "      <td>29.960617</td>\n",
       "      <td>161.655173</td>\n",
       "      <td>-210.87</td>\n",
       "      <td>-2294.911333</td>\n",
       "      <td>-3.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.61</td>\n",
       "      <td>3861.243333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10133650.0</td>\n",
       "      <td>10276856.0</td>\n",
       "      <td>34.537693</td>\n",
       "      <td>151.399416</td>\n",
       "      <td>-135.83</td>\n",
       "      <td>-826.372667</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.07</td>\n",
       "      <td>1604.811667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10133800.0</td>\n",
       "      <td>10276836.0</td>\n",
       "      <td>53.460353</td>\n",
       "      <td>141.121091</td>\n",
       "      <td>-127.58</td>\n",
       "      <td>-446.471667</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>254.98</td>\n",
       "      <td>890.269333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10133980.0</td>\n",
       "      <td>10276712.0</td>\n",
       "      <td>97.119345</td>\n",
       "      <td>133.384267</td>\n",
       "      <td>-179.04</td>\n",
       "      <td>-276.707667</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>382.65</td>\n",
       "      <td>665.407333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10134500.0</td>\n",
       "      <td>10277268.0</td>\n",
       "      <td>128.603024</td>\n",
       "      <td>130.929166</td>\n",
       "      <td>-235.17</td>\n",
       "      <td>-262.889667</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1014.93</td>\n",
       "      <td>1317.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10136500.0</td>\n",
       "      <td>10274616.0</td>\n",
       "      <td>776.608631</td>\n",
       "      <td>431.781936</td>\n",
       "      <td>-191.07</td>\n",
       "      <td>50.872000</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436.46</td>\n",
       "      <td>89.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10137500.0</td>\n",
       "      <td>10274270.0</td>\n",
       "      <td>107.073089</td>\n",
       "      <td>157.761899</td>\n",
       "      <td>33.05</td>\n",
       "      <td>-43.958333</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.66</td>\n",
       "      <td>225.927333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10140100.0</td>\n",
       "      <td>10275828.0</td>\n",
       "      <td>270.370388</td>\n",
       "      <td>245.704399</td>\n",
       "      <td>-134.83</td>\n",
       "      <td>-50.676000</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813.07</td>\n",
       "      <td>746.018667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10140700.0</td>\n",
       "      <td>10274376.0</td>\n",
       "      <td>268.872860</td>\n",
       "      <td>289.840660</td>\n",
       "      <td>-54.52</td>\n",
       "      <td>6.185667</td>\n",
       "      <td>0.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325.10</td>\n",
       "      <td>288.651333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10141000.0</td>\n",
       "      <td>10273232.0</td>\n",
       "      <td>1317.964422</td>\n",
       "      <td>513.808371</td>\n",
       "      <td>-412.87</td>\n",
       "      <td>38.813333</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1670.95</td>\n",
       "      <td>155.362667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10145400.0</td>\n",
       "      <td>10331031.0</td>\n",
       "      <td>14.703517</td>\n",
       "      <td>155.573486</td>\n",
       "      <td>27.32</td>\n",
       "      <td>-1045.184667</td>\n",
       "      <td>0.51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.40</td>\n",
       "      <td>2560.042333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10149000.0</td>\n",
       "      <td>10349162.0</td>\n",
       "      <td>17.955594</td>\n",
       "      <td>139.024298</td>\n",
       "      <td>48.43</td>\n",
       "      <td>-458.682333</td>\n",
       "      <td>0.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.60</td>\n",
       "      <td>519.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10153100.0</td>\n",
       "      <td>10348934.0</td>\n",
       "      <td>37.032186</td>\n",
       "      <td>138.247417</td>\n",
       "      <td>2.38</td>\n",
       "      <td>-296.990667</td>\n",
       "      <td>0.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.00</td>\n",
       "      <td>659.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10155000.0</td>\n",
       "      <td>10373622.0</td>\n",
       "      <td>317.555275</td>\n",
       "      <td>508.566862</td>\n",
       "      <td>21.78</td>\n",
       "      <td>46.916333</td>\n",
       "      <td>0.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.39</td>\n",
       "      <td>136.885667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10155200.0</td>\n",
       "      <td>10373794.0</td>\n",
       "      <td>226.263752</td>\n",
       "      <td>259.232790</td>\n",
       "      <td>2.69</td>\n",
       "      <td>35.093000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.78</td>\n",
       "      <td>32.096667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10157500.0</td>\n",
       "      <td>10375690.0</td>\n",
       "      <td>96.068404</td>\n",
       "      <td>164.329446</td>\n",
       "      <td>-1688.09</td>\n",
       "      <td>-3376.053667</td>\n",
       "      <td>-16.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25067.34</td>\n",
       "      <td>54701.771333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10164500.0</td>\n",
       "      <td>10329013.0</td>\n",
       "      <td>110.840940</td>\n",
       "      <td>138.866774</td>\n",
       "      <td>-123.23</td>\n",
       "      <td>-306.268000</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.71</td>\n",
       "      <td>882.192667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10171000.0</td>\n",
       "      <td>10390290.0</td>\n",
       "      <td>3558.880086</td>\n",
       "      <td>55.151416</td>\n",
       "      <td>-2822.39</td>\n",
       "      <td>-34.760000</td>\n",
       "      <td>-29.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3342.15</td>\n",
       "      <td>60.440000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        USGSid   NHDPlusid     NWM_rmse     MLP_rmse  NWM_pbias    MLP_pbias  \\\n",
       "0   10105900.0    666170.0    90.667995   149.360619     -16.13   -82.384667   \n",
       "1   10126000.0   4605050.0  1861.439475  1831.621193     -53.15    88.168000   \n",
       "2   10129900.0  10093082.0    29.960617   161.655173    -210.87 -2294.911333   \n",
       "3   10133650.0  10276856.0    34.537693   151.399416    -135.83  -826.372667   \n",
       "4   10133800.0  10276836.0    53.460353   141.121091    -127.58  -446.471667   \n",
       "5   10133980.0  10276712.0    97.119345   133.384267    -179.04  -276.707667   \n",
       "6   10134500.0  10277268.0   128.603024   130.929166    -235.17  -262.889667   \n",
       "7   10136500.0  10274616.0   776.608631   431.781936    -191.07    50.872000   \n",
       "8   10137500.0  10274270.0   107.073089   157.761899      33.05   -43.958333   \n",
       "9   10140100.0  10275828.0   270.370388   245.704399    -134.83   -50.676000   \n",
       "10  10140700.0  10274376.0   268.872860   289.840660     -54.52     6.185667   \n",
       "11  10141000.0  10273232.0  1317.964422   513.808371    -412.87    38.813333   \n",
       "12  10145400.0  10331031.0    14.703517   155.573486      27.32 -1045.184667   \n",
       "13  10149000.0  10349162.0    17.955594   139.024298      48.43  -458.682333   \n",
       "14  10153100.0  10348934.0    37.032186   138.247417       2.38  -296.990667   \n",
       "15  10155000.0  10373622.0   317.555275   508.566862      21.78    46.916333   \n",
       "16  10155200.0  10373794.0   226.263752   259.232790       2.69    35.093000   \n",
       "17  10157500.0  10375690.0    96.068404   164.329446   -1688.09 -3376.053667   \n",
       "18  10164500.0  10329013.0   110.840940   138.866774    -123.23  -306.268000   \n",
       "19  10171000.0  10390290.0  3558.880086    55.151416   -2822.39   -34.760000   \n",
       "\n",
       "    NWM_kge  MLP__kge  NWM_mape      MLP_mape  \n",
       "0      0.72       NaN    108.61    362.513333  \n",
       "1      0.14       NaN    369.37     73.292333  \n",
       "2     -3.04       NaN    225.61   3861.243333  \n",
       "3     -0.56       NaN    271.07   1604.811667  \n",
       "4     -0.41       NaN    254.98    890.269333  \n",
       "5     -0.87       NaN    382.65    665.407333  \n",
       "6     -1.48       NaN   1014.93   1317.679000  \n",
       "7     -0.99       NaN    436.46     89.633000  \n",
       "8      0.48       NaN     38.66    225.927333  \n",
       "9     -0.42       NaN    813.07    746.018667  \n",
       "10     0.30       NaN    325.10    288.651333  \n",
       "11    -3.20       NaN   1670.95    155.362667  \n",
       "12     0.51       NaN     40.40   2560.042333  \n",
       "13     0.35       NaN     48.60    519.685000  \n",
       "14     0.66       NaN     69.00    659.794000  \n",
       "15     0.47       NaN    123.39    136.885667  \n",
       "16     0.59       NaN     53.78     32.096667  \n",
       "17   -16.18       NaN  25067.34  54701.771333  \n",
       "18    -0.95       NaN    130.71    882.192667  \n",
       "19   -29.81       NaN   3342.15     60.440000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for Daily Accumulated Supply (Acre-Feet)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USGSid</th>\n",
       "      <th>NHDPlusid</th>\n",
       "      <th>NWM_rmse</th>\n",
       "      <th>MLP_rmse</th>\n",
       "      <th>NWM_pbias</th>\n",
       "      <th>MLP_pbias</th>\n",
       "      <th>NWM_kge</th>\n",
       "      <th>MLP__kge</th>\n",
       "      <th>NWM_mape</th>\n",
       "      <th>MLP_mape</th>\n",
       "      <th>Obs_vol</th>\n",
       "      <th>NWM_vol</th>\n",
       "      <th>MLP_vol</th>\n",
       "      <th>NWM_vol_err</th>\n",
       "      <th>MLP_vol_err</th>\n",
       "      <th>NWM_vol_Perc_diff</th>\n",
       "      <th>MLP_vol_Perc_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10105900.0</td>\n",
       "      <td>666170.0</td>\n",
       "      <td>1.432436e+04</td>\n",
       "      <td>38395.037230</td>\n",
       "      <td>-2.92</td>\n",
       "      <td>-47.620333</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.359000</td>\n",
       "      <td>27.83</td>\n",
       "      <td>168.213333</td>\n",
       "      <td>37232.679662</td>\n",
       "      <td>50915.508</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>1.368283e+04</td>\n",
       "      <td>54351.868515</td>\n",
       "      <td>36.749513</td>\n",
       "      <td>145.978933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10126000.0</td>\n",
       "      <td>4605050.0</td>\n",
       "      <td>3.522460e+05</td>\n",
       "      <td>685859.751922</td>\n",
       "      <td>-28.50</td>\n",
       "      <td>90.420333</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.334667</td>\n",
       "      <td>50.06</td>\n",
       "      <td>88.548000</td>\n",
       "      <td>655183.188796</td>\n",
       "      <td>1149487.593</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>4.943044e+05</td>\n",
       "      <td>-563598.640619</td>\n",
       "      <td>75.445221</td>\n",
       "      <td>-86.021536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10129900.0</td>\n",
       "      <td>10093082.0</td>\n",
       "      <td>8.093940e+03</td>\n",
       "      <td>65235.452003</td>\n",
       "      <td>-219.00</td>\n",
       "      <td>-1902.438333</td>\n",
       "      <td>-2.08</td>\n",
       "      <td>-23.894333</td>\n",
       "      <td>220.63</td>\n",
       "      <td>2319.142667</td>\n",
       "      <td>3759.072756</td>\n",
       "      <td>18822.636</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>1.506356e+04</td>\n",
       "      <td>87825.475421</td>\n",
       "      <td>400.725504</td>\n",
       "      <td>2336.360085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10133650.0</td>\n",
       "      <td>10276856.0</td>\n",
       "      <td>9.281958e+03</td>\n",
       "      <td>56810.286018</td>\n",
       "      <td>-94.33</td>\n",
       "      <td>-658.212333</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-7.271000</td>\n",
       "      <td>105.73</td>\n",
       "      <td>867.646333</td>\n",
       "      <td>9663.116575</td>\n",
       "      <td>30161.430</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>2.049831e+04</td>\n",
       "      <td>81921.431603</td>\n",
       "      <td>212.129423</td>\n",
       "      <td>847.774431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10133800.0</td>\n",
       "      <td>10276836.0</td>\n",
       "      <td>1.579701e+04</td>\n",
       "      <td>53950.671870</td>\n",
       "      <td>-93.53</td>\n",
       "      <td>-356.516667</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-3.488667</td>\n",
       "      <td>108.76</td>\n",
       "      <td>494.656333</td>\n",
       "      <td>18057.498883</td>\n",
       "      <td>47800.215</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>2.974272e+04</td>\n",
       "      <td>73527.049294</td>\n",
       "      <td>164.711161</td>\n",
       "      <td>407.182909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10133980.0</td>\n",
       "      <td>10276712.0</td>\n",
       "      <td>3.113292e+04</td>\n",
       "      <td>45103.131903</td>\n",
       "      <td>-133.03</td>\n",
       "      <td>-202.751667</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-1.425667</td>\n",
       "      <td>155.75</td>\n",
       "      <td>302.033333</td>\n",
       "      <td>23337.537876</td>\n",
       "      <td>78025.101</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>5.468756e+04</td>\n",
       "      <td>68247.010301</td>\n",
       "      <td>234.333045</td>\n",
       "      <td>292.434492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10134500.0</td>\n",
       "      <td>10277268.0</td>\n",
       "      <td>4.855101e+04</td>\n",
       "      <td>50585.680163</td>\n",
       "      <td>-279.98</td>\n",
       "      <td>-309.210333</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>-2.489000</td>\n",
       "      <td>876.92</td>\n",
       "      <td>1142.431333</td>\n",
       "      <td>37262.276126</td>\n",
       "      <td>96970.683</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>5.970841e+04</td>\n",
       "      <td>54322.272051</td>\n",
       "      <td>160.238217</td>\n",
       "      <td>145.783558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10136500.0</td>\n",
       "      <td>10274616.0</td>\n",
       "      <td>2.882531e+05</td>\n",
       "      <td>134843.611629</td>\n",
       "      <td>-171.96</td>\n",
       "      <td>57.360000</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>359.13</td>\n",
       "      <td>67.168000</td>\n",
       "      <td>172949.053299</td>\n",
       "      <td>490788.534</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>3.178395e+05</td>\n",
       "      <td>-81364.505122</td>\n",
       "      <td>183.776363</td>\n",
       "      <td>-47.045360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10137500.0</td>\n",
       "      <td>10274270.0</td>\n",
       "      <td>2.471682e+04</td>\n",
       "      <td>32514.007408</td>\n",
       "      <td>39.60</td>\n",
       "      <td>-17.906667</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.579000</td>\n",
       "      <td>41.05</td>\n",
       "      <td>131.442667</td>\n",
       "      <td>54744.014371</td>\n",
       "      <td>39350.652</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-1.539336e+04</td>\n",
       "      <td>36840.533806</td>\n",
       "      <td>-28.118804</td>\n",
       "      <td>67.296003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10140100.0</td>\n",
       "      <td>10275828.0</td>\n",
       "      <td>6.143533e+04</td>\n",
       "      <td>60135.976737</td>\n",
       "      <td>-95.65</td>\n",
       "      <td>-16.653667</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.249333</td>\n",
       "      <td>535.25</td>\n",
       "      <td>535.748333</td>\n",
       "      <td>28156.377964</td>\n",
       "      <td>119525.325</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>9.136895e+04</td>\n",
       "      <td>63428.170213</td>\n",
       "      <td>324.505329</td>\n",
       "      <td>225.271057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10140700.0</td>\n",
       "      <td>10274376.0</td>\n",
       "      <td>4.413617e+04</td>\n",
       "      <td>78646.414093</td>\n",
       "      <td>-36.60</td>\n",
       "      <td>23.251333</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.182333</td>\n",
       "      <td>219.56</td>\n",
       "      <td>217.031000</td>\n",
       "      <td>69646.011173</td>\n",
       "      <td>128776.020</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>5.913001e+04</td>\n",
       "      <td>21938.537004</td>\n",
       "      <td>84.900783</td>\n",
       "      <td>31.500062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10141000.0</td>\n",
       "      <td>10273232.0</td>\n",
       "      <td>5.095959e+05</td>\n",
       "      <td>150120.327644</td>\n",
       "      <td>-353.55</td>\n",
       "      <td>49.704667</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>-0.078667</td>\n",
       "      <td>921.44</td>\n",
       "      <td>93.952333</td>\n",
       "      <td>120033.770709</td>\n",
       "      <td>685973.241</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>5.659395e+05</td>\n",
       "      <td>-28449.222532</td>\n",
       "      <td>471.483539</td>\n",
       "      <td>-23.701015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10145400.0</td>\n",
       "      <td>10331031.0</td>\n",
       "      <td>3.225033e+03</td>\n",
       "      <td>60337.144391</td>\n",
       "      <td>31.13</td>\n",
       "      <td>-898.583333</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-8.844333</td>\n",
       "      <td>28.22</td>\n",
       "      <td>1913.048333</td>\n",
       "      <td>5799.237239</td>\n",
       "      <td>4021.524</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-1.777713e+03</td>\n",
       "      <td>85785.310938</td>\n",
       "      <td>-30.654260</td>\n",
       "      <td>1479.251622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10149000.0</td>\n",
       "      <td>10349162.0</td>\n",
       "      <td>6.448950e+03</td>\n",
       "      <td>54518.427794</td>\n",
       "      <td>49.03</td>\n",
       "      <td>-423.952667</td>\n",
       "      <td>0.37</td>\n",
       "      <td>-4.726333</td>\n",
       "      <td>53.70</td>\n",
       "      <td>463.903333</td>\n",
       "      <td>13063.497566</td>\n",
       "      <td>8671.659</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-4.391839e+03</td>\n",
       "      <td>78521.050611</td>\n",
       "      <td>-33.619163</td>\n",
       "      <td>601.072188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10153100.0</td>\n",
       "      <td>10348934.0</td>\n",
       "      <td>6.876268e+03</td>\n",
       "      <td>49654.959354</td>\n",
       "      <td>-5.25</td>\n",
       "      <td>-237.114667</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-1.689333</td>\n",
       "      <td>38.24</td>\n",
       "      <td>401.462000</td>\n",
       "      <td>17670.778202</td>\n",
       "      <td>14955.786</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-2.714992e+03</td>\n",
       "      <td>73913.769975</td>\n",
       "      <td>-15.364305</td>\n",
       "      <td>418.282484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10155000.0</td>\n",
       "      <td>10373622.0</td>\n",
       "      <td>5.574286e+04</td>\n",
       "      <td>97110.707675</td>\n",
       "      <td>30.27</td>\n",
       "      <td>55.050000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>33.88</td>\n",
       "      <td>56.860000</td>\n",
       "      <td>181010.801017</td>\n",
       "      <td>126493.587</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-5.451721e+04</td>\n",
       "      <td>-89426.252840</td>\n",
       "      <td>-30.118210</td>\n",
       "      <td>-49.403821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10155200.0</td>\n",
       "      <td>10373794.0</td>\n",
       "      <td>4.199103e+04</td>\n",
       "      <td>48273.913962</td>\n",
       "      <td>-6.42</td>\n",
       "      <td>32.496000</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.405667</td>\n",
       "      <td>40.88</td>\n",
       "      <td>29.610000</td>\n",
       "      <td>166897.779313</td>\n",
       "      <td>134740.884</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>-3.215690e+04</td>\n",
       "      <td>-75313.231135</td>\n",
       "      <td>-19.267420</td>\n",
       "      <td>-45.125364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10157500.0</td>\n",
       "      <td>10375690.0</td>\n",
       "      <td>3.328159e+04</td>\n",
       "      <td>57875.411351</td>\n",
       "      <td>-1245.39</td>\n",
       "      <td>-2346.145000</td>\n",
       "      <td>-13.27</td>\n",
       "      <td>-24.542000</td>\n",
       "      <td>3133.10</td>\n",
       "      <td>7799.798667</td>\n",
       "      <td>766.768077</td>\n",
       "      <td>39558.867</td>\n",
       "      <td>89244.796615</td>\n",
       "      <td>3.879210e+04</td>\n",
       "      <td>88478.028537</td>\n",
       "      <td>5059.169789</td>\n",
       "      <td>11539.086088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10164500.0</td>\n",
       "      <td>10329013.0</td>\n",
       "      <td>2.727205e+04</td>\n",
       "      <td>50462.456051</td>\n",
       "      <td>-139.66</td>\n",
       "      <td>-284.457333</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>-2.157333</td>\n",
       "      <td>153.31</td>\n",
       "      <td>725.669333</td>\n",
       "      <td>19609.368808</td>\n",
       "      <td>59842.974</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>4.023361e+04</td>\n",
       "      <td>71975.179369</td>\n",
       "      <td>205.175422</td>\n",
       "      <td>367.044855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10171000.0</td>\n",
       "      <td>10390290.0</td>\n",
       "      <td>1.474403e+06</td>\n",
       "      <td>20030.337612</td>\n",
       "      <td>-3049.08</td>\n",
       "      <td>-42.186000</td>\n",
       "      <td>-40.42</td>\n",
       "      <td>0.488000</td>\n",
       "      <td>3175.54</td>\n",
       "      <td>50.718000</td>\n",
       "      <td>81024.872023</td>\n",
       "      <td>1912801.800</td>\n",
       "      <td>91584.548177</td>\n",
       "      <td>1.831777e+06</td>\n",
       "      <td>10559.676154</td>\n",
       "      <td>2260.758804</td>\n",
       "      <td>13.032635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        USGSid   NHDPlusid      NWM_rmse       MLP_rmse  NWM_pbias  \\\n",
       "0   10105900.0    666170.0  1.432436e+04   38395.037230      -2.92   \n",
       "1   10126000.0   4605050.0  3.522460e+05  685859.751922     -28.50   \n",
       "2   10129900.0  10093082.0  8.093940e+03   65235.452003    -219.00   \n",
       "3   10133650.0  10276856.0  9.281958e+03   56810.286018     -94.33   \n",
       "4   10133800.0  10276836.0  1.579701e+04   53950.671870     -93.53   \n",
       "5   10133980.0  10276712.0  3.113292e+04   45103.131903    -133.03   \n",
       "6   10134500.0  10277268.0  4.855101e+04   50585.680163    -279.98   \n",
       "7   10136500.0  10274616.0  2.882531e+05  134843.611629    -171.96   \n",
       "8   10137500.0  10274270.0  2.471682e+04   32514.007408      39.60   \n",
       "9   10140100.0  10275828.0  6.143533e+04   60135.976737     -95.65   \n",
       "10  10140700.0  10274376.0  4.413617e+04   78646.414093     -36.60   \n",
       "11  10141000.0  10273232.0  5.095959e+05  150120.327644    -353.55   \n",
       "12  10145400.0  10331031.0  3.225033e+03   60337.144391      31.13   \n",
       "13  10149000.0  10349162.0  6.448950e+03   54518.427794      49.03   \n",
       "14  10153100.0  10348934.0  6.876268e+03   49654.959354      -5.25   \n",
       "15  10155000.0  10373622.0  5.574286e+04   97110.707675      30.27   \n",
       "16  10155200.0  10373794.0  4.199103e+04   48273.913962      -6.42   \n",
       "17  10157500.0  10375690.0  3.328159e+04   57875.411351   -1245.39   \n",
       "18  10164500.0  10329013.0  2.727205e+04   50462.456051    -139.66   \n",
       "19  10171000.0  10390290.0  1.474403e+06   20030.337612   -3049.08   \n",
       "\n",
       "      MLP_pbias  NWM_kge   MLP__kge  NWM_mape     MLP_mape        Obs_vol  \\\n",
       "0    -47.620333     0.78   0.359000     27.83   168.213333   37232.679662   \n",
       "1     90.420333     0.46  -0.334667     50.06    88.548000  655183.188796   \n",
       "2  -1902.438333    -2.08 -23.894333    220.63  2319.142667    3759.072756   \n",
       "3   -658.212333    -0.30  -7.271000    105.73   867.646333    9663.116575   \n",
       "4   -356.516667    -0.30  -3.488667    108.76   494.656333   18057.498883   \n",
       "5   -202.751667    -0.73  -1.425667    155.75   302.033333   23337.537876   \n",
       "6   -309.210333    -2.35  -2.489000    876.92  1142.431333   37262.276126   \n",
       "7     57.360000    -0.99   0.010333    359.13    67.168000  172949.053299   \n",
       "8    -17.906667     0.54   0.579000     41.05   131.442667   54744.014371   \n",
       "9    -16.653667    -0.01   0.249333    535.25   535.748333   28156.377964   \n",
       "10    23.251333     0.63   0.182333    219.56   217.031000   69646.011173   \n",
       "11    49.704667    -2.87  -0.078667    921.44    93.952333  120033.770709   \n",
       "12  -898.583333     0.52  -8.844333     28.22  1913.048333    5799.237239   \n",
       "13  -423.952667     0.37  -4.726333     53.70   463.903333   13063.497566   \n",
       "14  -237.114667     0.67  -1.689333     38.24   401.462000   17670.778202   \n",
       "15    55.050000     0.59   0.129333     33.88    56.860000  181010.801017   \n",
       "16    32.496000     0.79   0.405667     40.88    29.610000  166897.779313   \n",
       "17 -2346.145000   -13.27 -24.542000   3133.10  7799.798667     766.768077   \n",
       "18  -284.457333    -0.73  -2.157333    153.31   725.669333   19609.368808   \n",
       "19   -42.186000   -40.42   0.488000   3175.54    50.718000   81024.872023   \n",
       "\n",
       "        NWM_vol       MLP_vol   NWM_vol_err    MLP_vol_err  NWM_vol_Perc_diff  \\\n",
       "0     50915.508  91584.548177  1.368283e+04   54351.868515          36.749513   \n",
       "1   1149487.593  91584.548177  4.943044e+05 -563598.640619          75.445221   \n",
       "2     18822.636  91584.548177  1.506356e+04   87825.475421         400.725504   \n",
       "3     30161.430  91584.548177  2.049831e+04   81921.431603         212.129423   \n",
       "4     47800.215  91584.548177  2.974272e+04   73527.049294         164.711161   \n",
       "5     78025.101  91584.548177  5.468756e+04   68247.010301         234.333045   \n",
       "6     96970.683  91584.548177  5.970841e+04   54322.272051         160.238217   \n",
       "7    490788.534  91584.548177  3.178395e+05  -81364.505122         183.776363   \n",
       "8     39350.652  91584.548177 -1.539336e+04   36840.533806         -28.118804   \n",
       "9    119525.325  91584.548177  9.136895e+04   63428.170213         324.505329   \n",
       "10   128776.020  91584.548177  5.913001e+04   21938.537004          84.900783   \n",
       "11   685973.241  91584.548177  5.659395e+05  -28449.222532         471.483539   \n",
       "12     4021.524  91584.548177 -1.777713e+03   85785.310938         -30.654260   \n",
       "13     8671.659  91584.548177 -4.391839e+03   78521.050611         -33.619163   \n",
       "14    14955.786  91584.548177 -2.714992e+03   73913.769975         -15.364305   \n",
       "15   126493.587  91584.548177 -5.451721e+04  -89426.252840         -30.118210   \n",
       "16   134740.884  91584.548177 -3.215690e+04  -75313.231135         -19.267420   \n",
       "17    39558.867  89244.796615  3.879210e+04   88478.028537        5059.169789   \n",
       "18    59842.974  91584.548177  4.023361e+04   71975.179369         205.175422   \n",
       "19  1912801.800  91584.548177  1.831777e+06   10559.676154        2260.758804   \n",
       "\n",
       "    MLP_vol_Perc_diff  \n",
       "0          145.978933  \n",
       "1          -86.021536  \n",
       "2         2336.360085  \n",
       "3          847.774431  \n",
       "4          407.182909  \n",
       "5          292.434492  \n",
       "6          145.783558  \n",
       "7          -47.045360  \n",
       "8           67.296003  \n",
       "9          225.271057  \n",
       "10          31.500062  \n",
       "11         -23.701015  \n",
       "12        1479.251622  \n",
       "13         601.072188  \n",
       "14         418.282484  \n",
       "15         -49.403821  \n",
       "16         -45.125364  \n",
       "17       11539.086088  \n",
       "18         367.044855  \n",
       "19          13.032635  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb2a06-96bb-4a84-a2e8-b70753a60c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import s_FigureGenerator    \n",
    "plotname = 'MLP_TS_plot'\n",
    "freq = 'D'\n",
    "supply = True\n",
    "title = 'Observed and Modeled flows for NHDPlus Reaches \\n with Upstream Reservoirs in the Great Salt Lake Basin'\n",
    "path_figure_1 = f\"{path_save_figure}/{plotname}\"\n",
    "s_FigureGenerator.TS_plot(best_output, model_name, path_figure_1, title, freq, supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7778762-427f-44ef-bdc9-eb7df4d6089c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotname = 'MLP_ParityPlot'\n",
    "path_figure_2 = f\"{path_save_figure}/{plotname}\"\n",
    "s_FigureGenerator.Parity_plot(best_output, model_name, path_figure_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc5cb-9850-4535-a180-e0499407265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reach = 10273232\n",
    "variables =['NWM_flow', 'flow_cfs']\n",
    "colors = ['blue', 'green']\n",
    "plotname = 'NWMFlow'\n",
    "path_figure_3 = f\"{path_save_figure}/{plotname}.png\"\n",
    "units = 'cfs'\n",
    "y_lab = f\"Flow ({units})\"\n",
    "title = f\"Daily NWM Estimates \\n Reach: {str(reach)}\"\n",
    "\n",
    "s_FigureGenerator.Var_TS_plot(best_output, reach, variables, colors, model_name,y_lab, path_figure_3, title, units, supply = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceedce4-734d-4860-881c-f0eed955b183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9252cb21-e7a3-4098-acc8-377b6a0849e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d175d2-2215-42a9-a8f4-6362891849dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import AWS_transfer\n",
    "state = 'ut'\n",
    "AWS_transfer.Predictions2AWS(model_name, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0cdc22-9dd3-4abe-9dca-c63863eafdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciroh_pytorch_gpu_00",
   "language": "python",
   "name": "ciroh_pytorch_gpu_00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
