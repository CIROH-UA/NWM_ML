{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b2d62-be71-4ba4-8a8f-1da3e858df44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install progressbar xgboost matplotlib boto3 openpyxl tqdm hydroeval hydrotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:40.666198Z",
     "start_time": "2023-11-09T04:18:40.658467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluation_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhydrotools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnwm_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# my packages\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation_metric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MAPE, RMSE, KGE, PBias\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01ms_evalaution_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evtab\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ms_FigureGenerator\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluation_metric'"
     ]
    }
   ],
   "source": [
    "# hydrological packages\n",
    "from hydrotools.nwm_client import utils \n",
    "\n",
    "# my packages\n",
    "from g_evaluation_metric import MAPE, RMSE, KGE, PBias\n",
    "from s_evalaution_table import evtab\n",
    "import s_FigureGenerator\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# system packages\n",
    "from progressbar import ProgressBar\n",
    "from datetime import datetime, date\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "\n",
    "# data analysi packages\n",
    "from scipy import optimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58486c593749230",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:41.288250Z",
     "start_time": "2023-11-09T04:18:41.280798Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    onedrive_path = 'E:/OneDrive/OneDrive - The University of Alabama/10.material/01.data/usgs_data/'\n",
    "    box_path = 'C:/Users/snaserneisary/Box/NWM-ML/'\n",
    "\n",
    "elif platform.system() == 'Darwin':\n",
    "    onedrive_path = '/Users/savalan/Library/CloudStorage/OneDrive-TheUniversityofAlabama/02.projects/03.ciroh/04.data/'\n",
    "    box_path = '/Users/savalan/Library/CloudStorage/Box-Box/NWM-ML/Data/NWM/ut/'\n",
    "    \n",
    "elif platform.system() == 'Linux':\n",
    "    path_01 = '/home/snaserneisary/01.projects/01.ciroh_p8/NWM-ML/Savalan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c64e4a7611ac3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T04:18:43.689720Z",
     "start_time": "2023-11-09T04:18:43.650794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_training_data = pd.read_csv(path_01 + '03.output/raw_training_data.csv')\n",
    "raw_training_data.pop('Unnamed: 0')\n",
    "raw_training_data['station_id'] = raw_training_data['station_id'].astype('str')\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58d9b00594ff3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:23.114069Z",
     "start_time": "2023-11-10T04:40:23.082463Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Training_DF = raw_training_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d8ff5-b95a-40ed-af26-9029e1b54947",
   "metadata": {},
   "source": [
    "### Editing the features based on the feature importance should be in the next cell!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b8226-79f9-4f8d-8528-8989050bbdea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Editing the features based on the feature importance should be done here!!!!!!!!!!!!!!!\n",
    "\n",
    "Training_DF.drop(['precipitation_in', 'temperature_F', 'Mean_Ann_Precip_in', 'Perc_Herbace', 'Perc_Forest',\n",
    "                        'Mean_Basin_Elev_ft'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3765d89e-a4e4-481d-bdd1-f708678004af",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove headwater stations!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2172979-7110-45a7-9778-e30173e34736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headwater_stations = ['10011500', '10109000', '10113500', '10128500', '10131000', '10146400', '10150500', '10154200',\n",
    "'10172700', '10172800', '10172952']\n",
    "Training_DF = Training_DF[~raw_training_data['station_id'].isin(headwater_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2448b6151fb377",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:30.482191Z",
     "start_time": "2023-11-10T04:40:30.463560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Training_DF.datetime = pd.to_datetime(Training_DF.datetime)\n",
    "Training_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a160a8633a96ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:40.495727Z",
     "start_time": "2023-11-10T04:40:40.474155Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train_temp = Training_DF[Training_DF.datetime < '01-01-2015']\n",
    "x_train_temp.pop('station_id')\n",
    "x_train_temp.pop('datetime')\n",
    "y_train_temp = x_train_temp['flow_cfs']\n",
    "x_train_temp.pop('flow_cfs')\n",
    "x_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0faa488a9e9e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:41.923386Z",
     "start_time": "2023-11-10T04:40:41.912929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the train inputs of the NN model\n",
    "# First we need to convert it from pandas dataframe to a numpy array \n",
    "y_train = y_train_temp.to_numpy()\n",
    "x_train = x_train_temp.to_numpy()\n",
    "scaler = MinMaxScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "y_scaled_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "y_scaled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd46d43cce5d1387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:44.100124Z",
     "start_time": "2023-11-10T04:40:44.086373Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determining the test dataset. \n",
    "x_test_temp = Training_DF[Training_DF.datetime >= '01-01-2015']\n",
    "x_test_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48a068a-2fdd-4ac7-9079-9d35edd9e013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the test inputs of the NN model\n",
    "# First we need to convert it from pandas dataframe to a numpy array \n",
    "x_test_temp_1 = x_test_temp.copy()\n",
    "station_index_list = x_test_temp_1['station_id']\n",
    "x_test_temp_1.pop('station_id')\n",
    "x_test_temp_1.pop('datetime')\n",
    "y_test_temp_1 = x_test_temp_1['flow_cfs']\n",
    "x_test_temp_1.pop('flow_cfs')\n",
    "x_test_1_np = x_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "y_test_1_np = y_test_temp_1.reset_index(drop=True).to_numpy()\n",
    "x_test_1_scaled = scaler.fit_transform(x_test_1_np)\n",
    "y_scaled_test_1 = scaler.fit_transform(y_test_1_np.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d7dffbf2edc7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T04:40:52.472498Z",
     "start_time": "2023-11-10T04:40:52.464352Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape input for MLP model\n",
    "x_train_scaled_test = torch.Tensor(x_train_scaled)\n",
    "y_train_scaled_test = torch.Tensor(y_scaled_train)\n",
    "print('test shape', x_train_scaled_test.shape)\n",
    "print('train shape', y_train_scaled_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cd2fd-2500-47c7-8005-7e81521a8b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomMLP(nn.Module):\n",
    "    def __init__(self, layer_sizes, optimizer, device=None):\n",
    "        super(CustomMLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.device = device\n",
    "        self.to(self.device)\n",
    "        self.validation_indicator = 0\n",
    "        self.optim = optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, train_loader, epochs, early_stopping_patience=0, save_path=None, val_loader=None):\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()  # Set the model to training mode\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            \n",
    "            val_loss = 0\n",
    "            if val_loader is not None:\n",
    "                self.validation_indicator = 1\n",
    "                val_loss = self.evaluate_model(val_loader)[1]\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    torch.save(self.state_dict(), save_path)\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "                if epochs_no_improve == early_stopping_patience and early_stopping_patience > 0:\n",
    "                    print('Early stopping triggered')\n",
    "                    break\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Training Loss: {loss.item()}', f'Validation Loss: {val_loss}')\n",
    "        self.validation_indicator = 0\n",
    "        print('Training is done!')\n",
    "\n",
    "    def evaluate_model(self, data_loader):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in data_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_function(outputs, targets)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                total += inputs.size(0)\n",
    "        avg_loss = total_loss / total\n",
    "        if self.validation_indicator == 0:\n",
    "            print(f'Validation Loss: {avg_loss}')\n",
    "        return outputs, avg_loss\n",
    "        #outputs if self.validation_indicator == 0 else avg_loss\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path, map_location=self.device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b119b-2344-40ba-a0ae-378d56ff15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets and dataloaders\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_train_scaled_test, y_train_scaled_test)\n",
    "train_dataset = TensorDataset(x_train_scaled_test, y_train_scaled_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_dataset = TensorDataset(X_valid, y_valid)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe54f58-8061-4c5b-b4b9-fa85fbac9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod='MLP'\n",
    "tries = 1\n",
    "epochs = 10\n",
    "batch_size = 50\n",
    "learning_rate = 1e-4\n",
    "early_stopping_patience = 5\n",
    "decay = 0\n",
    "path_model_save = f\"{path_01}/03.output/mlp/best_model.pkl\"\n",
    "layer_sizes = [x_train_scaled_test.shape[1] ,128, 128, 64, 64, 32, 16, 1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84a3b6-9949-4ff6-86b3-fea539b657c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Create variables\n",
    "test_best_val = float('inf')\n",
    "EvalDF = {}\n",
    "SupplyEvalDF = {}\n",
    "EvalDF_all = np.zeros([len(station_index_list.drop_duplicates()), 10])\n",
    "SupplyEvalDF_all = np.zeros([len(station_index_list.drop_duplicates()), 17])\n",
    "\n",
    "# Start running the model several times. \n",
    "for try_number in range(1, tries+1):\n",
    "\n",
    "    # Create the variables. \n",
    "    EvalDF[try_number] = np.zeros([len(station_index_list.drop_duplicates()), 10])\n",
    "    SupplyEvalDF[try_number] = np.zeros([len(station_index_list.drop_duplicates()), 17])\n",
    "    SitesDict = {}\n",
    "    val_loss_all = 0\n",
    "    print(f'Trial Number {try_number} ==========================================================')\n",
    "    \n",
    "    # Set the optimizer, create the model, and train it. \n",
    "    mlp_optimizer = optim.Adam(self.layers.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "    mlp_model = CustomMLP(layer_sizes, mlp_optimizer, device)\n",
    "    mlp_model.train_model(train_loader, epochs, early_stopping_patience, path_model_save, validation_loader)\n",
    "    \n",
    "    # Evaluate it for different stations. \n",
    "    for station_index, station_number in enumerate(station_index_list.drop_duplicates()):\n",
    "        index = station_index_list == station_number # Finind the rows that have this station number.\n",
    "        temp_x_scaled_test = torch.Tensor(x_test_1_scaled)\n",
    "        temp_y_scaled_test = torch.Tensor(y_scaled_test_1)\n",
    "        index_np = torch.tensor(index.to_numpy())\n",
    "        test_dataset = TensorDataset(temp_x_scaled_test[index_np], temp_y_scaled_test[index_np])\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "        \n",
    "        # Evaluation\n",
    "        yhat_test, val_loss = mlp_model.evaluate_model(test_loader)\n",
    "        \n",
    "        # Invert scaling for actual and concat it with the rest of the dataset. \n",
    "        inv_yhat_test = scaler.inverse_transform(yhat_test.numpy())\n",
    "        inv_yhat_test[inv_yhat_test<0] = 0 # THIS IS NOT CORRECT !!!!!!!!!!!!!!!\n",
    "        nwm_test = pd.DataFrame(inv_yhat_test, columns=['MLP_flow'])\n",
    "        Dfs = [nwm_test.reset_index(drop=True), x_test_temp[index].reset_index(drop=True)]\n",
    "        Eval_DF_mine = pd.concat(Dfs, axis=1)\n",
    "        SitesDict[nhdreach] = Eval_DF_mine\n",
    "    \n",
    "        # Get reach id for model eval.\n",
    "        nhdreach = utils.crosswalk(usgs_site_codes=station_number)\n",
    "        nhdreach = nhdreach['nwm_feature_id'].iloc[0]\n",
    "        \n",
    "        # Calculate the results. \n",
    "        prediction_columns = ['NWM_flow', f\"{mod}_flow\"]\n",
    "        observation_column = 'flow_cfs'\n",
    "        result = evtab(Eval_DF_mine, prediction_columns, nhdreach, observation_column, mod)\n",
    "        EvalDF[try_number][station_index, :] = result[0]\n",
    "        SupplyEvalDF[try_number][station_index, :] = result[1]\n",
    "\n",
    "    # Finding the best model. \n",
    "    val_loss_all += val_loss\n",
    "    val_loss_all = val_loss_all / len(station_index_list.drop_duplicates())\n",
    "    if val_loss_all < test_best_val:\n",
    "        test_best_val = val_loss_all\n",
    "        best_model = mlp_model.state_dict()\n",
    "        best_try = try_number\n",
    "        best_output = SitesDict\n",
    "    EvalDF_all = EvalDF[try_number] + EvalDF_all\n",
    "    print(EvalDF_all.shape)\n",
    "    SupplyEvalDF_all = SupplyEvalDF[try_number] + SupplyEvalDF_all\n",
    "        \n",
    "# Save the average results for all of the trials. \n",
    "EvalDF_all = EvalDF_all / tries\n",
    "SupplyEvalDF_all = SupplyEvalDF_all / tries\n",
    "\n",
    "# Sort the outputs of the best model based on date. \n",
    "keys = list(best_output.keys())\n",
    "for key_number in keys:\n",
    "    best_output[key_number] = best_output[key_number].sort_values(by='datetime')\n",
    "    \n",
    "print('finish')\n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b841bd-7a81-44f7-9332-245d47b6bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model scores into a dataframe for comparison\n",
    "mod = 'MLP'\n",
    "\n",
    "#Evaluation columns for prediction time series\n",
    "cols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{mod}_rmse\", 'NWM_pbias', f\"{mod}_pbias\", \n",
    "        'NWM_kge', f\"{mod}__kge\", 'NWM_mape',  f\"{mod}_mape\"]\n",
    "\n",
    "#Evaluation columns for accumulated supply time series\n",
    "supcols = ['USGSid', 'NHDPlusid', 'NWM_rmse', f\"{mod}_rmse\", 'NWM_pbias', f\"{mod}_pbias\", \n",
    "        'NWM_kge', f\"{mod}__kge\", 'NWM_mape',  f\"{mod}_mape\", 'Obs_vol', 'NWM_vol', f\"{mod}_vol\",\n",
    "        'NWM_vol_err', f\"{mod}_vol_err\", 'NWM_vol_Perc_diff', f\"{mod}_vol_Perc_diff\"]\n",
    "    \n",
    "#save model results\n",
    "EvalDF_all = pd.DataFrame(EvalDF_all, columns=cols)\n",
    "SupplyEvalDF_all = pd.DataFrame(SupplyEvalDF_all, columns=supcols)\n",
    "path_save_data = f\"{path_01}/03.output/02.mlp/012.data/\" \n",
    "EvalDF.to_csv(f\"{path_save_data}}/{mod}_Performance.csv\")   \n",
    "SupplyEvalDF.to_csv(f\"{path_save_data}}/{mod}_Supply_Performance.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2dea70-6237-48e0-ad91-a93de9b01084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb2a06-96bb-4a84-a2e8-b70753a60c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(s_FigureGenerator)\n",
    "\n",
    "model = 'MLP'\n",
    "plotname = 'MLP_TS_plot'\n",
    "freq = 'D'\n",
    "supply = True\n",
    "title = 'Observed and Modeled flows for NHDPlus Reaches \\n with Upstream Reservoirs in the Great Salt Lake Basin'\n",
    "path_figures = f\"{path_01}/03.output/02.mlp/01.figures/{plotname}.png\"\n",
    "s_FigureGenerator.TS_plot(best_output, model, path, title, freq, supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7778762-427f-44ef-bdc9-eb7df4d6089c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(s_FigureGenerator)\n",
    "plotname = 'MLP_ParityPlot'\n",
    "path_figures = f\"{path_01}/03.output/02.mlp/01.figures/{plotname}.png\"\n",
    "s_FigureGenerator.Parity_plot(best_output, model, path_figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6cc5cb-9850-4535-a180-e0499407265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(s_FigureGenerator)\n",
    "reach = 10273232\n",
    "variables =['NWM_flow', 'flow_cfs']\n",
    "colors = ['blue', 'green']\n",
    "model = 'MLP'\n",
    "plotname = 'NWMFlow'\n",
    "path_figures = f\"{path_01}/03.output/02.mlp/01.figures/{plotname}.png\"\n",
    "units = 'cfs'\n",
    "y_lab = f\"Flow ({units})\"\n",
    "title = f\"Daily NWM Estimates \\n Reach: {str(reach)}\"\n",
    "\n",
    "s_FigureGenerator.Var_TS_plot(best_output, reach, variables, colors, model,y_lab, path_figures, title, units, supply = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d175d2-2215-42a9-a8f4-6362891849dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import AWS_transfer\n",
    "model = 'MLP'\n",
    "state = 'ut'\n",
    "AWS_transfer.Predictions2AWS(model, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3481cdd-b452-42b7-a4bd-c8e4908eab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciroh_pytorch_gpu_00",
   "language": "python",
   "name": "ciroh_pytorch_gpu_00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
