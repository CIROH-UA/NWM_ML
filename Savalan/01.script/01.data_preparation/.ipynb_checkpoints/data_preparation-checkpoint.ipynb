{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install hydrotools hydroeval progressbar xgboost dataretrieval streamstats earthengine-api geopandas matplotlib boto3 openpyxl tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminary preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T03:01:24.309005Z",
     "start_time": "2023-11-09T03:01:24.303939Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from scipy import optimize\n",
    "#import ee\n",
    "import ulmo\n",
    "import streamstats\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import platform\n",
    "import os\n",
    "import time\n",
    "\n",
    "from data_get import Retriever\n",
    "import Get_StreamStats\n",
    "#import EE_funcs\n",
    "\n",
    "# AWS packages\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Find the directory based on the OS system\n",
    "\n",
    "In this function we find the address to the directory that we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T03:00:42.542692Z",
     "start_time": "2023-11-09T03:00:42.531590Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    onedrive_path = 'E:/OneDrive/OneDrive - The University of Alabama/10.material/01.data/usgs_data/'\n",
    "    box_path = 'C:/Users/snaserneisary/Box/NWM-ML/'\n",
    "\n",
    "elif platform.system() == 'Darwin':\n",
    "    onedrive_path = '/Users/savalan/Library/CloudStorage/OneDrive-TheUniversityofAlabama/02.projects/03.ciroh/04.data/'\n",
    "    box_path = '/Users/savalan/Library/CloudStorage/Box-Box/NWM-ML/Data/NWM/ut/'\n",
    "    \n",
    "elif platform.system() == 'Linux':\n",
    "    path_01 = '/home/ec2-user/SageMaker/01.projects/01.ciroh_p8/NWM-ML/savalan/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set the parameters \n",
    "\n",
    "Here we set parameters like the start and end dates, and station numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T03:00:45.058244Z",
     "start_time": "2023-11-09T03:00:45.046643Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = '1980-01-01'  # Start date.\n",
    "end = '2020-12-31'  # End date.\n",
    "end_year = 2020  # Prefered end date. \n",
    "missing_data = 10  # Prefered missing data value. \n",
    "time_period = 11  # Prefered number of years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the GSL station list. \n",
    "states_list = pd.read_excel(path_01 + '02.input/final_stations_p8.xlsx', sheet_name='all')\n",
    "\n",
    "# Find the valid stations. \n",
    "accpeted_stations = Retriever.valid_station(missing_data, time_period, end_year, states_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting NHD reaches\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accpeted_stations['station'] = accpeted_stations['station'].astype(str)  # Change the column dtype. \n",
    "NWIS_NHDPlus = Retriever.get_nhd_model_info(accpeted_stations)[1]  # Get NWM reaches for each USGS station\n",
    "NWIS_NHDPlus.dropna(inplace=True)  # Drop the stations that don't have NWM reaches. \n",
    "nwis_sites = NWIS_NHDPlus.NWISid.tolist()  # Convert the NWIS site list to a Python list. \n",
    "len(nwis_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the S3 bucket access. \n",
    "\n",
    "#load access key\n",
    "home = os.path.expanduser('~')\n",
    "keypath = \"SageMaker/aws_key.csv\"\n",
    "access = pd.read_csv(f\"{home}/{keypath}\")\n",
    "\n",
    "#start session\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=access['Access key ID'][0],\n",
    "    aws_secret_access_key=access['Secret access key'][0])\n",
    "bucket_name = 'streamflow-app-data'\n",
    "s3 = boto3.resource('s3', region_name = 'us-east-1')\n",
    "bucket = s3.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Get various datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Get USGS station info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T06:24:35.636946Z",
     "start_time": "2023-11-08T06:22:24.274456Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which data source [local/online]? local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 2.7061219215393066 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ask the user for the data source\n",
    "ask = input('Which data source [local/online]?')\n",
    "if ask == 'online':\n",
    "    # Get StreamStats information\n",
    "    StreamStats, coordinates_list = Get_StreamStats.get_USGS_site_info(nwis_sites)\n",
    "\n",
    "    # Modify data\n",
    "    StreamStats['station_id'] = StreamStats['station_id'].astype('str')\n",
    "\n",
    "    # Save data\n",
    "    #StreamStats.to_csv(path_01 + '03.output/stream_stats.csv')\n",
    "    with open(path_01 + '03.output/coordinates.pickle', 'wb') as handle:\n",
    "        pickle.dump(coordinates_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "elif ask == 'local':\n",
    "    StreamStats = pd.read_csv(path_01 + '03.output/stream_stats.csv')\n",
    "    StreamStats.pop('Unnamed: 0') # Drop the residual column created after reading the local file. \n",
    "    # Modify data\n",
    "    StreamStats['station_id'] = StreamStats['station_id'].astype(int).astype(str)\n",
    "    with open(path_01 + '03.output/coordinates.pickle', 'rb') as handle:\n",
    "        coordinates_list = pickle.load(handle)\n",
    "    \n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop the last six rows since we don't have the data for them\n",
    "StreamStats = StreamStats.iloc[:-6, :]\n",
    "nwis_sites = nwis_sites[:-6]\n",
    "NWIS_NHDPlus = NWIS_NHDPlus.iloc[:-6, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Get USGS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T03:01:24.301768Z",
     "start_time": "2023-11-09T03:00:47.718957Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:13<00:00,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 73.70196175575256 seconds \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create variables\n",
    "flow = {}\n",
    "all_sites_flow = pd.DataFrame()\n",
    "pbar = ProgressBar()\n",
    "# Run the code for each station\n",
    "for site in tqdm(nwis_sites):\n",
    "    #print('Getting streamflow data for ', site, end='\\r', flush=True)\n",
    "    flow[site] = Retriever.get_usgs(site, start, end)\n",
    "\n",
    "    #make the date the index for plotting and resampling\n",
    "    flow[site]['datetime'] = flow[site]['value_time']\n",
    "    flow[site].set_index('datetime', inplace = True)\n",
    "\n",
    "    #clean the data\n",
    "    flow[site] = flow[site][flow[site]['value'] > 0]\n",
    "\n",
    "    #resample to a daily resolution\n",
    "    flow[site] = pd.DataFrame(flow[site]['value']).rename(columns = {'value':'flow_cfs'})\n",
    "    flow[site] = flow[site].resample('D').mean()\n",
    "    flow[site]['station_id'] = site\n",
    "    all_sites_flow = pd.concat([all_sites_flow, flow[site]])\n",
    "all_sites_flow.reset_index(inplace=True)\n",
    "    \n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Get storage data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 15.210110902786255 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "storage_info = pd.read_csv(path_01 + '02.input/storage_info.csv', nrows=12)  # Storage list and their information. \n",
    "storage_data = pd.read_csv(path_01 + '02.input/storage_data.csv')  # Daily data list for all the storages. \n",
    "storage_data['Date'] = pd.to_datetime(storage_data['Date'])  # Convert the type of a column.. \n",
    "\n",
    "# Create the variables.\n",
    "all_storage = pd.DataFrame()\n",
    "\n",
    "for i in tqdm(range(len(StreamStats))):\n",
    "    #print(StreamStats.iloc[i, 0], end=\"\\033[K\")\n",
    "    \n",
    "    # Get each station's watershed coordinates. \n",
    "    #watershed = streamstats.watershed.Watershed(StreamStats.iloc[i, 1], StreamStats.iloc[i, 2])\n",
    "    #watershed_boundary = watershed.boundary['features'][0]['geometry']['coordinates']\n",
    "\n",
    "    # Given list of coordinates\n",
    "    #coordinates = watershed_boundary\n",
    "    coordinates = coordinates_list[StreamStats.iloc[i, 0]]\n",
    "\n",
    "    # Create Polygon geometry from the coordinates\n",
    "    geometry = [Polygon(coord) for coord in coordinates]\n",
    "\n",
    "    # Create a GeoDataFrame with the Polygon geometry\n",
    "    gdf = gpd.GeoDataFrame(geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    # Create a GeoDataFrame with storage coordiates. \n",
    "    geometry_points = [Point(xy) for xy in zip(storage_info['lon'], storage_info['lat'])]\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=geometry_points, crs='EPSG:4326')\n",
    "\n",
    "    # Search to see which storages are in the watershed. \n",
    "    snotel_station_list = storage_info[points_gdf.within(gdf.geometry.iloc[0])]\n",
    "    \n",
    "    # Create variables\n",
    "    all_snotel_swe = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    # Check the number of storages for each station. \n",
    "    if len(snotel_station_list) > 1:\n",
    "        for station_index in range(len(snotel_station_list)):\n",
    "            sitecode = snotel_station_list.iloc[station_index, 0]  # Get the storage name. \n",
    "            start_date = pd.to_datetime(start)  # Get the start date.\n",
    "            end_date = pd.to_datetime(end)  # Get the end date. \n",
    "\n",
    "            # Read the specific storage data, and prepare it for the next steps. \n",
    "            snotel_swe = storage_data[(storage_data['storage_name'] == sitecode) & (storage_data['Date'] >= start) & (storage_data['Date'] <= end)]\n",
    "            snotel_swe.drop([snotel_swe.columns[0], 'storage_name'], inplace=True, axis=1) \n",
    "            \n",
    "            # Get the percent of storage capacity which is full. \n",
    "            snotel_swe['Storage (af)'] = snotel_swe['Storage (af)'] / snotel_station_list.iloc[station_index, 4] * 100\n",
    "            snotel_swe.rename(columns={'Storage (af)': sitecode}, inplace=True)\n",
    "\n",
    "            # If it is the first snow station.\n",
    "            if station_index == 0:\n",
    "                all_snotel_swe = snotel_swe\n",
    "                \n",
    "            # If it is not the first snow station. \n",
    "            else:\n",
    "                all_snotel_swe = pd.merge(all_snotel_swe, snotel_swe, on='Date')  # Merge based on date. \n",
    "\n",
    "        # Prepare data. \n",
    "        all_snotel_swe.set_index('Date', inplace=True)\n",
    "        all_snotel_swe['sum'] = all_snotel_swe.sum(axis=1)/ len(snotel_station_list)  # Average of all stations. \n",
    "        all_snotel_swe.drop(all_snotel_swe.iloc[:, :-1], inplace=True, axis=1)\n",
    "        all_snotel_swe.rename(columns={'sum': 'storage'}, inplace=True)\n",
    "        all_snotel_swe['station_id'] = StreamStats.iloc[i, 0]\n",
    "        \n",
    "    # If there is only one storage for this station. \n",
    "    elif len(snotel_station_list) == 1:\n",
    "            sitecode = snotel_station_list.iloc[0, 0]  # Get the storage name. \n",
    "            start_date = pd.to_datetime(start)  # Get the start date.\n",
    "            end_date = pd.to_datetime(end)  # Get the end date. \n",
    "            \n",
    "            # Read the specific storage data, and prepare it for the next steps. \n",
    "            all_snotel_swe = storage_data[(storage_data['storage_name'] == sitecode) & (storage_data['Date'] >= start) & (storage_data['Date'] <= end)]\n",
    "            all_snotel_swe.drop([all_snotel_swe.columns[0], 'storage_name'], inplace=True, axis=1) \n",
    "            all_snotel_swe['Storage (af)'] = all_snotel_swe['Storage (af)'] / snotel_station_list.iloc[0, 4] * 100\n",
    "            all_snotel_swe.rename(columns={'Storage (af)': 'storage'}, inplace=True)\n",
    "            all_snotel_swe['station_id'] = StreamStats.iloc[i, 0]\n",
    "            all_snotel_swe.set_index('Date', inplace=True)\n",
    "            \n",
    "\n",
    "    # If there is no storage for this station. \n",
    "    else:\n",
    "        \n",
    "        # Create an zero dataframe. \n",
    "        all_snotel_swe = pd.DataFrame({'storage': [0], 'station_id': StreamStats.iloc[i, 0]}, index=pd.date_range(start=start, end=end))\n",
    "\n",
    "    # Add all the data for each station to the previous one and make a complete dataset.\n",
    "    all_storage = pd.concat([all_storage, all_snotel_swe])\n",
    "all_storage.reset_index(names='datetime', inplace=True) \n",
    "all_storage['datetime'] = pd.to_datetime(all_storage['datetime'])\n",
    "all_storage['station_id'] = all_storage['station_id'].astype(int).astype(str)\n",
    "all_storage.to_csv(path_01 + '03.output/storage_data.csv')\n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. Get snotel swe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:36<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 36.36195182800293 seconds \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "points_df = pd.read_csv(path_01 + '02.input/Snotel-CDEC-SnowObs.csv')\n",
    "\n",
    "all_snow_swe = pd.DataFrame()\n",
    "for i in tqdm(range(len(StreamStats))):\n",
    "    #print(StreamStats.iloc[i, 0])\n",
    "    #watershed = streamstats.watershed.Watershed(StreamStats.iloc[i, 1], StreamStats.iloc[i, 2])\n",
    "    #watershed_boundary = watershed.boundary['features'][0]['geometry']['coordinates']\n",
    "\n",
    "    # Given list of coordinates\n",
    "    coordinates = coordinates_list[str(int(StreamStats.iloc[i, 0]))]\n",
    "\n",
    "    # Create Polygon geometry from the coordinates\n",
    "    geometry = [Polygon(coord) for coord in coordinates]\n",
    "\n",
    "    # Create a GeoDataFrame with the Polygon geometry\n",
    "    gdf = gpd.GeoDataFrame(geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "    geometry_points = [Point(xy) for xy in zip(points_df['longitude'], points_df['latitude'])]\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=geometry_points, crs='EPSG:4326')\n",
    "\n",
    "    snotel_station_list = points_df[points_gdf.within(gdf.geometry.iloc[0])]\n",
    "\n",
    "    all_snotel_swe = pd.DataFrame()\n",
    "\n",
    "    if len(snotel_station_list) > 1:\n",
    "        for station_index in range(len(snotel_station_list)):\n",
    "            sitecode = snotel_station_list.iloc[station_index, 0]\n",
    "            start_date = pd.to_datetime(start)\n",
    "            end_date = pd.to_datetime(end)\n",
    "\n",
    "            snotel_swe = Retriever.get_snotel(sitecode, start_date, end_date)\n",
    "            #snotel_swe.rename(columns={snotel_swe.columns[0]: 'swe'}, inplace=True)\n",
    "            snotel_swe.drop(snotel_swe.iloc[:, 1:], inplace=True, axis=1)\n",
    "\n",
    "            if station_index == 0:\n",
    "\n",
    "                all_snotel_swe = snotel_swe\n",
    "\n",
    "            else:\n",
    "\n",
    "                all_snotel_swe = pd.merge(all_snotel_swe, snotel_swe, left_index=True, right_index=True)\n",
    "        all_snotel_swe['sum'] = all_snotel_swe.sum(axis=1)/ len(snotel_station_list)\n",
    "        all_snotel_swe.drop(all_snotel_swe.iloc[:, :-1], inplace=True, axis=1)\n",
    "        all_snotel_swe.rename(columns={'sum': 'swe'}, inplace=True)\n",
    "        all_snotel_swe['station_id'] = StreamStats.iloc[i, 0]\n",
    "    elif len(snotel_station_list) == 1:\n",
    "            sitecode = snotel_station_list.iloc[0, 0]\n",
    "            start_date = pd.to_datetime(start)\n",
    "            end_date = pd.to_datetime(end)\n",
    "\n",
    "            all_snotel_swe = Retriever.get_snotel(sitecode, start_date, end_date)\n",
    "            all_snotel_swe.rename(columns={all_snotel_swe.columns[0]: 'swe'}, inplace=True)\n",
    "            all_snotel_swe.drop(snotel_swe.iloc[:, 1:], inplace=True, axis=1)\n",
    "            all_snotel_swe['station_id'] = StreamStats.iloc[i, 0]\n",
    "\n",
    "    else:\n",
    "\n",
    "        all_snotel_swe = pd.DataFrame({'swe': [0], 'station_id': StreamStats.iloc[i, 0]}, index=pd.date_range(start=start, end=end))\n",
    "    all_snow_swe = pd.concat([all_snow_swe, all_snotel_swe])\n",
    "all_snow_swe = all_snow_swe.iloc[:, :2]\n",
    "all_snow_swe.reset_index(names='datetime', inplace=True)\n",
    "all_snow_swe['datetime'] = pd.to_datetime(all_snow_swe['datetime'])\n",
    "all_snow_swe['station_id'] = all_snow_swe['station_id'].astype(int).astype(str)\n",
    "all_snow_swe.to_csv(path_01 + '03.output/snow_data.csv')\n",
    "print(\"Run Time:\" + \" %s seconds \" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T06:24:35.648275Z",
     "start_time": "2023-11-08T06:24:35.645733Z"
    },
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "def dict_to_df(DF, temporal_resample, variable):\n",
    "    var_df = pd.DataFrame()\n",
    "    kg_in = 0.04\n",
    "    pbar = ProgressBar()\n",
    "    for site in pbar(DF):\n",
    "        DF[site].set_index('datetime', inplace = True)\n",
    "\n",
    "        if variable == 'temperature':\n",
    "            #make columns for Fahrenheit\n",
    "            DF[site] = DF[site].resample(temporal_resample).max()\n",
    "            DF[site].reset_index(inplace = True)\n",
    "            DF[site]['temperature_F'] = (DF[site]['temperature']*9/5)+32\n",
    "            DF[site].pop('temperature')\n",
    "            DF[site]['station_id'] = site\n",
    "        if variable == 'total_precipitation':\n",
    "            DF[site] = DF[site].resample(temporal_resample).sum()\n",
    "            DF[site].reset_index(inplace = True)\n",
    "            #make columns for inches\n",
    "            DF[site]['precipitation_in'] = DF[site]['total_precipitation']*kg_in\n",
    "            DF[site].pop('total_precipitation')\n",
    "            DF[site]['station_id'] = site\n",
    "\n",
    "        var_df = pd.concat([var_df, DF[site]])\n",
    "\n",
    "    var_df.reset_index(inplace=True, drop=True)\n",
    "    return var_df, DF\n",
    "#we can make a plotting function that takes in a list of up to 4 model predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_NWM_data(NWIS_NHDPlus, start, end):\n",
    "    NWM_obs = {}\n",
    "    reaches = NWIS_NHDPlus['NHDPlusid'].to_list()\n",
    "    for reach_index, reach in enumerate(reaches):\n",
    "\n",
    "        if reach > 1:\n",
    "            r = str(int(reach))\n",
    "            s = NWIS_NHDPlus.iloc[reach_index, 2]\n",
    "            reachid = f\"NWM_v2.1/NHD_segments_{s}.h5/NWM_{r}.csv\"\n",
    "            obj = bucket.Object(reachid)\n",
    "            body = obj.get()['Body']\n",
    "            NWM_obs[r] = pd.read_csv(body)\n",
    "            NWM_obs[r].rename(columns = {'Datetime':'datetime'}, inplace = True)\n",
    "            NWM_obs[r].set_index('datetime', inplace = True)\n",
    "            NWM_obs[r]=NWM_obs[r][start:end]\n",
    "            NWM_obs[r] = pd.DataFrame(NWM_obs[r]['NWM_flow'])\n",
    "            NWM_obs[r]['station_id'] =  NWIS_NHDPlus['NWISid'][ NWIS_NHDPlus['NHDPlusid'] == reach].values[0]\n",
    "            NWM_obs[r].reset_index(inplace = True)\n",
    "        else:\n",
    "            r = str(reach)\n",
    "\n",
    "    return NWM_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 967.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get data for each reach\n",
    "NWM_obs = get_NWM_data(NWIS_NHDPlus, start, end)\n",
    "\n",
    "# Merge into one DF\n",
    "NWM_obs_df = pd.DataFrame()\n",
    "pbar = ProgressBar()\n",
    "for site in tqdm(NWM_obs):\n",
    "    NWM_obs_df = pd.concat([NWM_obs[site], NWM_obs_df])\n",
    "\n",
    "# Convert to datetime to make sure it joins nicely with the clim_df\n",
    "NWM_obs_df['datetime'] = pd.to_datetime(NWM_obs_df['datetime'])\n",
    "NWM_obs_df.to_csv(path_01 + '03.output/nwm_flow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Get temperature and precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n",
      "100% |########################################################################|\n",
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# # Activate google-earth engine\n",
    "# def climate():\n",
    "#     #ee.Authenticate()\n",
    "#     ee.Initialize()\n",
    "#     import gc\n",
    "#     # get temperature and precipitation data\n",
    "# \n",
    "#     #NLDAS temperature\n",
    "#     temp = ee.ImageCollection('NASA/NLDAS/FORA0125_H002').select('temperature').filterDate(start, end)\n",
    "#     #NLDAS precipitation\n",
    "#     precip = ee.ImageCollection('NASA/NLDAS/FORA0125_H002').select('total_precipitation').filterDate(start, end)\n",
    "# \n",
    "#     # Define slc location of interest, lets use the USGS guage locations!\n",
    "#     lon = {}\n",
    "#     lat = {}\n",
    "#     poi = {}\n",
    "# \n",
    "#     for site in tqdm(np.arange(0,len(nwis_sites),1)):\n",
    "#         lon[site] = StreamStats['Long'][site]\n",
    "#         lat[site] = StreamStats['Lat'][site]\n",
    "#         poi[site] = ee.Geometry.Point(lon[site], lat[site])\n",
    "# \n",
    "#     scale = 1000  # scale in meters\n",
    "#     temp_temp = {}\n",
    "#     precip_temp = {}\n",
    "#     # Get the data for the pixel intersecting the point at each station\n",
    "# \n",
    "# \n",
    "#     pbar = ProgressBar()\n",
    "#     for site in tqdm(np.arange(0,len(nwis_sites),1)):\n",
    "#         name = StreamStats['station_id'][site]\n",
    "#         temp_poi = temp.getRegion(poi[site], scale).getInfo()\n",
    "#         precip_poi = precip.getRegion(poi[site], scale).getInfo()\n",
    "# \n",
    "#     #Convert pointer object array to dataframe to work with\n",
    "# \n",
    "# \n",
    "# \n",
    "#         name = StreamStats['station_id'][site]\n",
    "#         temp_temp[name] = EE_funcs.ee_array_to_df(temp_poi,['temperature'])\n",
    "#         precip_temp[name] = EE_funcs.ee_array_to_df(precip_poi,['total_precipitation'])\n",
    "#         #need to add in time to precip data, same daterange as temperaure\n",
    "#         precip_temp[name]['time'] = temp_temp[name]['time']\n",
    "#         del temp_poi\n",
    "#         del precip_poi\n",
    "# \n",
    "# \n",
    "#         gc.collect()\n",
    "# \n",
    "# \n",
    "#     #resample to the desired temporal scale, take the respective statistic from the data of resampling\n",
    "#     temporal_resample = 'D'\n",
    "# \n",
    "#     T, T_DF = dict_to_df(temp_temp, temporal_resample, 'temperature')\n",
    "#     P, P_DF = dict_to_df(precip_temp, temporal_resample, 'total_precipitation')\n",
    "# \n",
    "#     Clim_DF = pd.merge(T, P, on=['datetime', 'station_id'])\n",
    "#     Clim_DF.pop('time_x')\n",
    "#     Clim_DF.pop('time_y')\n",
    "#     return Clim_DF\n",
    "# climate_all = climate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T06:25:06.279313Z",
     "start_time": "2023-11-08T06:25:06.263071Z"
    },
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save the temperature and precipitation data\n",
    "# climate_all['station_id'] = climate_all['station_id'].astype(int).astype(str)\n",
    "# climate_all.to_csv(path_01 + '03.output/climate_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>temperature_F</th>\n",
       "      <th>precipitation_in</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.00</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.70</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-13</td>\n",
       "      <td>45.356945</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>35.096</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.00</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.70</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>35.258</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.45</td>\n",
       "      <td>62.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.00</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.70</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>52.483334</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>36.860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.35</td>\n",
       "      <td>65.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.00</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.70</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-16</td>\n",
       "      <td>60.296875</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>38.120</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>63.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10011500</td>\n",
       "      <td>40.965225</td>\n",
       "      <td>-110.853508</td>\n",
       "      <td>174.00</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>67.70</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.94</td>\n",
       "      <td>27.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-17</td>\n",
       "      <td>68.876045</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>38.102</td>\n",
       "      <td>0.04698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.85</td>\n",
       "      <td>65.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62710</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.858530</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.20</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.644</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62711</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.858530</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.20</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.876</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62712</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.858530</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.20</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>1.545208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.438</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62713</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.858530</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.20</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>1.717708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.498</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62714</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.858530</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.20</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>1.840937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.378</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60044 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      station_id        Lat        Long  Drainage_area_mi2  \\\n",
       "0       10011500  40.965225 -110.853508             174.00   \n",
       "1       10011500  40.965225 -110.853508             174.00   \n",
       "2       10011500  40.965225 -110.853508             174.00   \n",
       "3       10011500  40.965225 -110.853508             174.00   \n",
       "4       10011500  40.965225 -110.853508             174.00   \n",
       "...          ...        ...         ...                ...   \n",
       "62710   10172952  41.858530 -113.327219               8.57   \n",
       "62711   10172952  41.858530 -113.327219               8.57   \n",
       "62712   10172952  41.858530 -113.327219               8.57   \n",
       "62713   10172952  41.858530 -113.327219               8.57   \n",
       "62714   10172952  41.858530 -113.327219               8.57   \n",
       "\n",
       "       Mean_Basin_Elev_ft  Perc_Forest  Perc_Develop  Perc_Imperv  \\\n",
       "0                  9720.0        67.70           1.2        0.120   \n",
       "1                  9720.0        67.70           1.2        0.120   \n",
       "2                  9720.0        67.70           1.2        0.120   \n",
       "3                  9720.0        67.70           1.2        0.120   \n",
       "4                  9720.0        67.70           1.2        0.120   \n",
       "...                   ...          ...           ...          ...   \n",
       "62710              8190.0         6.01           0.0        0.021   \n",
       "62711              8190.0         6.01           0.0        0.021   \n",
       "62712              8190.0         6.01           0.0        0.021   \n",
       "62713              8190.0         6.01           0.0        0.021   \n",
       "62714              8190.0         6.01           0.0        0.021   \n",
       "\n",
       "       Perc_Herbace  Perc_Slop_30  ...   datetime   flow_cfs        s1  \\\n",
       "0              2.94          27.2  ... 2012-03-13  45.356945  0.515038   \n",
       "1              2.94          27.2  ... 2012-03-14  49.750000  0.515038   \n",
       "2              2.94          27.2  ... 2012-03-15  52.483334  0.515038   \n",
       "3              2.94          27.2  ... 2012-03-16  60.296875  0.515038   \n",
       "4              2.94          27.2  ... 2012-03-17  68.876045  0.515038   \n",
       "...             ...           ...  ...        ...        ...       ...   \n",
       "62710         40.20          57.5  ... 2020-02-25   1.530000  0.000000   \n",
       "62711         40.20          57.5  ... 2020-02-26   1.530000  0.000000   \n",
       "62712         40.20          57.5  ... 2020-02-27   1.545208  0.000000   \n",
       "62713         40.20          57.5  ... 2020-02-28   1.717708  0.000000   \n",
       "62714         40.20          57.5  ... 2020-02-29   1.840937  0.000000   \n",
       "\n",
       "             s2  temperature_F  precipitation_in  storage   swe  NWM_flow  DOY  \n",
       "0      0.857167         35.096           0.00000      0.0  7.70      60.0   73  \n",
       "1      0.857167         35.258           0.00000      0.0  7.45      62.0   74  \n",
       "2      0.857167         36.860           0.00000      0.0  7.35      65.0   75  \n",
       "3      0.857167         38.120           0.00000      0.0  7.25      63.0   76  \n",
       "4      0.857167         38.102           0.04698      0.0  6.85      65.0   77  \n",
       "...         ...            ...               ...      ...   ...       ...  ...  \n",
       "62710  1.000000         27.644           0.00000      0.0  0.00       1.0   56  \n",
       "62711  1.000000         29.876           0.00000      0.0  0.00       1.0   57  \n",
       "62712  1.000000         35.438           0.00000      0.0  0.00       1.0   58  \n",
       "62713  1.000000         38.498           0.00000      0.0  0.00       1.0   59  \n",
       "62714  1.000000         41.378           0.00000      0.0  0.00       1.0   60  \n",
       "\n",
       "[60044 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasonality = pd.read_csv(path_01 + '02.input/seasonality.csv')\n",
    "# variable_list = [climate_all, all_storage, all_snow_swe, NWM_obs_df]\n",
    "variable_list = [all_storage, all_snow_swe, NWM_obs_df]\n",
    "final_data_set = pd.merge(StreamStats, all_sites_flow, on=['station_id'])\n",
    "final_data_set['month'] = final_data_set.datetime.dt.month\n",
    "final_data_set = pd.merge(final_data_set, seasonality, on=['month'])\n",
    "for variable in variable_list:\n",
    "    final_data_set = pd.merge(final_data_set, variable, on=['datetime', 'station_id'])\n",
    "final_data_set['DOY'] = final_data_set.datetime.dt.dayofyear\n",
    "final_data_set.pop('month')\n",
    "\n",
    "#What feature are relevant. Lets remove NWIS_site_id\n",
    "#final_data_set.pop('station_id')\n",
    "\n",
    "#remove old datetime column\n",
    "#final_data_set.pop('datetime')\n",
    "\n",
    "#drop rows with nan\n",
    "final_data_set.dropna(inplace = True)\n",
    "final_data_set.to_csv(path_01 + '03.output/raw_training_data.csv')\n",
    "final_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Drainage_area_mi2</th>\n",
       "      <th>Mean_Basin_Elev_ft</th>\n",
       "      <th>Perc_Forest</th>\n",
       "      <th>Perc_Develop</th>\n",
       "      <th>Perc_Imperv</th>\n",
       "      <th>Perc_Herbace</th>\n",
       "      <th>Perc_Slop_30</th>\n",
       "      <th>...</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>temperature_F</th>\n",
       "      <th>precipitation_in</th>\n",
       "      <th>storage</th>\n",
       "      <th>swe</th>\n",
       "      <th>NWM_flow</th>\n",
       "      <th>DOY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-09</td>\n",
       "      <td>1.510441</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>43.700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-10</td>\n",
       "      <td>1.808750</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>47.660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-11</td>\n",
       "      <td>1.955104</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>49.244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>1.940313</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>51.998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-03-13</td>\n",
       "      <td>1.889479</td>\n",
       "      <td>0.515038</td>\n",
       "      <td>0.857167</td>\n",
       "      <td>54.554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62710</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-25</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27.644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62711</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-26</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>29.876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62712</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-27</td>\n",
       "      <td>1.545208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62713</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-28</td>\n",
       "      <td>1.717708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62714</th>\n",
       "      <td>10172952</td>\n",
       "      <td>41.85853</td>\n",
       "      <td>-113.327219</td>\n",
       "      <td>8.57</td>\n",
       "      <td>8190.0</td>\n",
       "      <td>6.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>40.2</td>\n",
       "      <td>57.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>1.840937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2907 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      station_id       Lat        Long  Drainage_area_mi2  Mean_Basin_Elev_ft  \\\n",
       "5168    10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "5169    10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "5170    10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "5171    10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "5172    10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "...          ...       ...         ...                ...                 ...   \n",
       "62710   10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "62711   10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "62712   10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "62713   10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "62714   10172952  41.85853 -113.327219               8.57              8190.0   \n",
       "\n",
       "       Perc_Forest  Perc_Develop  Perc_Imperv  Perc_Herbace  Perc_Slop_30  \\\n",
       "5168          6.01           0.0        0.021          40.2          57.5   \n",
       "5169          6.01           0.0        0.021          40.2          57.5   \n",
       "5170          6.01           0.0        0.021          40.2          57.5   \n",
       "5171          6.01           0.0        0.021          40.2          57.5   \n",
       "5172          6.01           0.0        0.021          40.2          57.5   \n",
       "...            ...           ...          ...           ...           ...   \n",
       "62710         6.01           0.0        0.021          40.2          57.5   \n",
       "62711         6.01           0.0        0.021          40.2          57.5   \n",
       "62712         6.01           0.0        0.021          40.2          57.5   \n",
       "62713         6.01           0.0        0.021          40.2          57.5   \n",
       "62714         6.01           0.0        0.021          40.2          57.5   \n",
       "\n",
       "       ...   datetime  flow_cfs        s1        s2  temperature_F  \\\n",
       "5168   ... 2012-03-09  1.510441  0.515038  0.857167         43.700   \n",
       "5169   ... 2012-03-10  1.808750  0.515038  0.857167         47.660   \n",
       "5170   ... 2012-03-11  1.955104  0.515038  0.857167         49.244   \n",
       "5171   ... 2012-03-12  1.940313  0.515038  0.857167         51.998   \n",
       "5172   ... 2012-03-13  1.889479  0.515038  0.857167         54.554   \n",
       "...    ...        ...       ...       ...       ...            ...   \n",
       "62710  ... 2020-02-25  1.530000  0.000000  1.000000         27.644   \n",
       "62711  ... 2020-02-26  1.530000  0.000000  1.000000         29.876   \n",
       "62712  ... 2020-02-27  1.545208  0.000000  1.000000         35.438   \n",
       "62713  ... 2020-02-28  1.717708  0.000000  1.000000         38.498   \n",
       "62714  ... 2020-02-29  1.840937  0.000000  1.000000         41.378   \n",
       "\n",
       "       precipitation_in  storage  swe  NWM_flow  DOY  \n",
       "5168                0.0      0.0  0.0       2.0   69  \n",
       "5169                0.0      0.0  0.0       2.0   70  \n",
       "5170                0.0      0.0  0.0       2.0   71  \n",
       "5171                0.0      0.0  0.0       2.0   72  \n",
       "5172                0.0      0.0  0.0       2.0   73  \n",
       "...                 ...      ...  ...       ...  ...  \n",
       "62710               0.0      0.0  0.0       1.0   56  \n",
       "62711               0.0      0.0  0.0       1.0   57  \n",
       "62712               0.0      0.0  0.0       1.0   58  \n",
       "62713               0.0      0.0  0.0       1.0   59  \n",
       "62714               0.0      0.0  0.0       1.0   60  \n",
       "\n",
       "[2907 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_set[final_data_set['station_id'] == '10172952']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
